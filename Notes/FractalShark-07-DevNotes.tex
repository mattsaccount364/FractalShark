\section{Development Notes}

This section simply has some development notes that were posted over time.

\subsection{2026-2-14 News --- Version 0.51}

Version \code{0.51} introduces the \textbf{Feature Finder} (\cref{sec:feature-finder}), a major new tool
for exploring the Mandelbrot set, along with several usability improvements.

\begin{itemize}
\item \textbf{Feature Finder.}  Move mouse near a feature, select feature
  finder, and \FractalShark{} will locate the nearest interesting feature---a
  mini-Mandelbrot or satellite bulb---using Newton's method (\cref{subsec:ff-newton}).  A line is drawn
  from your cursor to the detected feature so you can see exactly where it is.
  The finder works at any zoom depth.  Currently, you manually choose between
  direct iteration, perturbation theory, and linear approximation depending on
  how deep you are.  In the future, this choice will be automatic.

\item \textbf{Scan mode.}  Instead of a single click, scan mode surveys a grid
  of points across the screen and marks up to 144 discovered features with small
  circles.  This gives you a quick overview of what is nearby before you decide
  where to zoom next.

\item \textbf{Auto-zoom to feature.}  Once a feature has been found you can tell
  the program to zoom into it automatically.  The iteration count is ramped up
  smoothly during the zoom so the image stays sharp as you approach the target.
  This feature remains hacky and experimental but if it works it's fun to
  play with.

\item \textbf{Mouse-wheel zoom.}  Scroll the mouse wheel forward to zoom in
  toward the cursor, or backward to zoom out.  The code is rewritten.

\item \textbf{Rendering improvements.}  Parts of the OpenGL display layer are
  rewritten for better reliability and to support the new feature-finder
  overlays (lines and circles drawn on top of the fractal).

\item \textbf{Under the hood.}  Extended the derivative math in the
  linear-approximation pipeline so the feature finder works at extreme depths.
  Improved precision tracking during zoom.  Added a \code{.clang-format}
  configuration for consistent code formatting.
\end{itemize}

\subsection{2026-1-18 News --- Version 0.501}

\begin{itemize}
\item \textbf{Basic Wine compatibility.}  Tested without a GPU and working under
Wine 9.0 on Linux.   There are still some quirks, but basic rendering works in
CPU-only mode. Whether \FractalShark{} can work with Wine + CUDA remains unknown as
an appropriate test platform is not currently available.
\item \textbf{Proper fallback when no GPU is present.}  The program now detects lack of a CUDA
device and falls back to CPU-only rendering, with a warning message.
\item \textbf{Fixed some windowing bugs.}
\item \textbf{Improved these notes.}
\item \textbf{Linked at FractalForums.}  See \url{https://fractalforums.org/index.php?topic=5564.0}
\end{itemize}

\subsection{2025-12-29 News --- Version 0.5}
\begin{itemize}
  \item \textbf{Major feature: GPU-accelerated Mandelbrot reference orbit.} Adds
  an experimental CUDA reference-orbit calculator (\cref{subsec:ref-orbit-gpu}) intended to complement the
  existing CPU/MPIR multithreaded approach.

  \item \textbf{High-precision arithmetic pipeline on GPU.} Includes a full
  multiply/add/subtract pipeline (\cref{sec:hp-add,sec:ntt-multiply}), a
  Number Theoretic Transform (NTT) implementation, parallel-prefix carry
  propagation, and HDR-style exponent tracking.

  \item \textbf{Interoperability and correctness.} Supports conversion between
  MPIR and the GPU high-precision float format and adds manually-run test
  infrastructure.

  \item \textbf{Performance notes.} Release notes report large speedups at high
  limb counts on RTX 4090/5090-class GPUs; intended to shine at very deep
  built-in views.

  \item \textbf{Project plumbing.} Hooks up GitHub Actions to produce “official”
  builds, plus some refactoring; notes a startup windowing quirk.
\end{itemize}

\subsection{2025-11-30 News}

The neat thing about this GPU approach is that it still has low-hanging fruit 
related to optimization, unlike MPIR.  I've been wasting a bunch of time on the 
(Thanksgiving in USA) holiday weekend working on it.  I'll keep fussing with it 
and will probably not post again until I put out a version of \FractalShark{} with 
it hooked up, which I expect to happen later in December when I have time off.  
All code is on github, just no new version yet since it remains rather 
experimental and hacked up.

Example times in ms of updated implementation, comparing against serial host-
based approach (1 thread MPIR AVX2 for experiments).  This is the first 20000 
iterations of View 30 in \FractalShark{}, which is a depth $\sim 1\mathrm{e}{100000
}$ point.  This uses 16384 limbs on the GPU.  The CPU/MPIR uses the minimum 
bits required for that point, which is less, because the GPU implementation 
requires a power of 2.

\begin{verbatim}
Host (ms)    GPU(ms)    Ratio
57990        2055    28.2189781
58227        2022    28.79673591
57478        2041    28.16168545
57538        2014    28.56901688
55997        2015    27.79007444

Averages:
57446        2029.4    28.30729816
\end{verbatim}

Here's a summary of what I want to get done before \FractalShark{} 0.5 happens:

\begin{itemize}

\item Perform code cleanup, better comments etc.  It's a mess right now, this 
is just a weekend hobby after all and because it was unclear this would even 
work it's a real hack job.

\item Improve integration with \FractalShark{}, it's really just hacked in there 
currently since I've mostly used a standalone test program

\item Improve reference orbit memory usage - I would like to be able to 
allocate a fixed amount of memory to store the orbit and expand as needed.  
Right now it pessimistically allocates a lot, and it may be the wrong amount 
because it does periodicity detection on the GPU (\cref{sec:hp-periodicity}).  This is just engineering 
work and nothing fundamental, but will take time and without it the current 
implementation is rather impractical.

\item Automatically choose kernel launch parameters, right now it's manual.  
More engineering.  This is required to support other cards than mine.  In any 
case this implementation requires a feature call cooperative groups, which I 
believe implies Nvidia RTX 2XXX series or newer.

\end{itemize}

Those are the main things offhand, and I'm hoping with time off in December I can clean these up.

\subsection{2025-9-1 News}

The full reference orbit works with CUDA, though without periodicity detection 
or high precision to ``float exp'' conversion.  That's future work but I'm not 
worried about it.  To be clear, this is not hooked up end-to-end with 
\FractalShark{} itself, it's only working in a standalone test environment.  But 
the results are promising and prove it works.

This initial implementation relies on Karatsuba for the multiplies/squaring and 
then follows those with the high-precision adds/subtracts.  Initial results 
suggest a $\sim 12\times$ perf improvement relative to single-threaded CPU 
only, when comparing an overclocked 5950X vs an RTX 4090 with CUDA.  I'm happy 
with that, but not completely.

The main performance problem is this Karatsuba implementation.  Getting decent 
performance out of Karatsuba obviously requires recursion, and that gets costly 
on the GPU.  This implementation recurses several levels, which avoids costly 
local memory spill, but bites us because of register pressure.  The high 
register pressure limits the parallelism we can achieve.  The nice thing about 
Karatsuba for me is that it's not that hard to understand conceptually, so it 
was a great initial target for someone who doesn't know what they're doing.

Now that it's working, and I have a better sense of what's going on, I'm going 
to try a full NTT-based high-precision multiply approach (\cref{sec:ntt-multiply}).  The idea here is to 
rely on the number theoretic transform, similar to FFT, and parallelize the 
high precision multiply that way.

With this commit, we have a working host-based (CPU-only) approach to NTT high-
precision multiply that supports power-of-2 mantissa sizes and should scale 
effectively to CUDA but that's TBD.  It will be at least several months more 
work at my current rate (a few hours on the weekends) to achieve a first-cut 
CUDA implementation.

\subsubsection{NTT-based high-precision multiply (magic prime \texorpdfstring{$2^{64} - 2^{32} + 1$}{2\^{}64 - 2\^{}32 + 1})}

The following summary is a lightly edited draft.

I'm experimenting with an NTT implementation over the 64-bit ``magic'' prime $p 
= 2^{64} - 2^{32} + 1$. This prime is NTT-friendly: it admits $2^{32}$-th roots 
of unity, so power-of-two transform sizes are straightforward, and it enables 
fast modular reduction on 128-bit products using the identity $2^{64} \equiv 2^{
32} - 1 \pmod p$.

\paragraph{High-level plan}

\begin{itemize}

\item Represent big mantissas as base-2 limbs (currently 32-bit limbs are 
convenient on GPU/CPU). Choose $N =$ next power of two $\ge 2\cdot L$ ($L =$ 
limb count) for the convolution length.

\item Forward NTT(A), NTT(B) mod $p$, pointwise multiply, inverse NTT, multiply 
by $N^{-1} \bmod p$, then perform carry propagation back to the chosen limb 
base.

\item Use iterative radix-2 Cooley--Tukey with an explicit bit-reversal 
permutation (DIT). Twiddles (powers of a primitive root) are precomputed and 
cached.

\item Butterflies and pointwise products operate in Montgomery form; 128-bit 
products are reduced via Montgomery multiplication ($R = 2^{64}$). A direct 
pseudo-Mersenne fold $(\text{lo} + (\text{hi} \ll 32) - \text{hi})$ exists but 
isn’t used on the hot path.

\end{itemize}

\paragraph{Notes and guardrails}

\begin{itemize}

\item Single-prime NTT is attractive here because $p$ fits in 64 bits and gives 
ample dynamic range; if/when larger bases or tighter bounds are desired, a multi
-prime CRT variant is the next step.

\item Power-of-two sizes only: that matches the current host prototype and 
simplifies CUDA mapping.

\item Carry fix-up remains outside the NTT and is done in base-$2^k$ with linear
-time passes; lazy (deferred) carries may help throughput.

\end{itemize}

\paragraph{Why this might beat Karatsuba on GPU}

\begin{itemize}

\item Avoids deep recursion and its register pressure; most work is regular 
butterflies, which parallelize and schedule well.

\item Pointwise multiplies dominate cost but are simple $64\times 64 \rightarrow
 128$ with fast reduction; memory access is structured and coalesced.

\end{itemize}

If the CUDA path pans out, the NTT route should scale better across precisions 
while keeping occupancy higher than the recursive Karatsuba path.

\paragraph{References (NTT / GPU big-int)}
\begin{itemize}
\item CGBN: CUDA Big-Num with Cooperative Groups \cite{CGBN_NVlabs}
\item Number-theoretic transform \cite{NTT_Wikipedia}
\end{itemize}


\subsection{2025-07-28 News --- Version 0.46}

Version \code{0.46} was published as a pre-release, focusing on refactoring,
reference compression experimentation, save-file compatibility, and test infrastructure.

\begin{itemize}

  \item Continued internal \textbf{refactoring} of core rendering and control logic.
  
  \item Experimental support for \textbf{Imagina-style “max” reference compression},
        as an alternative to the existing “simple” scheme.
  
  \item Added preliminary support for \textbf{Imagina-compatible save files}.

  \item Enhanced internal \textbf{test infrastructure} with broader coverage and tooling.
    
  \item Work in progress toward additional infrastructure improvements.

\end{itemize}

This release was published on GitHub on July 28, 2025 (UTC) and is tagged as a
pre-release build.  


\subsection{2025-6-15 News}

This page actually gets traffic occasionally, so I just wanted to post a short 
update.  Since last August, I've been working on a CUDA-based, high-precision 
reference orbit implementation.  The objective is to beat \FractalShark{}'s 
existing multithreaded reference-orbit performance at higher digit counts, at 
least if you have a decent card.  Scroll down to ``[2025-6-15](\#2025-6-15)'' 
for the latest information on this subject.

Still fussing with it, with some delays because of vacation etc.  Having some 
issues with the optimized ``add'' implementation that does the 5-way add/
subtract.  It's a fun project, but has ended up more complex than I'd 
expected.  The reference implementation is almost working the way I want.

Worst case I could dump it and fallback to a series of regular A+B adds/
subtracts but I'm pretty determined to make the optimized approach work.  TBD 
if the performance actually pays off.  (Yes, I can hear you saying the 
Mandelbrot multiplies/squares dominate the cost, but it's bugging me and fun to 
play with).

\subsection{2025-4-26}

This high-precision arithmetic project is a lot of fun even if it's pretty 
amateur-hour -- I know I'm leaving a lot of perf on the table yet.

Here's a brief update.  I'm happy enough with Karatsuba multiply now.  I've got 
$3\times$ parallel multiplies working.  For Mandelbrot, that corresponds to $x^2
$, $y^2$ and $x\cdot y$.  Rather than doing an optimized squaring 
implementation, I'm just jamming everything into the same Karatsuba 
implementation, so that all the synchronization is re-used.

A few weeks ago I had CUDA floating point add working.  That's much easier of 
course, though carry propagation is interesting.  I tried a parallel prefix sum 
but the performance was a bit underwhelming in the average case, which is what 
I care about.  I instead implemented a more naive strategy that has better 
average case perf and linear worst-case performance, which I think I'm fine 
with for Mandelbrot.  I'm not using warp-level primitives and haven't hooked up 
shared memory on that, so it's horrid performance compared to simply doing it 
on the CPU but as a percentage of the total it's minor, because large 
multiplies are so costly.  I'm not that worried about Add at this point.

I'm currently hooking up a 5-way combined add/subtract that does $A - B + C$ 
and $D + E$ in parallel to produce two outputs.  These inputs corresponds to $X^
2 - Y^2 + C_x$ and $2\cdot X\cdot Y + C_y$.

Strategy-wise, the idea is to complete this multi-way add operator, and then we 
should be able to do a reference orbit using alternating $3$-way multiply and $5
$-way add calls in CUDA.  It also needs periodicity detection and high-
precision to float+exp conversion, which shouldn't be bad.  Maybe in a few 
months I'll have something working end-to-end.

I was also speculating about trying Schönhage--Strassen CUDA multiply, but 
that's a ways out.

\subsection{2025-3}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepancy arises in the CUDA 
calculation.  The discrepancy points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \code{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \code{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \code{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

Current best result (5950X MPIR vs RTX 4090), 50000 sequential multiplies of 
7776-limb numbers, 128 blocks, 96 threads/block, uses shared memory and 162 
registers (max 255):

Host iter time: 26051 ms

GPU iter time: 2757 ms

Edited from earlier, fussing with perf-related parameters.  I'm very happy with how it's looking now.

\subsection{2025-1 -- What's going on with this native CUDA reference orbit calculation?}

The repository (``TestCuda'') has a new Karatsuba, high-precision, floating 
point multiply implementation working on my GPU and results are showing the CPU 
take $3$--$4\times$ longer (MPIR/AVX2) than the GPU on sequential multiplies of 
random numbers.  That's a key point -- sequential multiplies, so the result is 
applicable to e.g.\ a reference orbit calculation.  I'm really happy with this 
result, because there's leftover hardware on the GPU that could be used to run 
a couple of these in parallel (e.g.\ two squares and a multiplication or three 
squares for Mandelbrot).

Getting high-throughput high-precision on-GPU is already more-or-less solved:  
Nvidia already provides a library for it (see related work below).  But getting 
sequential to work decently at sizes that are still interesting (e.g.\ ones we 
might actually try on the Mandelbrot) is not as widely investigated, which is 
why I've been dwelling on this for a while and still have only mediocre results 
:p

A variety of caveats currently:

\begin{itemize}

\item I'm comparing a 5950X vs RTX 4090, which clearly affects the relative 
numbers.

\item The approach requires CUDA cooperative groups, which I think is RTX 2xxx 
and later, so fairly recent cards.

\item The size I'm getting the best result at currently is relatively large: 
8192 32-bit limbs, which is pretty big.  It still beats the CPU down to 2048 
limbs though (CPU takes $1.6\times$ longer here) and at that size there is a 
bunch of unused hardware on the GPU so it should be possible to do the three 
multiplies for Mandelbrot in parallel.

\end{itemize}

Anyhow, I wanted to post it because this is a pretty complex investigation and 
I expect to spend some more time on it because it's been fun to look at.

Here are some bullet points on the approach:

\begin{itemize}

\item Uses CUDA cooperative groups, so we should be able to do a reference 
orbit with a single kernel invocation.

\item 32-bit limbs, 128-bit intermediate results (2x64 integers) because of 
intermediate carries.  Really it's just 64 bits + plus a few more.  This 
approach is likely not super-efficient but it's where we're at.

\item Stores the input mantissa in the chip ``shared memory''.  For 8192 limbs, 
that's $8192 \cdot 4$ bytes $\cdot 2$ numbers to multiply $= 64$KB, and then 
stores $2 \cdot 16$KB extra for an intermediate Karatsuba result, for 96KB 
total.  This piece is negotiable and I could bring down/eliminate the shared 
memory requirement depending on how things progress.

\item One GPU thread per input limb, or two output limbs per thread.  It does a 
full 16384 limb output in this example and then truncates/shifts it.

\item The GPU floating point format has a separate integer exponent, which is 
overkill for Mandelbrot but I figured I'd keep it for now because it's not a 
performance problem.  It also keeps a sign separately.

\item The application I'm using to test this has a bunch of cases to verify 
that it's producing correct results.  It generates pseudo-random numbers with 
many \code{0xFFF...} limbs, zero-limbs, and related, to force carries/
borrows.  The test program compares all results against my own Karatsuba CPU-
based implementation I can use as a reference, and more importantly, the MPIR 
implementation (\code{mpf\_mul}) for correctness.

\item I've got MPIR to GPU and GPU to MPIR conversion capability, so it's easy 
to translate formats as needed.

\item The GPU implementation gets its best performance when it doesn't recurse, 
and instead switches straight to convolutions on the sub problems.  Recursing 
is still getting me slightly worse performance and I think I know why but 
haven't worked out how to fix it.

\end{itemize}

Here is some related work:

\begin{itemize}

\item A Study of High Performance Multiple Precision Arithmetic on Graphics 
Processing Units: Niall Emmart \cite{Emmart_GPU_MultiPrecision}

\item Missing a Trick: Karatsuba Variations: Michael Scott \cite{Scott_Karatsuba_MissingTrick}

\item MFFT: A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT 
Framework: \cite{MFFT_GPU_FFT}

\item Karatsuba Multiplication in GMP: \cite{GMP_Karatsuba}

\item Karatsuba Algorithm: \cite{Karatsuba_Wikipedia}

\item Toom--Cook Multiplication: \cite{ToomCook_Wikipedia}

\item Sch{\"o}nhage--Strassen Algorithm: \cite{SchonhageStrassen_Wikipedia}

\item CGBN: CUDA Accelerated Multiple Precision Arithmetic Using Cooperative 
Groups: \cite{CGBN_NVlabs}

\end{itemize}

All the code is here / GPL etc but it's just an experiment: \\
\url{https://github.com/mattsaccount364/FractalShark}

It'll all end up in \FractalShark{} eventually assuming I can get something end-to-
end working, and at this point I believe I can, but this is very slow-going 
yet.  Anyway, it's a fun area and I'll probably continuing dinking with it, so 
there probably won't be much new on \FractalShark{} proper until I can get this 
behemoth under better control.  And we'll see if it works out -- lots of 
remaining details to resolve and it may not work decently when combined into a 
full reference orbit.  Overall though I'm really happy with where it's at.  
Happy to answer questions etc.

\subsection{2024-04-20 --- Version 0.45}

\begin{itemize}
  
  \item Adds a first cut at reference compression for an intermediate “perturbed
  perturbation” reference-orbit strategy.

  \item Includes three internal variants (v1: uncompressed/3 threads; v2:
  uncompressed/4 threads; v3: compressed/4 threads).

  \item v3 becomes the default at very deep “auto” perturbation settings;
  guidance given for when to stick to the older MT2 + periodicity workflow.

\end{itemize}


\subsection{2024-03-31 News --- Version 0.44}

Version \code{0.44} represents a substantial internal refactor emphasizing memory management, allocator control, and reference-orbit performance.

\begin{itemize}

  \item Removed the Boost dependency and introduced a custom
  \textbf{HighPrecision} wrapper.

  \item Improved reference-orbit performance, especially for low-precision,
  high-period locations.

  \item Improved performance of the \textbf{perturbed perturbation} algorithm.

  \item Introduced a custom file-backed \textbf{bump allocator} (\cref{sec:disk_backed_growable_vectors}) with stable
  interior pointers.

  \item Extended \code{GrowableVector} to support allocator-style usage
  without increasing committed memory.
  
  \item Updated reference orbit and linear approximation save-file formats.

  \item Added an \textbf{automatic perturbation mode} switching between single-threaded and multithreaded execution.

  \item Instrumented for memory-leak detection and fixed all known leaks.

  \item Fixed a long-standing correctness bug in BLAv1 dating back to version
  0.21.

  \item \textbf{Clarified ``perturbed perturbation'' design.} The implementation
  is framed as storing an intermediate-resolution reference orbit (e.g.\
  $\sim$500 bits), then using low-precision perturbation off that intermediate
  orbit to generate subsequent nearby reference orbits more efficiently.

\end{itemize}


\subsection{2024-03-03 News --- Version 0.43}
\begin{itemize}
  
  \item Significant performance wins for lower-precision, high-period
  reference-orbit computation.
  
  \item Adds a custom per-thread bump allocator for MPIR high-precision numbers.
  
  \item Rebalances work across the existing 3-thread multithreaded
  reference-orbit approach.

  \item \textbf{Allocator usage guidance.} The bump allocator is primarily
  beneficial for low-precision, high-period reference orbits; at very high
  precision the arithmetic dominates and allocation overhead is negligible. A
  suggested heuristic is to use the fixed-block bump allocator below roughly
  1000 digits and fall back to the global allocator above that.

  \item \textbf{Empirical sizing recommendation.} Tested successfully on a $\sim
  1\mathrm{e}{6000}$ location with about 4\,MB of bump-allocator space;
  recommendation is to use the smallest practical buffer (cache behavior matters).

  \item \textbf{Implementation refinement note.} In the current multithreaded
  orbit implementation, most allocations may occur on the main thread;
  thread-local allocation may be unnecessary. A few small allocator issues were
  identified for cleanup in the following release.

\end{itemize}


\subsection{2024-02-24 News --- Version 0.42}

Version \code{0.42} introduced major improvements to linear approximation performance, benchmarking, and configurability.

\begin{itemize}
  \item Added \textbf{multithreaded linear approximation table generation}.
  \item Dramatically improved performance for high-period locations, especially with reference compression enabled.
  \item Added fine-grained benchmarking and performance breakdowns.
  \item Enabled regeneration of linear approximation tables independently of per-pixel data.
  \item Introduced adjustable linear approximation presets trading memory, accuracy, and performance.
\end{itemize}

This release corrected a prior precision regression and restored high-accuracy defaults.


\subsection{2024-01-15 News --- Version 0.41}

\begin{itemize}
  \item Fixes a minor bug in reference compression / GPU.
  \item Memory-use overhaul for storing LA tables and reference orbit to minimize RAM needed for hard views (e.g., View \#27).
  \item Default mode now uses temporary files + memory-mapped IO; avoids needing 2$\times$ memory during resize/copy and reduces committed-memory requirements with no measurable overhead per the release note.
\end{itemize}


\subsection{2024-01-01 News --- Version 0.4}

\begin{itemize}
  \item First working implementation of \textbf{runtime} reference compression
  (decompress-on-demand during rendering).

  \item Motivated as reducing end-to-end RAM requirements dramatically on very
  high-period locations; not enabled by default for “Auto” rendering and
  described as work-in-progress.

  \item \textbf{Motivation: memory pressure.} Reference compression is
  explicitly framed as a response to high RAM requirements at difficult views
  (notably after enabling 64-bit iteration counts).
\end{itemize}


\subsection{2023-12 News --- Version 0.32}

Version \code{0.32} focused on usability, correctness, and incremental architectural cleanup.

\begin{itemize}
  \item Added \textbf{progressive rendering}, allowing partial image updates
  during long GPU renders.
  \item Added a \textbf{2x32 non-HDR linear approximation} path for numerically
  difficult shallow zooms.
  \item Fixed a subtle but severe correctness bug in the 2x32 implementation
  present in version 0.31.
  \item Added a basic regression test suite.
  \item Refactored code and restored buildability across inactive code paths.
\end{itemize}


\subsection{2023-12-09 News --- Version 0.31}

Version \code{0.31} expanded the set of available rendering algorithms and
substantially improved default performance at shallow and mid-depth zooms.

\begin{itemize}
  \item Added \textbf{1x32 linear approximation / perturbation} for
  high-performance shallow zooms.
  
  \item Added \textbf{1x64 linear approximation / perturbation}, primarily for
  correctness testing.
  
  \item Implemented \textbf{automatic render-algorithm selection} based on zoom
  depth.
  
  \item Eliminated unnecessary reference-orbit copying, significantly improving
  frame-to-frame performance.
  
  \item Fixed several bugs, including 2x32 correctness issues and unaligned
  HDRx64 memory access.

  \item \textbf{Auto-selection policy detail.} Auto mode is described as staging
  from direct 32-bit rendering at very shallow zooms, to 1x32 perturb-only, to
  1x32 LA, and finally 1x32 float-exp LA at deeper zooms, with the goal of
  improving default performance below roughly $10^{38}$.

  \item \textbf{Reference-orbit reuse optimization.} Removing unnecessary
  reference-orbit copying is called out as a major win for deep-zoom
  frame-to-frame responsiveness when reusing a reference orbit.

\end{itemize}


\subsection{2023-11-26 News --- Version 0.3}

\begin{itemize}
  
  \item Adds HDRx2x32 linear approximation: faster than LA/HDRx64 on consumer
  cards while retaining nearly native-64-bit precision.
  
  \item Notes that a “Debug View 20” anomaly is resolved under HDRx2x32,
  supporting the hypothesis that HDRx32 precision limits caused the issue.
  
  \item Misc performance improvements and bug fixes elsewhere.

  \item \textbf{Debug View 20 consistency improvement.} The ``Debug View 20''
  anomaly is reported as resolved under HDRx2x32, supporting the conclusion that
  the earlier HDRx32 behavior was a precision-limit issue.

  \item \textbf{HDRx2x32 precision characterization.} Described as ``Linear
  Approximation + Float EXP'' using a pair of 32-bit floats plus an exponent,
  with an estimated combined precision of roughly 46 bits.

\end{itemize}


\subsection{2023-11-12 News --- Version 0.24}

\begin{itemize}
\item \textbf{UI/UX improvements.} Adds a hotkey help view and general UI
cleanup.
\item \textbf{Random palette generation.} Adds a random palette generator; notes
the palette system remains intentionally constrained and may be expanded.
\item \textbf{Rendering diagnostics.} Adds a ``Show Rendering Details'' view for
inspecting current algorithm/settings.
\item \textbf{Early HDRx2x32 work.} Begins a GPU HDR 2x32 float implementation
(explicitly noted as not working yet).
\item \textbf{Crash diagnostics.} Automatically generates a crash dump on
unhandled exceptions.
\end{itemize}


\subsection{2023-11-05 News --- Version 0.23}
\begin{itemize}
  \item Adds switchable 64-bit iteration count support (enabling tens of
  billions of iterations per pixel, leveraging LA).
  \item Moves antialiasing/coloring into its own CUDA kernel for better
  performance.
  \item Multiple memory-reduction optimizations (systems-level) and related perf
  work.
  \item Adds reference-orbit save/load (no compression yet), useful for
  debugging deep spots.
  \item Notes a known bug manifesting in built-in View \#20 under a particular
  configuration (64-bit iterations + default HDRx32 LAv2), with long render
  times due to slow reference-orbit calculation.
\end{itemize}


\subsection{2023-10-14 News --- Version 0.22}

\begin{itemize}

  \item Significant LAv2 performance improvements (bug fixes); notes BLAv1 still
  winning in some cases.

  \item Fixes an LAv2 correctness bug; release note claims no known correctness
  bugs remaining.

  \item \textbf{Performance data point.} Reports a minibrot example improving
  from roughly 470\,ms to 370\,ms (render-only; reference orbit excluded) on the
  author’s machine.

\end{itemize}


\subsection{2023-10-08 News --- Version 0.21}
\begin{itemize}
  \item Performance improvements to BLA and LAv2.
  \item Memory-usage improvements (commit size / virtual address space issues
  called out).
  \item Generates more CUDA kernels for older GPUs (system requirements
  unchanged).
  \item Notes LAv2 is strong for many deep-zoom cases but still behind older BLA
  in some shallower cases.
\end{itemize}


\subsection{2023-09-16 News --- Version 0.2}

Version \code{0.2} introduced a major algorithmic milestone: the integration
of Imagina's linear approximation implementation as a CUDA kernel.

\begin{itemize}
  \item Added Imagina-based linear approximation (\code{LAv2}), now the
  default rendering algorithm.
  \item Resolved known correctness bugs in the new approximation path.
  \item Fixed a major performance regression caused by an earlier implementation oversight.
  \item Achieved large performance gains at deeper zoom levels compared to the
  existing bilinear approximation.
\end{itemize}

While the legacy bilinear approximation remains faster at very shallow zooms,
the new approach dominates once meaningful magnification is reached.


\subsection{2023-07-22 News --- Version 0.11}

Version \code{0.11} was released with a focus on performance improvements and
improved diagnostics.

\begin{itemize}
  \item Approximately \textbf{+10\% performance improvement} in the HDRx32 CUDA
  rendering path.
  \item Fixed several minor bugs affecting correctness and stability.
  \item Improved CUDA error reporting by displaying descriptive error strings
  instead of numeric codes.
\end{itemize}

A long-duration technology demonstration video accompanied this release,
showcasing a zoom to approximately $10^{4000}$ using the default HDRx32/BLA CUDA
kernel.  Rendering and post-processing each required roughly one and a half
days.
