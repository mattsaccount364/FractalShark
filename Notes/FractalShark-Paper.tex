\section{\FractalShark{} Overview}
\label{sec:goal}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{mandelbrot.png}
  \caption{The Mandelbrot set.}
  \label{fig:mandelbrot}
\end{figure}

\FractalShark{} is an interactive Mandelbrot set renderer focused on
\textbf{extreme deep-zoom exploration}, numerical correctness, and
algorithmic experimentation.  It supports zoom depths ranging from
conventional views to magnifications exceeding $10^{10000}$, while
exposing internal rendering choices and precision tradeoffs to the user.

\paragraph{Interactive navigation and view control.}
\FractalShark{} provides direct mouse-driven navigation, including centering
the view at an arbitrary point, zooming in and out at the cursor location,
stepping backward through navigation history, and invoking automatic zoom
modes.  Window management options allow toggling between windowed and
full-screen modes, including square-aspect rendering for precise analysis.

\paragraph{Built-in views for demonstration and validation.}
The application includes an extensive set of built-in views covering a wide
range of numerical regimes: GPU precision limits, high-period locations,
known hard points, historical bug cases, regression tests, and ultra-deep
zoom demonstrations.  These views serve both as showcases of capability and
as repeatable test cases for performance and correctness.

\paragraph{Explicit algorithm selection and transparency.}
A central design goal of \FractalShark{} is to make rendering algorithms
explicit and selectable.  Users may rely on an automatic algorithm selector
or manually choose among many rendering paths (see \cref{subsec:ref-orbit-mt-tradeoff} for automatic selection), including:
\begin{itemize}
  \item GPU-based low-zoom renderers with selectable iteration precision and
        multiple numeric formats (single-, dual-, and quad-limb 32- and 64-bit
        variants, as well as deeper representations discussed subsequently);
        see \cref{sec:mandel-base-kernels}.
  \item Scaled perturbation algorithms for extending low-precision arithmetic
        to deeper zooms (\cref{sec:perturbation-concept}).
  \item Bilinear approximation (BLA v1) perturbation paths (\cref{sec:bilinear-approx}).
  \item Linear approximation (LAv2) pipelines, including full rendering,
        linear-approximation-only, and perturbation-only variants
        (\cref{sec:la-v2-perturb}).
  \item Reference-compression–aware algorithms, including Imagina-compatible
        ``max'' compression formats (\cref{subsec:ref-orbit-compression-reuse}).
  \item CPU-only algorithms for very high precision, verification, and
        fallback scenarios.
\end{itemize}

Many of these modes are exposed explicitly for testing and comparison, and
not all are intended as default or production-quality paths.

\paragraph{Linear approximation configuration.}
Linear approximation behavior can be configured for multithreaded or
single-threaded execution and tuned via presets that prioritize accuracy,
performance, or memory usage.  These controls reflect ongoing development
and experimentation with LA parameter tradeoffs.

\paragraph{Image quality and coloring.}
\FractalShark{} supports GPU antialiasing at multiple sample levels and a
flexible palette system.  Palettes may be selected from predefined themes,
generated randomly, and rendered at configurable color depths.  Some palette
features are intentionally limited or disabled where they are known to be
incomplete.

\paragraph{Iteration limits and precision control.}
Users can dynamically adjust iteration limits, switch between 32-bit and
64-bit iteration counters, and modify shallow-zoom iteration precision.
These controls are primarily relevant for low-zoom or testing scenarios and
are not universally applicable to all rendering algorithms.

\paragraph{Perturbation and reference-orbit management.}
\FractalShark{} places strong emphasis on reference-orbit reuse and
perturbation-based rendering (\cref{sec:ref-orbit-calc}).  It supports multiple perturbation strategies
(single-threaded, multithreaded, periodicity-assisted (\cref{sec:hp-periodicity}), and experimental GPU
variants), along with tools to clear, inspect, save, reload, and reuse
reference orbits.  Automatic and manual reference-orbit persistence is
supported via file-backed storage (\cref{sec:disk_backed_growable_vectors}).

\paragraph{Memory management and scalability.}
To enable extreme zoom depths without exhausting system memory, \FractalShark{}
employs file-backed storage, optional reference compression, configurable
memory limits, and automatic cleanup policies.  These mechanisms are closely
tied to recent allocator and reference-orbit refactors discussed in the
project development history.

\paragraph{Diagnostics, benchmarking, and testing.}
The application includes facilities for displaying detailed rendering
parameters, running repeatable benchmarks, executing regression tests, and
comparing reference orbits (including Imagina-compatible formats).  These
features reflect \FractalShark{}’s dual role as both a visualization tool and a
development platform.

\paragraph{Data export and interoperability.}
\FractalShark{} can save rendered images, high-resolution bitmaps, iteration
counts, and reference orbits in multiple text and compressed formats.  It
supports loading external reference orbits and matching Imagina-compatible
data for cross-tool validation.

\medskip
\noindent
Overall, \FractalShark{} functions as both a \textbf{deep-zoom Mandelbrot
explorer} and a \textbf{research and experimentation platform} for
high-precision fractal rendering, prioritizing transparency, correctness, and
performance.  It is not intended as a polished end-user application, but rather
as an experimental platform for exploring and validating advanced fractal
rendering algorithms.

\section{The Mandelbrot set and escape-time rendering}
\label{sec:mandelbrot-intro}

The Mandelbrot set \(M\subset\mathbb{C}\) is defined as the set of complex
parameters \(c\) for which the orbit of
\begin{equation}
z_{n+1} = z_n^2 + c,\qquad z_0 = 0
\end{equation}
remains bounded. A common rendering method is \emph{escape-time}: iterate until
the orbit magnitude exceeds a bailout threshold, or until a maximum iteration
count is reached.

A standard bailout uses \(|z_n|^2 \ge 4\), since \(|z|>2\) implies divergence:
\begin{equation}
|z_n|^2 = \Re(z_n)^2 + \Im(z_n)^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}
For each pixel, the stored value is the smallest \(n\) (or a bounded proxy) at
which escape occurs, or the maximum iteration limit if escape never occurs.


\section{Pixel-to-parameter mapping}
\label{sec:mapping}

Each CUDA thread computes a pixel coordinate \((X,Y)\) and maps it to a complex
parameter \(c = x_0 + i y_0\). A typical kernel starts with:
\begin{verbatim}
int X = blockIdx.x * blockDim.x + threadIdx.x;
int Y = blockIdx.y * blockDim.y + threadIdx.y;
if (X >= width || Y >= height) return;
size_t idx = ConvertLocToIndex(X, height - Y - 1, width);
\end{verbatim}

\subsection{Thread-to-pixel mapping}
A 2D CUDA grid of 2D blocks covers the image. Each thread is responsible for one
pixel. The bounds check prevents out-of-range threads from writing.

\subsection{Y-axis convention}
The expression \code{height - Y - 1} flips \(Y\). This is common when the image
buffer uses a top-left origin but the complex-plane mapping assumes a
bottom-left origin (or vice versa).

\subsection{Affine map into the complex plane}
The parameter \(c\) is computed by an affine transform:

\begin{align}
x_0 &= cx + dx \cdot X, \\
y_0 &= cy + dy \cdot Y,
\end{align}

where \code{cx,cy} anchor the plane (e.g., the coordinate at pixel \((0,0)\)),
and \code{dx,dy} are per-pixel increments. Different kernels compute these
expressions in different numeric types; the intent is always the same: map each
pixel to its associated complex parameter \(c\).


\section{Mandelbrot recurrence in real arithmetic}
\label{sec:real-form}

Writing \(z = x + i y\) and \(c = x_0 + i y_0\), the iteration becomes:

\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + x_0, \\
y_{n+1} &= 2 x_n y_n + y_0.
\end{align}

The escape test is:

\begin{equation}
x_n^2 + y_n^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}

Many kernels cache squares:
\[
zrsqr = x^2,\quad zisqr = y^2,
\]
so that \(x^2-y^2\) and \(x^2+y^2\) can be formed cheaply as \code{zrsqr - zisqr}
and \code{zrsqr + zisqr}. This is especially valuable when \(x\) and \(y\) are
represented by multi-component expansion types.


\section{Mandelbrot base kernels}
\label{sec:mandel-base-kernels}

The goal of these kernels in \FractalShark{} is to \emph{render the Mandelbrot set}
by computing an \emph{escape-time} iteration count (\cref{sec:mandelbrot-intro}) per pixel over a 2D image
grid efficiently. Each CUDA thread evaluates a single complex parameter \(c\)
corresponding to one pixel (\cref{sec:mapping}), iterates the Mandelbrot recurrence (\cref{sec:real-form}), and stores the
iteration count into an output buffer. Separate (or fused) stages can map
iteration counts to colors, apply palettes, and perform anti-aliasing.

Across these base kernels, the primary variation is the numeric representation
used for the orbit arithmetic: from IEEE-754 \code{float}/\code{double} up
through expansion types (float-float, double-double, quad-float, quad-double)
and HDR-normalized formats. The shared objective remains the same: compute the
escape-time for the Mandelbrot iteration as accurately and efficiently as needed
for a desired zoom depth.

Each kernel variant renders the same Mandelbrot escape-time field; the only
difference is the numeric type used for mapping and orbit iteration. The
following sections describe how each type realizes the same recurrence and
escape test.  This set of kernels does not use linear approximation or
perturbation; they simply evaluate the Mandelbrot iteration directly in the
chosen numeric format.

Throughout, \code{IterType} is the integer type used to store the escape-time
iteration count (e.g., \code{uint32\_t} or \code{uint64\_t}).

\subsection{Kernel: \code{mandel\_1x\_float}}
\label{sec:mandel-1x-float}

\subsubsection{Numeric type}
This variant uses IEEE-754 single precision \code{float}. It provides the
highest throughput but limits usable zoom depth due to rounding error and loss
of significance in \(c\) and the orbit.

\subsubsection{FMA-based orbit update}
The implementation uses fused multiply-add intrinsics:

\begin{verbatim}
ytemp = __fmaf_rd(-y, y, x0);     // x0 - y^2
xtemp = __fmaf_rd(x, x, ytemp);   // x^2 - y^2 + x0
xtemp2 = 2.0f * x;
y = __fmaf_rd(xtemp2, y, y0);     // 2xy + y0
x = xtemp;
\end{verbatim}

These intrinsics correspond exactly to:

\begin{align}
x &\leftarrow x^2 - y^2 + x_0,\\
y &\leftarrow 2xy + y_0.
\end{align}

Using FMA reduces intermediate rounding and can improve performance. The
\code{\_\_fmaf\_rd} variant rounds downward; if IEEE round-to-nearest is desired,
use \code{\_\_fmaf\_rn} (or plain \code{fmaf}).

\subsubsection{Escape test}
The kernel tests \(x^2+y^2 < 4\), which implies recomputing squares each loop in
this simplest variant.  Perhaps this kernel could explicitly cache the squares
for better performance.

\subsection{Kernel: \code{mandel\_1x\_double}}
\label{sec:mandel-1x-double}

\subsubsection{Numeric type}
This variant uses IEEE-754 \code{double} and therefore carries substantially
more mantissa precision than float. The Mandelbrot recurrence is identical, but
performance depends strongly on GPU FP64 throughput.  Consumer GPUs often
have much lower FP64 throughput than FP32, so this kernel may be slower than
\code{mandel\_1x\_float} on such hardware.

\subsubsection{Orbit update and escape test}
The update uses double-precision FMA intrinsics (e.g., \code{\_\_fma\_rd}) or
equivalent arithmetic to compute:
\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]
with the same bailout condition \(|z|^2 \ge 4\).

\subsection{Kernel: \code{mandel\_2x\_float}}
\label{sec:mandel-2x-float}

\subsubsection{Numeric type: float-float expansion}
This variant uses a float-float expansion type \code{dblflt} to represent a
value as:
\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]
where \code{head} (hi) carries the leading magnitude and \code{tail} (lo) is a
correction term. Arithmetic uses compensated routines such as
\code{add\_dblflt}, \code{sub\_dblflt}, \code{mul\_dblflt}, \code{sqr\_dblflt},
and often a specialized \code{mul\_dblflt2x(x,y)} to compute \(2xy\) with good
accuracy.

All double- and quad- float/double implementations are based on work from Andrew
Thall \cite{andrew-thall-dblflt}.  The implementations are modified here to
support both float and double base types.  These changes are unique to
\FractalShark{}.

\subsubsection{Mapping and orbit iteration}

The affine mapping for \(c\) is performed in \code{dblflt}:

\[
x_0 = cx + dx \cdot X,\qquad y_0 = cy + dy \cdot Y,
\]

and the orbit update follows the same real-form recurrence using expansion
operations. Cached squares are typically maintained as \code{dblflt}:

\[
zrsqr = x^2,\quad zisqr = y^2.
\]

\subsubsection{Escape test}
The implementations compares only the leading component (e.g., \code{head}) for
speed:

\[
zrsqr.\mathrm{head} + zisqr.\mathrm{head} < 4.
\]

This approach is fast but can misclassify points extremely near the boundary. A
fully robust bailout can incorporate both components (or a conservative bound).

\subsection{Kernel: \code{mandel\_2x\_double}}
\label{sec:mandel-2x-double}

\subsubsection{Numeric type: double-double expansion}
This variant uses a double-double type \code{dbldbl} representing:

\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]

with both components in double precision, yielding \(\sim\)106 bits of precision
in favorable cases. This approach enables deeper zoom while retaining a
structure similar to the float-float kernel.

\subsubsection{Mapping, orbit update, and escape test}

The kernel computes the same affine mapping and iterates the same recurrence,
but with double-double arithmetic. The escape predicate also uses the leading
component for speed.

\subsection{Kernel: \code{mandel\_4x\_float}}
\label{sec:mandel-4x-float}

\subsubsection{Numeric type: quad-float (4-term expansion)}

This variant uses a four-float expansion type \code{GQF::gqf\_real}:

\[
a \approx a_0 + a_1 + a_2 + a_3,
\]

with decreasing-magnitude components. Pixel coordinates and constants are lifted
into this type (e.g., \code{make\_qf(X,0,0,0)}), then the affine mapping and
orbit update are performed in quad-float arithmetic.  The implementation of this
numeric type is also based on Andrew Thall's work \cite{andrew-thall-dblflt}
with some minor changes specific to \FractalShark{}.

\subsubsection{Orbit update and escape test}
The update implements:

\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]

using quad-float operations (including specialized square and power-of-two
multiply helpers). The escape test can be performed in the full quad-float type:

\[
zrsqr + zisqr \le 4.
\]

\subsection{Kernel: \code{mandel\_4x\_double}}
\label{sec:mandel-4x-double}

\subsubsection{Numeric type: quad-double (4-term expansion)}
This variant uses a four-double expansion type \code{GQD::gqd\_real}:
\[
a \approx a_0 + a_1 + a_2 + a_3,\qquad a_k\in\mathbb{R}_{double}.
\]
It supports very deep zoom rendering with high numerical stability.

\subsubsection{Mapping, orbit update, and escape test}
The affine mapping and orbit update are evaluated in quad-double arithmetic. A
literal bailout constant (e.g., \code{4.0}) is promoted via overloads, so the
escape compare remains a full-precision comparison in the quad-double domain.

\subsection{Kernel: \code{mandel\_hdr\_float}}
\label{sec:mandel-hdr-float}

\subsubsection{Numeric type: HDR-normalized expansion}
This variant uses an HDR wrapper around an expansion type, e.g.:

\[
\code{HDRFloat<CudaDblflt<dblflt>>}.
\]

This implementation has no practical value because precision is limited by the
base expansion type; however, it serves as a testbed for HDR arithmetic in the
Mandelbrot context.  Thus, this kernel is primarily of academic interest.

\subsubsection{Reduction and stable comparisons}

The kernel frequently calls \code{HdrReduce()} on intermediate values. These
reductions are part of the numeric contract: norms and comparisons are assumed
to be applied to reduced/normalized values, enabling specialized comparators
without repeatedly materializing primitive scalars.

Instead of testing \(x^2+y^2 < 4\) in primitive form, the kernel maintains:

\[
zsq\_sum = zrsqr + zisqr
\]

and checks escape via a reduced comparator:

\begin{verbatim}
while (zsq_sum.compareToBothPositiveReduced(Four) < 0)
\end{verbatim}

This directly supports escape-time Mandelbrot rendering in HDR arithmetic while
keeping comparisons meaningful and stable.

\section{Iteration chunking via \code{iteration\_precision}}
\label{sec:chunking}

A few of the base kernels just described (\cref{sec:mandel-base-kernels}), which exclude linear approximation or
perturbation, are templated on an integer \code{iteration\_precision}
(\(1,2,4,8,16\)) and unroll multiple Mandelbrot steps inside the loop:

\begin{itemize}
  \item Each loop iteration performs \code{iteration\_precision} updates.
  \item The counter \code{iter} increases by that amount.
  \item The maximum iteration \code{n\_iterations} is adjusted so \code{iter}
        does not exceed the requested limit.
\end{itemize}

This approach reduces loop overhead. The trade-off is that the escape predicate
is typically checked only once per chunk; therefore the reported escape
iteration can be larger than the true first-escape iteration by up to
\code{iteration\_precision - 1}. For strict escape iteration counts (e.g., for
continuous/smooth coloring based on the first bailout), use a chunk size of 1 or
insert bailout checks within the unrolled body.

This optimization was mainly explored for educational purposes; in practice, the
benefit is small compared to other optimizations such as perturbation (\cref{sec:perturbation-concept}) and linear
approximation (\cref{sec:approx-accel}).

\section{Perturbation Rendering of the Mandelbrot Set}
\label{sec:perturbation-concept}

Rendering the Mandelbrot set (\cref{sec:mandelbrot-intro}) at extreme zoom levels presents a fundamental
numerical challenge. As the image is magnified, the parameters \(c\) associated
with individual pixels differ by increasingly small amounts, while the number
of iterations required to determine escape behavior typically grows. Accurately
tracking these orbits therefore requires both high numerical precision and a
large number of iterations, making naive per-pixel evaluation prohibitively
expensive.

Perturbation provides a mathematical and algorithmic framework that addresses
this challenge by exploiting the strong coherence between nearby orbits. Rather
than evaluating each pixel independently at full precision, perturbation
separates the computation into a shared high-precision component and many
pixel-local low-precision components. This section introduces the perturbation
approach and explains how it enables efficient deep-zoom Mandelbrot rendering.

\subsection{Baseline iteration and its limitations}

The Mandelbrot set is defined by the iteration
\begin{equation}
z_{n+1} = z_n^2 + c, \qquad z_0 = 0,
\end{equation}
where \(c \in \mathbb{C}\) is the parameter associated with a pixel and the orbit
\(\{z_n\}\) is iterated until either escape is detected or a maximum iteration
count is reached.

At modest zoom levels, this iteration can be evaluated accurately using standard
floating-point arithmetic. At deep zoom levels, however, the differences between
nearby parameters \(c\) may be many orders of magnitude smaller than the values
of \(z_n\) encountered during iteration. In such regimes, rounding error and
loss of significance make fixed-precision arithmetic unreliable. While arbitrary
precision arithmetic can restore accuracy, applying it independently to every
pixel scales poorly.

\subsection{Coherence of nearby orbits}

A key observation underlying perturbation is that nearby parameters generate
orbits that remain close for many iterations. In a typical image tile, all pixel
parameters \(c\) lie within a small neighborhood of a central value. Their
orbits therefore share a common large-scale structure, differing only by small
corrections that grow gradually over time.

Perturbation makes this structure explicit by selecting a single
\emph{reference parameter} and expressing all nearby orbits relative to it. This
transforms the problem from one of many independent high-precision computations
into one of a single high-precision computation plus many low-precision updates.

\subsection{Reference orbit}

Let \(c_\star\) denote a reference parameter, typically chosen as the center of
the image region. Its orbit is computed using sufficiently high precision to
serve as a reliable baseline:
\begin{equation}
z_{n+1}^\star = (z_n^\star)^2 + c_\star, \qquad z_0^\star = 0.
\end{equation}
The sequence \(\{z_n^\star\}\) is stored and reused for all pixels within the
region. Because this orbit underpins the evaluation of many pixels, its
accuracy is critical.

\subsection{Perturbation as a mixed-precision decomposition}
\label{subsec:perturbation-mixed-precision}

For a nearby pixel parameter \(c = c_\star + \Delta c\), the orbit can be written
as a small deviation from the reference orbit:
\begin{equation}
z_n = z_n^\star + \Delta z_n.
\end{equation}
Substituting this expression into the Mandelbrot recurrence yields an exact
update rule for the deviation:
\begin{equation}
\Delta z_{n+1}
= 2 z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturb-exact}
\end{equation}
No approximation has been introduced: as long as the reference orbit is computed
accurately, this formulation preserves the exact Mandelbrot dynamics.

This decomposition has an important numerical consequence. The reference orbit
\(z_n^\star\) typically grows in magnitude and requires arbitrary precision to be
represented accurately at deep zoom. In contrast, the deviation \(\Delta z_n\)
remains small for many iterations and can usually be evolved using much lower
precision arithmetic. Perturbation therefore reorganizes the computation into a
mixed-precision pipeline: expensive high-precision arithmetic is concentrated in
a single shared orbit, while the majority of per-pixel work is performed using
lower precision with significantly higher throughput.

\subsection{Real-valued formulation for implementation}

For practical implementation, particularly on GPUs, it is convenient to express
the perturbation update in real arithmetic. Writing
\begin{align*}
z_n^\star &= x_n^\star + i y_n^\star, \\
\Delta z_n &= \Delta x_n + i \Delta y_n, \\
\Delta c &= \Delta c_x + i \Delta c_y,
\end{align*}
and expanding \cref{eq:perturb-exact} yields
\begin{align}
\Delta x_{n+1}
&= 2(x_n^\star \Delta x_n - y_n^\star \Delta y_n)
   + (\Delta x_n^2 - \Delta y_n^2)
   + \Delta c_x, \\
\Delta y_{n+1}
&= 2(x_n^\star \Delta y_n + y_n^\star \Delta x_n)
   + 2 \Delta x_n \Delta y_n
   + \Delta c_y.
\end{align}
These equations map directly onto scalar arithmetic and can be evaluated
efficiently in parallel for many pixels.

\subsection{Reconstruction and escape testing}

Although perturbation evolves only the deviation \(\Delta z_n\), escape-time
rendering requires testing the magnitude of the full orbit value. At each
iteration, the current iterate is reconstructed as
\begin{equation}
z_n = z_n^\star + \Delta z_n,
\end{equation}
and the standard Mandelbrot bailout condition is applied:
\begin{equation}
|z_n|^2 \ge 4.
\end{equation}
The iteration at which this condition is first satisfied determines the pixel’s
escape time.

\subsection{Numerical stability and rebasing}

Perturbation remains efficient only while the deviation \(\Delta z_n\) remains
small relative to the reference orbit. As the deviation grows, numerical
cancellation and loss of significance may occur. Practical implementations
therefore employ \emph{rebasing},\footnote{I believe Zhuoran discovered this
technique but don't have a definitive reference} in which the current deviation
is folded into the reference state and perturbation continues relative to this
new baseline. This process preserves accuracy while maintaining numerical
stability.

\subsection{Relation to other acceleration techniques}

Perturbation preserves the exact dynamics of the Mandelbrot map and forms the
foundation of many advanced deep-zoom rendering techniques. It can be used by
itself (\cref{sec:perturb-only}) or combined with additional approximations, such as bilinear approximation
(\cref{sec:bilinear-approx}) or linear approximation (\cref{sec:la-v2-perturb}),
to skip multiple iterations at once. In all cases, perturbation provides
the conceptual framework that makes efficient deep-zoom Mandelbrot rendering
possible.

\paragraph{Terminology note: HDR scalar representation.}
In this work, the term \emph{HDR} refers to a \emph{high-dynamic-range} numeric
representation that pairs a fixed-precision mantissa (float or double) with an
explicitly managed integer exponent (see below for the full description).  When the GPU reference-orbit kernel or
perturbation pipeline needs lightweight control-flow decisions---periodicity
detection, escape testing, magnitude comparisons---it converts the full
arbitrary-precision state into an HDR value via a lossy reduction step
(\texttt{ToHDRFloat}).  The resulting HDR value preserves a wide exponent range,
sufficient to compare magnitudes spanning many orders of magnitude, but carries
only the fixed mantissa precision of the underlying float type rather than the
full arbitrary-precision mantissa.  The use of the term HDR here is purely
numerical and is unrelated to high-dynamic-range imaging.

\section{HDR Floating-Point Representation}
\label{sec:hdr-float}

Deep-zoom Mandelbrot rendering places unusual demands on numerical
representation. Orbit values may span an extreme dynamic range over the course
of iteration, while small relative differences between nearby orbits must be
tracked accurately. Standard floating-point formats are poorly matched to this
combination of requirements: fixed-precision mantissas limit relative accuracy
at large magnitudes, while arbitrary-precision formats incur prohibitive
computational cost when used per pixel.

To address this gap, we employ a custom \emph{high dynamic range (HDR)}
floating-point representation that explicitly separates scale and precision.
This representation is designed to preserve relative accuracy across a wide
range of magnitudes while remaining efficient on GPUs.

\subsection{Decoupling scale and precision}

In standard IEEE floating-point arithmetic, each value is represented as a
mantissa multiplied by a power of two, with both components stored implicitly in
a single word. While convenient, this tightly couples scale and precision: as
values grow in magnitude, fewer bits remain available to represent small
relative differences.

The HDR representation instead stores values in the form
\begin{equation}
x = m \cdot 2^{e},
\end{equation}
where:
\begin{itemize}
\item \(m\) is a fixed-precision mantissa stored in standard floating-point or
      double-double format, and
\item \(e\) is an explicitly managed integer exponent.
\end{itemize}

By storing the exponent separately, the mantissa can remain centered near unit
magnitude regardless of the overall scale of the value. This preserves relative
precision even when the absolute value becomes extremely large or small.

\subsection{Normalization and arithmetic}

HDR arithmetic maintains the invariant that the mantissa remains within a fixed
normalized range. After each arithmetic operation, the mantissa is renormalized
and any excess scaling is folded into the exponent. Conceptually, this mirrors
the behavior of floating-point normalization, but with explicit control over the
process.

Arithmetic operations are performed as follows:
\begin{itemize}
\item \textbf{Multiplication} combines mantissas and adds exponents.
\item \textbf{Addition and subtraction} align exponents explicitly before
      operating on mantissas.
\item \textbf{Renormalization} ensures that mantissas remain well-scaled and
      numerically stable.
\end{itemize}

Because mantissas are stored using fixed-precision types, these operations map
naturally to GPU hardware while avoiding the overhead of general-purpose
arbitrary-precision arithmetic.

\subsection{HDR complex numbers}

Mandelbrot iteration operates on complex numbers. In the HDR formulation, each
complex value
\[
z = x + i y
\]
is represented by storing separate HDR values for the real and imaginary
components. Both components share the same conceptual structure but may have
independent exponents.

This representation allows the magnitude of \(z\) to grow or shrink over many
orders of magnitude while maintaining accurate relative phase and magnitude
information, which is critical for both escape testing and perturbation-based
methods.

\subsection{Role of HDR in perturbation and reference orbits}

The HDR representation plays a complementary role to perturbation (\cref{sec:perturbation-concept}). In the
perturbation framework, the reference orbit (\cref{sec:ref-orbit-calc}) typically grows rapidly in magnitude
and therefore demands high dynamic range, while perturbation deltas remain small
and can often be represented using lower precision.

HDR arithmetic is used primarily for:
\begin{itemize}
\item computing reference orbits at deep zoom levels,
\item evaluating linear and nonlinear terms involving large reference values,
\item maintaining numerical stability across long iteration sequences.
\end{itemize}

By decoupling scale from precision, HDR arithmetic allows the reference orbit to
be computed efficiently without sacrificing accuracy, while still enabling the
mixed-precision structure exploited by perturbation rendering.

\subsection{Advantages over conventional representations}

Compared to standard floating-point formats, HDR arithmetic offers:
\begin{itemize}
\item substantially increased dynamic range,
\item stable relative precision independent of magnitude,
\item predictable and controllable numerical behavior.
\end{itemize}

Compared to arbitrary-precision libraries, it avoids dynamic memory allocation,
irregular control flow, and large per-operation overheads, making it well suited
for massively parallel execution on GPUs.

\subsection{Summary}

The HDR floating-point representation provides a numerically robust and
computationally efficient foundation for deep-zoom Mandelbrot rendering. By
explicitly separating scale and precision, it bridges the gap between standard
floating-point arithmetic and full arbitrary-precision methods. In combination
with perturbation, it enables accurate, high-performance rendering across
extreme zoom levels while remaining compatible with GPU execution models.


\section{Reference Orbit Calculation}
\label{sec:ref-orbit-calc}

This section describes how \texttt{RefOrbitCalc} constructs (and optionally 
reuses) a high-precision \emph{reference orbit} for perturbation rendering (\cref{sec:perturbation-concept}) of 
the quadratic map

\begin{equation}
  z_{n+1} = z_n^2 + c,\qquad z,c\in\mathbb{C},
\end{equation}

with an implementation that supports single-threaded CPU, multi-threaded CPU,
and GPU backends, plus several storage/compression modes that trade memory
footprint against recomputation.

\subsection{High-level pipeline and dispatch}
\label{subsec:ref-orbit-pipeline}

A reference orbit is stored in a \texttt{PerturbationResults<IterType,T,PExtras>}
instance, where:

\begin{itemize}
  \item \texttt{IterType} is the iteration index type (\texttt{uint32\_t} or \texttt{uint64\_t}).
  \item \texttt{T} is the low-precision numeric type used for downstream perturbation math (e.g.\ \texttt{float},
        \texttt{double}, \texttt{HDRFloat<...>}).
  \item \texttt{PExtras} selects the storage format for per-iteration orbit data:
        uncompressed (\texttt{Disable}), a lightweight compressor (\texttt{SimpleCompression}),
        or additional diagnostic bookkeeping (e.g.\ \texttt{Bad}).
\end{itemize}

Orbit construction is initiated through \texttt{AddPerturbationReferencePoint()}, which:

\begin{enumerate}
  \item Picks an initial guess \((c_x,c_y)\) (center of the current view if unset).
  \item Chooses an algorithm (\texttt{ST}, \texttt{MT}, reuse-based hybrids, or \texttt{GPU}) based on
        \texttt{m\_PerturbationAlg} and zoom factor heuristics.
  \item Allocates a new \texttt{PerturbationResults} slot, initializes metadata and bounds, and runs the chosen
        orbit kernel until escape, periodicity detection, or the maximum iteration count is reached.
\end{enumerate}

To control memory pressure, \texttt{OptimizeMemory()} monitors process commit 
usage and opportunistically drops cached orbits that are not of the currently 
demanded variant type when the working set exceeds a configurable threshold.

\subsection{Single-threaded authoritative orbit}
\label{subsec:ref-orbit-st}

The single-threaded path (\texttt{AddPerturbationReferencePointST}) computes 
the authoritative orbit directly using MPIR/GMP \texttt{mpf\_t} for the 
recurrence, while simultaneously emitting a low-precision shadow copy \((\hat{x}
_n,\hat{y}_n)\in T^2\) for downstream work (compression, bailout checks, 
periodicity tests).

\paragraph{State and initialization.}

Given a selected reference parameter \(c = c_x + i c_y\), the implementation:

\begin{itemize}
  \item Initializes \texttt{mpf\_t} temporaries for \(x,y,x^2\), and scratch products.
  \item Sets the initial iterate \((x_0,y_0) = (c_x,c_y)\) (this code uses the common convention \(z_0=c\)).
  \item Computes low-precision cast values \(\hat{c}_x,\hat{c}_y \in T\) either via \texttt{mpf\_get\_d} for
        native float/double, or via a mantissa/exponent extraction for extended formats.
\end{itemize}

\paragraph{Recurrence.}
Writing \(z_n = x_n + i y_n\), one iteration evaluates
\begin{align}
  x_{n+1} &= x_n^2 - y_n^2 + c_x, \\
  y_{n+1} &= 2 x_n y_n + c_y.
\end{align}
The implementation uses:
\begin{itemize}
  \item \texttt{mpf\_mul} and \texttt{mpf\_sub}/\texttt{mpf\_add} for the high-precision update.
  \item A low-precision snapshot \((\hat{x}_n,\hat{y}_n)\) acquired once per iteration for storage/compression,
        periodicity heuristics, and bailout checks.
\end{itemize}

\paragraph{Bailout.}

The bailout threshold is evaluated in low precision using

\begin{equation}
  \|\hat{z}_n + \hat{c}\|^2 = (\hat{x}_n + \hat{c}_x)^2 + (\hat{y}_n + \hat{c}_y)^2 > 256,
\end{equation}

which matches the code's use of \texttt{TwoFiftySix} and avoids a high-precision
norm each step.  Here \(\hat{x}_n\) and \(\hat{y}_n\) are the low-precision
shadows of the orbit state captured \emph{before} the high-precision recurrence
update (i.e.\ they represent the current \(z_n\), not the just-computed
\(z_{n+1}\)).  The sum \(\hat{z}_n + \hat{c}\) is therefore a cheap proxy that
avoids re-extracting a low-precision snapshot after the update.
Note that the bailout here is different from the one used in
the lower-precision kernels discussed previously; this is acceptable because the
reference orbit is used only for perturbation, not direct rendering.

\subsection{Periodicity tracking via \texorpdfstring{$\partial z/\partial c$}{dz/dc}}
\label{subsec:ref-orbit-periodicity}

Several modes enable periodicity detection. The implementation tracks the complex derivative

\(\frac{\partial z_n}{\partial c}\) in low precision:
\begin{equation}
  d_{n+1} = 2 z_n d_n + 1,\qquad d_0 = 0.
\end{equation}
Mathematically \(d_0 = \partial z_0/\partial c = 0\), since \(z_0=0\).
Because the implementation begins its loop at \(z_1=c\) (folding in the
first iteration), the code variable \texttt{dzdcX} is initialized to~$1$,
which corresponds to \(d_1=1\) in the recurrence above.

with \(d_n = d_{x,n} + i d_{y,n}\). Expanding into real and imaginary parts yields:

\begin{align}
  d_{x,n+1} &= 2(x_n d_{x,n} - y_n d_{y,n}) + 1, \\
  d_{y,n+1} &= 2(x_n d_{y,n} + y_n d_{x,n}).
\end{align}

The code applies a radius-based heuristic: let

\begin{equation}
  n_2 = \max(|\hat{x}_n|,|\hat{y}_n|),\qquad
  r_0 = \max(|d_{x,n}|,|d_{y,n}|),
\end{equation}

and define a detection threshold

\begin{equation}
  n_3 = 2\,R_{\max}\,r_0,
\end{equation}

where \(R_{\max}\) is the maximum perturbation radius stored in \texttt{results}
. If \(n_2 < n_3\), the orbit is marked as \emph{maybe periodic} and the
reference loop terminates early (unless periodicity detection is disabled).
Otherwise, \((d_{x,n},d_{y,n})\) is advanced using the update above.
See \cref{sec:hp-periodicity} for the analogous GPU-side periodicity mechanism.

\subsection{Expanded versus factored evaluation of the reference orbit}

The high-precision reference orbit evaluates the quadratic map

\[
z_{n+1} = z_n^2 + c,
\qquad
z_n = x_n + i y_n,
\qquad
c = c_x + i c_y,
\]

with full arbitrary-precision arithmetic.

Writing the iteration in real and imaginary components gives the
\emph{expanded form}:

\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + c_x, \label{eq:ref-expanded-real} \\
y_{n+1} &= 2 x_n y_n + c_y. \label{eq:ref-expanded-imag}
\end{align}

This form follows directly from the algebraic definition of the polynomial.
Each term is evaluated explicitly, requiring two full-precision squares and
one full-precision multiplication per iteration.

The same quadratic map can be evaluated in a mathematically equivalent
\emph{factored form}. In particular, the real component may be written as

\begin{align}
x_{n+1}
&= (x_n - y_n)(x_n + y_n) + c_x, \label{eq:ref-factored-real}
\end{align}
while the imaginary component remains
\begin{align}
y_{n+1} &= 2 x_n y_n + c_y. \label{eq:ref-factored-imag}
\end{align}

In exact arithmetic, \cref{eq:ref-factored-real,eq:ref-factored-imag} are
identical to \cref{eq:ref-expanded-real,eq:ref-expanded-imag}. The difference
lies solely in how the products are formed.

From an implementation perspective, the expanded form evaluates
$x_n^2$ and $y_n^2$ independently and makes the subtraction
$x_n^2 - y_n^2$ explicit. This closely mirrors the mathematical definition
of the Mandelbrot polynomial and exercises the full squaring and
multiplication paths of the high-precision arithmetic.

The factored form reduces the number of full-precision squares by replacing
$x_n^2 - y_n^2$ with a single multiplication of the shared intermediates
$(x_n \pm y_n)$. This can reduce computational cost when multiplication and
squaring have similar expense, but it introduces stronger coupling between
terms and alters the way cancellation and rounding effects manifest in
finite-precision arithmetic.

In all cases, \FractalShark{} uses the expanded form despite the advantages offered
by the factored form. This choice simplifies reasoning about numerical behavior,
ensures that all arithmetic paths are exercised, and maintains consistency with
the perturbation update form used elsewhere in the codebase.  For better
performance it likely makes sense to implement both forms and compare their behavior
empirically.

\section{Compression and Reference Orbit Reuse Modes}
\label{subsec:ref-orbit-compression-reuse}

Two orthogonal storage decisions are made while iterating:

\begin{enumerate}
  \item \textbf{Orbit storage} for perturbation use (\texttt{PExtras}):
  \begin{itemize}
    \item \texttt{Disable}: store every \((\hat{x}_n,\hat{y}_n)\) uncompressed.
    \item \texttt{SimpleCompression}: store a compressed subset of iterations using an error exponent
          determined by \texttt{Fractal::CompressionError}.
    \item \texttt{Bad}: store orbit values plus underflow/diagnostic flags.
  \end{itemize}
  \item \textbf{Reuse storage} for intermediate-precision regeneration (\texttt{ReuseMode}):
  \begin{itemize}
    \item \texttt{SaveForReuse1/2}: store uncompressed \texttt{mpf\_t} reuse entries.
    \item \texttt{SaveForReuse3}: store an intermediate-compressed reuse stream.
    \item \texttt{SaveForReuse4}: store a maximally-compressed intermediate reuse stream.
  \end{itemize}
\end{enumerate}

The next two sections describe these mechanisms in more detail.

\subsection{Reuse-based orbit regeneration}
\label{subsec:ref-orbit-reuse}

We consider iteration of the quadratic map
\[
f_c(z) = z^2 + c,\qquad z,c \in \mathbb{C},
\]
and distinguish between an \emph{authoritative} reference orbit computed in very high precision and a hierarchy of
\emph{intermediate precision} reference orbits constructed via perturbation from that authoritative orbit.
The central motivation for this construction is performance at extreme zoom depths: while the authoritative orbit
is expensive to compute, it need only be regenerated infrequently as the view parameter drifts.
Intermediate precision orbits, by contrast, are substantially cheaper to evaluate and can be reused across many
incremental zoom steps before their accuracy envelope is exceeded.

\subsubsection{Authoritative Reference Orbit}

\paragraph{Notation.}
The earlier perturbation sections
(\cref{sec:perturbation-concept,sec:perturb-only})
write the reference orbit and parameter as
\(z_n^\star\) and \(c_\star\).
Here we use the indexed notation
\(z^{(0)}_n\) and \(c_0\) instead, so that the superscript can
distinguish the authoritative level~\(0\) from intermediate
levels~\(1,2,\ldots\) in the reuse hierarchy.
The two conventions describe the same object:
\(z^{(0)}_n \equiv z_n^\star\) and \(c_0 \equiv c_\star\).

Let
\[
c_0 \in \mathbb{C}
\]
denote the authoritative reference parameter.
The corresponding authoritative reference orbit is defined by
\begin{equation}
z^{(0)}_0 = 0, \qquad
z^{(0)}_{n+1} = \bigl(z^{(0)}_n\bigr)^2 + c_0,
\label{eq:authoritative_orbit}
\end{equation}
and is computed using sufficiently high precision that it is treated as exact for all practical purposes.
This orbit constitutes the single source of truth for all subsequent perturbative constructions.

At very deep zooms, recomputing~\eqref{eq:authoritative_orbit} at every navigation step would dominate runtime,
as the required precision grows with zoom depth.
The reuse-based framework therefore seeks to minimize how often this authoritative orbit must be recalculated,
while still maintaining numerical correctness.

\subsubsection{Perturbation Formulation}

For any nearby parameter
\[
c = c_0 + \Delta c,
\]
the corresponding orbit may be written as
\[
z_n(c) = z^{(0)}_n + \Delta z_n,
\]
where $\Delta z_n$ represents the perturbation relative to the authoritative orbit.
Substituting into the defining recurrence yields the exact perturbation equation
\begin{align}
\Delta z_0 &= 0, \\
\Delta z_{n+1}
&= 2 z^{(0)}_n \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturbation_exact}
\end{align}
Equation~\eqref{eq:perturbation_exact} is the fundamental relation used both to construct intermediate reference orbits
and to evaluate per-pixel perturbations during rendering.

\subsubsection{Intermediate Precision Reference Orbits}

An intermediate precision reference orbit is defined at a parameter
\[
c_1 = c_0 + \Delta c_1,
\]
where $\Delta c_1$ is chosen such that the perturbation $\Delta z^{(1)}_n$ remains bounded within a fixed target accuracy
(e.g.\ absolute error $\lesssim 10^{-100}$) when represented in a chosen intermediate precision arithmetic.

The intermediate reference orbit is defined by
\[
z^{(1)}_n \equiv z^{(0)}_n + \Delta z^{(1)}_n,
\]
where $\Delta z^{(1)}_n$ is obtained by iterating
\begin{equation}
\Delta z^{(1)}_{n+1}
= 2 z^{(0)}_n \Delta z^{(1)}_n
  + \bigl(\Delta z^{(1)}_n\bigr)^2
  + \Delta c_1
\label{eq:intermediate_perturbation}
\end{equation}
in the intermediate precision format.

Crucially, the cost of evaluating~\eqref{eq:intermediate_perturbation} is dramatically lower than that of recomputing
the authoritative orbit~\eqref{eq:authoritative_orbit}.
As a result, once an intermediate reference orbit has been established, it may be reused across many subsequent zoom
or pan operations, as long as the view parameter remains within its validity radius.
Only when accumulated drift causes the perturbation to exceed the fixed accuracy envelope does a new authoritative
orbit need to be computed.

In the single-thread execution path (beginning at
\texttt{AddPerturbationReferencePointST}), this construction is performed sequentially using the cached authoritative
orbit samples $\{z^{(0)}_n\}$ as coefficients.
No approximation beyond finite-precision rounding is introduced; the intermediate orbit is mathematically equivalent
to directly iterating $f_{c_1}$, subject only to the chosen precision bound.

\subsubsection{Per-Pixel Perturbation from an Intermediate Orbit}

For an individual pixel parameter
\[
c_{\text{px}} = c_1 + \delta c,
\]
with $|\delta c| \ll |\Delta c_1|$, the final orbit is expressed as
\[
z_n(c_{\text{px}}) = z^{(1)}_n + \delta z_n,
\]
where $\delta z_n$ satisfies
\begin{equation}
\delta z_{n+1}
= 2 z^{(1)}_n \delta z_n
  + (\delta z_n)^2
  + \delta c.
\label{eq:pixel_perturbation}
\end{equation}
Because $|\delta c|$ is small, $\delta z_n$ remains well within the same fixed precision envelope used for the
intermediate reference orbit.

From a performance perspective, this two-level perturbation hierarchy is
beneficial when interacting with the Mandelbrot. The expensive high-precision
authoritative orbit is amortized over many intermediate orbits, and each
intermediate orbit in turn supports an entire image worth of per-pixel
perturbations. As zoom depth increases, most navigation steps therefore reuse
existing intermediate data, with authoritative recomputation occurring only
sporadically.

The two-level approach nevertheless does have an important downside: instead of
maintaining only a low-precision copy of the reference orbit, the system must also
store the authoritative orbit samples $\{z^{(0)}_n\}$ to support intermediate
reconstruction.  This increases memory usage and data transfer requirements,
potentially impacting performance.

\subsubsection{SaveForReuse Modes}

Both \texttt{SaveForReuse1} and \texttt{SaveForReuse2} correspond to the mathematical framework described above.
They differ only in execution strategy.

\paragraph{SaveForReuse1.}
In \texttt{SaveForReuse1}, the authoritative orbit samples $\{z^{(0)}_n\}$ are stored after initial computation and reused
whenever intermediate reference orbits are constructed.
This avoids recomputation of~\eqref{eq:authoritative_orbit} and ensures that all perturbative steps are driven by the same
authoritative data.

\paragraph{SaveForReuse2.}
\texttt{SaveForReuse2} is a multithreaded optimization of the same procedure.
It performs the identical perturbation recurrences
\eqref{eq:intermediate_perturbation} and \eqref{eq:pixel_perturbation}, using the same authoritative orbit samples and
producing the same intermediate and per-pixel results.
The distinction lies solely in how data is staged, reused, and synchronized across threads in the multithreaded path.

\paragraph{Equivalence.}
From a mathematical standpoint,
\[
\texttt{SaveForReuse1} \;\equiv\; \texttt{SaveForReuse2}.
\]
Both modes define the same authoritative orbit, the same intermediate reference
orbits, and the same per-pixel perturbation orbits.
Any differences are strictly implementation-level optimizations and do not affect numerical results.
\texttt{SaveForReuse3} and \texttt{SaveForReuse4} are addressed in a subsequent section.

\subsection{Reference Compression}
\label{sec:ref-compression-zhuoran}

The idea in this section was developed by Zhuoran
\cite{Zhuoran2023ReferenceCompression} and \FractalShark{} implements it.

\subsubsection{Motivation}
For deep zoom escape-time fractals, pixel evaluation has become extremely fast
(e.g., via perturbation and GPU acceleration), while \emph{reference-orbit}
construction often remains comparatively expensive and can dominate total render
time.  Additionally, when the reference orbit is large, it can consume
significant amounts of memory.  The goal of \emph{reference compression} is to
store this potentially-large reference orbit in a compact form that can be
transmitted or cached, then \emph{reconstructed} efficiently with a guaranteed
bounded reconstruction error. The key idea is to store only a sparse set of
\emph{waypoints} and fill the gaps by recomputing intermediate iterations in
reduced precision.  This section describes the approach.

\subsubsection{High-precision reference and low-precision surrogate}
Let the (authoritative) high-precision reference orbit be
\(
z_n^{\mathrm{HP}} \in \mathbb{C}
\)
for \(n=0,\dots,N\).
During compression and decompression we also maintain a low-precision surrogate
\(
\hat z_n \in \mathbb{C}
\)
computed by iterating the same recurrence in a cheaper numeric type.

Because the dynamics are initially stable to rounding and only later amplify
numerical differences, \(\hat z_n\) typically tracks \(z_n^{\mathrm{HP}}\) for
many iterations before diverging.  This observation enables a streaming scheme:
store occasional exact anchors (waypoints) and recover all omitted states by
recomputing them in low precision between anchors.

\subsubsection{Waypoint selection criterion}
A \emph{waypoint} stores the iteration index and a corrective payload.
In the simplest mode, the payload is the full authoritative value
\(z_{n_k}^{\mathrm{HP}}\) at iteration \(n_k\).
Waypoints are chosen by simulating \(\hat z_n\) in low precision while comparing
against \(z_n^{\mathrm{HP}}\).  If the relative error exceeds a threshold, the
iteration must be retained.

A convenient test is

\begin{equation}
  \frac{\lVert z_n^{\mathrm{HP}} - \hat z_n \rVert}{\lVert z_n^{\mathrm{HP}} \rVert}
  \;>\; \varepsilon,
  \label{eq:relerr-test}
\end{equation}

where \(\lVert \cdot \rVert\) may be \(\ell_\infty\), \(\ell_2\), or another cheap norm
(consistent between compression and decompression), and \(\varepsilon\) is a
user-controlled tolerance.  Whenever the test fails, we emit a waypoint at \(n\)
and \emph{reset} the low-precision state to the authoritative value:

\begin{equation}
  \text{if waypoint at } n:\qquad \hat z_n \leftarrow z_n^{\mathrm{HP}}.
\end{equation}

Between waypoints, the compressor stores nothing, relying on \(\hat z\)-iteration
to reconstruct the missing values later.

\paragraph{Compressed representation.}
The compressed reference is thus a sparse, ordered list
\(
\mathcal{W} = \{(n_k, w_k)\}_{k=0}^{K-1}
\)
with strictly increasing indices \(0 \le n_0 < n_1 < \cdots < n_{K-1} \le N\),
where the payload \(w_k\) depends on the mode (\cref{sec:refcomp-pert}).

\subsubsection{Decompression by replay}

Decompression replays the same low-precision recurrence and applies waypoints as
hard resets:

\begin{equation}
  \hat z_{n+1} = \hat z_n^2 + c \quad\text{(low precision)},\qquad
  \text{and if } n = n_k:\ \hat z_n \leftarrow w_k.
\end{equation}

If the decompressor uses the same low-precision arithmetic, the same recurrence,
and encounters the same waypoints at the same indices, then the reconstructed
orbit matches what the compressor would have produced by replay; consequently,
the reconstruction error is bounded by the same tolerance logic used to place
waypoints.

\paragraph{On-the-fly use during rendering.}
Because decompression is just a forward scan with occasional resets, it can be
performed \emph{streaming} (no random access required), and can be interleaved
with perturbation-based pixel evaluation by iterating \(\hat z_n\) alongside the
main render loop.

\subsubsection{Perturbation-assisted compression}

\label{sec:refcomp-pert}
Waypointing only the absolute states \(z_n\) is useful but may not achieve high
compression at extreme depths, where the orbit's sensitivity forces frequent
anchors.  Compression improves substantially by switching to a perturbation form
once the orbit becomes suitable.

\paragraph{Self-referenced perturbation.}
Choose a \emph{base} (reference) iteration \(r\) and represent subsequent states as

\begin{equation}
  z_n = z_r + \Delta z_n, \qquad n \ge r,
\end{equation}

where \(z_r\) is treated as a reference and \(\Delta z_n\) is (ideally) small.
In standard perturbation rendering, \(\Delta z\) is evolved using derivatives
around a known reference orbit.  For \emph{reference compression}, the only
available reference is the reference itself, so the scheme starts in the
absolute waypoint mode (previous sections) and transitions to perturbation only
when \(\lVert \Delta z \rVert\) is sufficiently small that perturbation yields
meaningful extra precision in the reduced type.

\paragraph{Switch criterion.}
Let \(\tau\) be a user parameter.  When \(\lVert \Delta z_n \rVert < \tau\), the
compressor may begin emitting waypoints that store \(\Delta z_n\) rather than
\(z_n\).  After the switch, a waypoint payload becomes

\begin{equation}
  w_k = \Delta z_{n_k} \quad\text{(instead of } z_{n_k}\text{)}.
\end{equation}

\paragraph{Rebasing synchronization.}
Perturbation implementations commonly \emph{rebase} to prevent \(\Delta z\) from
growing too large: at some iteration \(b\), the reference is updated so that
\(\Delta z_b \leftarrow 0\) and the base index becomes \(r \leftarrow b\).
During decompression, if rebases occur at different iterations than during
compression, the decompressor will interpret stored \(\Delta z\) against the
wrong base \(z_r\), producing catastrophic corruption.

A robust compressed format therefore must encode enough information to keep
rebases in lockstep.  One approach is to include a single \emph{rebase flag} per
waypoint indicating whether a rebase occurs at that iteration; if a rebase
occurs at an iteration where no waypoint would otherwise be required, the
compressor must still record the iteration (e.g., by forcing a waypoint or
storing an index) so the decompressor can rebase at the same \(b\).

\subsubsection{Error propagation and correction}

Reconstruction introduces small errors.  Without perturbation, such errors are
often tolerable because each pixel's perturbation evaluation can be stable to
small reference drift.  However, once perturbation is used \emph{inside} the
reference reconstruction, errors can feed forward recursively: an error early in
the reconstructed reference becomes part of the base used later, and thus
contaminates subsequent \(\Delta z\) evolution.

\paragraph{Local linear model.}
Let \(e_n = z_n^{\mathrm{HP}} - \tilde z_n\) be the reconstruction error, where
\(\tilde z_n\) denotes the reconstructed (decompressed) value.  Linearizing the
map \(f(z)=z^2+c\) around the reconstructed state gives
\begin{equation}
  e_{n+1}
  \;=\; f(z_n^{\mathrm{HP}}) - f(\tilde z_n)
  \;\approx\; f'(\tilde z_n)\, e_n
  \;=\; 2\tilde z_n\, e_n.
  \label{eq:error-forward}
\end{equation}
Thus, when \(|2\tilde z_n|\) is large, errors amplify rapidly.

\paragraph{Inverse (Newton-style) correction sweep.}
At a waypoint iteration \(m\), the compressor knows the authoritative value
(either \(z_m\) or the perturbation payload consistent with the base).  The
decompressor can compute the current error at \(m\) directly:

\begin{equation}
  e_m = z_m^{\mathrm{HP}} - \tilde z_m
  \quad\text{(or the analogous difference in the perturbation representation).}
\end{equation}

Assuming the dominant source of \(e_m\) is accumulated from the previous segment,
one can approximate the error in earlier iterations by inverting the local model
\eqref{eq:error-forward}:

\begin{equation}
  e_n \;\approx\; \frac{e_{n+1}}{2\tilde z_n},
  \qquad \text{for } n=m-1,m-2,\dots.
  \label{eq:error-backward}
\end{equation}

A single backward sweep applying

\(
\tilde z_n \leftarrow \tilde z_n + e_n
\)

is equivalent to performing one Newton--Raphson-style correction pass over the
segment to enforce consistency with the waypoint constraint at \(m\).  In
practice this substantially reduces recursive drift and allows larger gaps
between waypoints in perturbation mode.

\paragraph{Compressor constraint.}
To keep the scheme well-defined, the compressor must ensure that iterations not
yet eligible for correction are never used as a perturbation base.  Operationally:
when the current perturbation base index approaches the end of the corrected
region, the compressor inserts a new waypoint so the decompressor has an
opportunity to correct that segment before it would be used as a base for future
\(\Delta z\) evolution.

\subsubsection{Algorithm sketch}

The following high-level procedure summarizes the method.

\begin{enumerate}
  \item \textbf{Compression pass:}
    \begin{enumerate}
      \item Initialize low-precision \(\hat z_0 \leftarrow 0\); iterate forward.
      \item At each \(n\), evaluate relative error \eqref{eq:relerr-test}.
      \item If error too large, emit waypoint \((n, z_n^{\mathrm{HP}})\) (or \((n,\Delta z_n)\) after perturbation switch),
            set \(\hat z_n \leftarrow z_n^{\mathrm{HP}}\) (or reset perturbation state), and continue.
      \item If in perturbation mode, record/synchronize any rebase events (bit flag or forced waypoint).
      \item Insert additional waypoints as needed to guarantee correction opportunities before uncorrected
            states would become perturbation bases.
    \end{enumerate}

  \item \textbf{Decompression pass:}
    \begin{enumerate}
      \item Replay the same low-precision recurrence, applying waypoints as resets.
      \item In perturbation mode, rebase exactly when signaled by the compressed stream.
      \item When a waypoint is reached, compute the current error and apply a backward correction sweep
            using \eqref{eq:error-backward} over the segment since the previous waypoint.
    \end{enumerate}
\end{enumerate}

\subsubsection{Practical tradeoffs}

Reference compression exposes a clear accuracy--size--time trade:
\begin{itemize}
  \item Smaller tolerance \(\varepsilon\) \(\Rightarrow\) more waypoints \(\Rightarrow\) larger compressed size but tighter fidelity.
  \item Larger gaps reduce storage but increase decompression work and may require more frequent correction points,
        especially in perturbation mode.
  \item Perturbation-mode waypoints (\(\Delta z\)) can greatly improve compression when \(\Delta z\) remains small for long
        stretches, but require careful rebase synchronization and correction to avoid catastrophic desynchronization.
\end{itemize}

Overall, the method turns an expensive, dense high-precision reference into a
compact, streamable representation whose reconstruction cost is dominated by
cheap low-precision iteration plus sparse waypoint application, enabling reuse
of deep references across sessions, machines, or render nodes.

\subsubsection{Supporting both random and sequential access patterns}
\label{sec:refcomp-runtime-access}

Reference-orbit consumers exhibit two fundamentally different access patterns.
On the one hand, perturbation-based evaluation and on-the-fly decompression
naturally traverse the orbit sequentially, requesting
\(z_0, z_1, z_2, \dots\) in order.  On the other hand, higher-level algorithms
such as linear (or affine) approximation, reuse heuristics, and error analysis
require \emph{direct access} to arbitrary iteration indices in order to evaluate
local behavior around a chosen anchor.  A decompression strategy optimized only
for sequential replay performs poorly for such random probes, while a strategy
optimized only for random access would impose unnecessary overhead on the
dominant sequential case.

To accommodate both use cases efficiently, the runtime decompressor implements a
hybrid access scheme: binary-search-based anchoring for random access, combined
with cached linear scans for sequential access.

\paragraph{Random access via waypoint search.}
The compressed reference orbit is stored as a strictly increasing sequence of
waypoints
\(
\{(i_k, z_{i_k})\}_{k=0}^{K-1}
\),
where \(i_k\) denotes the uncompressed iteration index and \(z_{i_k}\) the saved
state.  Given a request for an arbitrary iteration \(n\), we locate the nearest
preceding waypoint
\begin{equation}
  k^\star = \max\{k \mid i_k \le n\},
\end{equation}
using a binary search over the waypoint indices in
\(\mathcal{O}(\log K)\) time.  Reconstruction then proceeds by replaying the
low-precision recurrence forward from that anchor for
\(\Delta = n - i_{k^\star}\) steps:
\begin{equation}
  z_{t+1} \leftarrow z_t^2 + c,
  \qquad t = i_{k^\star}, \dots, n-1,
\end{equation}
with the same reduction/normalization operations used during compression.  This
provides efficient, bounded-cost random access suitable for linear approximation
and analysis routines that must probe isolated iteration indices.

\paragraph{Sequential access via cached linear scans.}
In contrast, perturbation pipelines typically consume the reference orbit in
monotone order.  Repeating a binary search for every \(n+1\) would be wasteful,
so the decompressor maintains a small cache of recently reconstructed states,
including both their uncompressed and compressed indices.  When a request
targets the immediate successor of a cached iteration, the decompressor advances
the state in-place:
\begin{itemize}
  \item If the next uncompressed index lies strictly between two waypoints, a
        single recurrence step is performed.
  \item If the next index coincides with a waypoint, the cached state is replaced
        by the stored waypoint value and the compressed index is advanced.
\end{itemize}
A dual-entry cache tolerates minor access jitter by retaining both the most
recent and the previously-recent state; exact cache hits are returned
immediately, and near-sequential requests avoid both binary search and replay
from older anchors.

\paragraph{Combined complexity and practical behavior.}
This hybrid design yields the desirable properties of both approaches:
\begin{align}
  \text{sequential access} &: \quad \mathcal{O}(1)\ \text{amortized per iteration}, \\
  \text{random access} &: \quad \mathcal{O}(\log K + \Delta),
\end{align}
where \(\Delta\) is the distance from the nearest preceding waypoint.  In typical
compressed references, \(\Delta\) is small relative to the full orbit length,
while sequential traversal dominates overall access volume.  The result is a
single runtime decompressor that efficiently supports both perturbation-driven
sequential replay and analysis-driven random probing, without duplicating
reference representations or specializing the data structure to a single access
pattern.

\paragraph{Example usage.}
See \cref{fig:view27-1-quadrillion} for an example of a deep-zoom render
that requires reference compression to complete.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{view27-1-quadrillion.png}
  \caption{View \#27, a period ~28-billion point rendered at 1 quadrillion
  iterations per pixel, requires reference compression to render on most
  hardware because of its memory requirements.  This render takes ~6h with an
  RTX 4090 and AMD 5950X and 128GB RAM with the multi-threaded reference orbit
  backend and is effectively the hardest point \FractalShark{} has likely ever
  rendered.}
  \label{fig:view27-1-quadrillion}
\end{figure}


\section{Multi-threaded Reference Orbit Acceleration}
\label{subsec:ref-orbit-mt}

The \texttt{MT3} reference-orbit path accelerates the high-cost MPIR arithmetic
of the single-threaded authoritative orbit (\cref{subsec:ref-orbit-st}) by
decomposing each iteration across multiple CPU threads.  The design targets
independent, high-latency big-integer operations that naturally overlap in the
perturbation recurrence, while minimizing synchronization overhead.  Two
recurring implementation patterns characterize this approach.

\paragraph{Asynchronous squaring.}
During authoritative orbit evaluation, two worker threads compute the squared
terms \(x_n^2\) and \(y_n^2\) concurrently.  In parallel, the main thread
evaluates the cross term in
\[
  y_{n+1} = 2 x_n y_n + c_y,
\]
performs low-precision periodicity checks, and manages operand reuse and
serialization.  Once the squared terms are returned, the real component update
is completed as
\begin{equation}
  x_{n+1} = x_n^2 - y_n^2 + c_x.
\end{equation}
This decomposition exposes substantial instruction-level and thread-level
parallelism while preserving the exact arithmetic semantics required for
reference orbit computation.

\paragraph{Lock-free handoff with prefetch.}
Inter-thread communication is implemented using a minimal lock-free mailbox
(\texttt{ThreadPtrs<T>}) consisting of atomic \texttt{In} and \texttt{Out}
pointers.  The protocol proceeds as follows:
\begin{enumerate}
  \item The producer publishes a work pointer by storing it into \texttt{In}.
  \item The worker spins until it atomically claims the pointer, prefetches the
        referenced MPIR operands, executes the required arithmetic, and then
        publishes the same pointer via \texttt{Out}.
  \item The producer spins until it retrieves the completed pointer from
        \texttt{Out}, after which the computed results are consumed.
\end{enumerate}
To reduce cache-miss latency when operating on large MPIR limb arrays, worker
threads explicitly prefetch both MPIR metadata and limb data using a fixed
64-byte stride.  This strategy is particularly beneficial at very high
precision, where memory latency can otherwise dominate execution time.

\subsection{Precision-dependent single-thread versus multi-thread trade-offs}
\label{subsec:ref-orbit-mt-tradeoff}

The relative effectiveness of single-threaded and multi-threaded reference-orbit
evaluation depends strongly on the active precision, which in practice is
closely correlated with zoom level and MPIR limb count.  Consequently, the
implementation supports both execution modes and selects between them
automatically when \texttt{PerturbationAlg::Auto} is enabled.

At lower precision (small limb counts), overall performance is often dominated
by fixed overheads such as synchronization, atomic mailbox operations, thread
spinning, and cache traffic.  In this regime, a single-threaded MPIR execution
path benefits from tight instruction locality, reduced fencing, and predictable
cache reuse, and may outperform a parallelized approach despite executing all
arithmetic serially.

As precision increases, the cost structure shifts toward throughput-dominated
big-integer arithmetic.  Large limb counts cause MPIR squaring and multiplication
to dominate total runtime, amortizing synchronization overhead and making
concurrent execution increasingly effective.  Overlapping independent
operations, such as the simultaneous computation of \(x_n^2\) and \(y_n^2\),
yields substantial gains, while explicit limb prefetching further mitigates
memory latency.  Beyond a hardware- and workload-dependent threshold,
multi-threaded reference-orbit computation consistently outperforms the
single-threaded path, with speedups increasing as precision grows.

To exploit this behavior without requiring manual tuning, the \texttt{Auto}
selection mode chooses the perturbation algorithm based on the current zoom
factor as a proxy for required precision.  At moderate zoom levels, it favors
single-threaded periodicity detection to avoid unnecessary parallel overhead.
At higher zoom levels, it transitions to the \texttt{MTPeriodicity3} path, where
parallel MPIR arithmetic becomes advantageous.  For extreme zoom, a hybrid
multi-threaded perturbation strategy (e.g.,
\texttt{MTPeriodicity3PerturbMTHighMTMed3}) is selected to maintain throughput at
very high precision, acknowledging that some configurations remain
experimental.

Overall, single-threaded and multi-threaded reference-orbit evaluation are
treated as complementary strategies rather than a strict hierarchy: fewer limbs
favor single-thread execution, while increasing precision increasingly favors
multi-threaded decomposition, with \texttt{Auto} providing an adaptive runtime
selection mechanism.


\section{GPU reference orbit backend}
\label{subsec:ref-orbit-gpu}

In many numerical algorithms, correctness and stability depend on arithmetic
precision far exceeding that of standard IEEE~754 floating-point formats. The
Mandelbrot reference orbits is a canonical example: small rounding errors
introduced early in the iteration can grow exponentially, eventually corrupting
orbit classification, perturbation terms, or bailout logic. While
arbitrary-precision libraries like MPIR can provide the required accuracy
(\cref{sec:ref-orbit-calc}), their
performance is often insufficient when reference orbits must be computed
repeatedly or at very high precision. This creates a fundamental tension between
numerical fidelity and throughput.

To resolve this tension, high-precision arithmetic operations---in particular
multiplication (\cref{sec:ntt-multiply}), addition, and subtraction (\cref{sec:hp-add}) of large significands---must be
implemented with both mathematical rigor and architectural efficiency. Addition
and subtraction are dominated by carry propagation across hundreds or thousands
of limbs, while multiplication scales quadratically unless asymptotically faster
algorithms are employed. For precisions relevant to deep zoom reference orbits,
naive limb-by-limb multiplication quickly becomes the dominant cost,
overwhelming all other parts of the computation.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{gpu-reference-orbit.png}
  \caption{View \#30 rendered at zoom factor \(10^{114,514}\) using the GPU
  reference orbit backend.  Requires ~73s to render on RTX 5090.}
  \label{fig:gpu-reference-orbit}
\end{figure}

\subsection{Why Fast Multiplication Matters for Reference Orbits}

Generating many fractal reference orbits typically involves iterating a
recurrence relation (e.g.\ $z_{n+1} = f(z_n)$) at very high precision, often for
thousands of iterations per reference point. Each iteration requires multiple
high-precision multiplications and additions, and the total cost grows linearly
with iteration count and superlinearly with precision. Even modest speedups in
the core arithmetic therefore compound dramatically over the full computation.

By using an NTT-based multiplication strategy, large significand products can be
computed in $O(n \log n)$ time instead of $O(n^2)$, shifting the performance
profile of high-precision arithmetic into a regime where GPUs can be effectively
utilized. When combined with carefully engineered carry propagation and
normalization steps, this enables high-precision multiply/add/subtract
operations that are fast enough to make reference orbit computation practical at
scales that would otherwise be prohibitive.

\subsection{System Architecture and Execution Context}
\label{subsec:ref-orbit-architecture}

The GPU reference orbit backend is a stateful, precision-specialized execution
engine that computes authoritative Mandelbrot reference orbits entirely on the
GPU. The backend advances the recurrence
\[
z_{n+1} \leftarrow z_n^2 + c
\]
at fixed high precision by executing multiple iterations per kernel invocation,
while the CPU orchestrates invocation, result collection, and termination.

\paragraph{Division of responsibility.}
The design follows a strict separation of concerns. The CPU is responsible for
selecting a supported precision, initializing GPU resources, invoking the kernel
in bounded iteration batches, and accumulating emitted iteration records into
\texttt{PerturbationResults}. The GPU owns all numerical state of the orbit,
executes the high-precision arithmetic pipeline, detects escape and periodicity,
and reports authoritative outcomes to the host.

\paragraph{Persistent backend state.}
Reference orbit computation is centered around a persistent \emph{combo} object
(\texttt{HpSharkReferenceResults<SharkFloatParams>}) that encapsulates all state
required to advance the orbit at a fixed precision. The combo contains:
\begin{itemize}
  \item high-precision orbit-related values (e.g.\ radius threshold and derivative terms),
  \item persistent arithmetic pipeline state for multiplication
        (\texttt{Multiply}, implementing NTT-based convolution) and addition
        (\texttt{Add}, implementing parallel-prefix carry propagation),
  \item a fixed-size output mailbox (\texttt{OutputIters} and
        \texttt{OutputIterCount}) with \texttt{MaxOutputIters}=1024,
  \item termination status (\texttt{PeriodicityStatus}),
  \item and host-only launch and resource plumbing, including a device pointer to
        the GPU-resident state, temporary device buffers, an associated CUDA
        stream, and cached kernel argument arrays.
\end{itemize}
The combo is initialized once, reused across multiple kernel invocations, and
destroyed only after the reference orbit computation completes.

\paragraph{Batch-oriented kernel invocation.}
Each call to \texttt{InvokeHpSharkReferenceKernel} launches a GPU kernel that
advances the Mandelbrot recurrence by up to a specified number of iterations
(\texttt{itersToRun}). Within a single invocation, the kernel performs multiple
full iterations internally, combining high-precision squaring and addition on
the GPU. As iterations progress, the kernel emits up to
\texttt{MaxOutputIters} iteration records into the combo’s output mailbox and
updates \texttt{OutputIterCount} accordingly.

This batching strategy bounds kernel runtime, enables incremental streaming of
orbit data back to the host, and allows long orbits to be computed without
reinitializing GPU state.

\paragraph{Termination and authority.}
After each invocation, the host inspects the combo’s \texttt{PeriodicityStatus}
and stops early if the GPU reports escape, detected periodicity, or an error
condition, or if the total requested iteration budget has been reached. In this
model, the GPU backend is authoritative: it owns the orbit state, performs all
high-precision arithmetic exactly with respect to the fixed significand
precision, and determines orbit outcomes without CPU-side recomputation or
correction.

\paragraph{Precision specialization.}
The entire lifecycle of a reference orbit computation is specialized by
\texttt{SharkFloatParams}. Precision is selected once at startup via
\texttt{DispatchByPrecision} and remains fixed for the lifetime of the combo
object. This ensures that kernel code, shared-memory usage, and arithmetic
pipelines are fully specialized for the chosen precision, at the cost of
supporting a finite, enumerated set of precisions.

\medskip

This architectural structure defines the execution context for the remainder of
this section. Subsequent subsections describe the internal arithmetic pipelines
used within each kernel invocation, including NTT-based multiplication,
high-precision addition, carry propagation, and normalization, and explain how
these components are organized to maintain correctness and performance within
the bounded-chunk execution model.

\subsection{High-Precision Floating-Point Addition on the GPU}
\label{sec:hp-add}

While high-precision multiplication (\cref{sec:ntt-multiply}) dominates the asymptotic cost of many
arbitrary-precision algorithms, addition and subtraction remain critical in
practice. In iterative computations such as reference orbit generation, each
iteration performs multiple additions and subtractions on large significands,
often regardless of whether multiplication is required. At precisions of
hundreds or thousands of limbs, even these ostensibly simple operations become
nontrivial due to long-range carry and borrow propagation.

A naive limbwise implementation resolves carries sequentially: a carry-out from
limb $i$ must be fully determined before limb $i+1$ can be finalized. This
introduces strict serial dependence and $O(N)$ latency, making naive carry
handling fundamentally incompatible with massively parallel GPU architectures.
Without reformulation, carry propagation quickly becomes a synchronization
bottleneck that erodes the benefits of fast high-precision multiplication.

\subsubsection{Numerical Representation}

We represent a high-precision floating-point number as
\begin{equation}
x = s_x \cdot 2^{e_x} \cdot \sum_{i=0}^{N-1} d_i \, 2^{-w i},
\end{equation}
where $s_x \in \{-1,+1\}$ is the sign, $e_x \in \mathbb{Z}$ is a shared exponent,
$d_i \in \{0,\dots,2^w-1\}$ are fixed-width limbs, $w$ is the limb bit width
(typically $32$ or $64$), and $N$ is the limb count. Arithmetic is performed on
the limb array, while the exponent is tracked separately. The mantissa is thus
treated as a fixed-point integer scaled by an implicit radix $2^{-w}$.

\subsubsection{Problem Statement and Exponent Alignment}

Given two high-precision floating-point numbers
\[
x = s_x \cdot 2^{e_x} \cdot M_x, \qquad
y = s_y \cdot 2^{e_y} \cdot M_y,
\]
the objective is to compute
\[
z = x + y
\]
exactly, or under a well-defined rounding mode, using GPU-parallel execution.

Assuming without loss of generality that $e_x \ge e_y$, we define
\[
\Delta = e_x - e_y
\]
and align the mantissas by shifting
\begin{equation}
M_y' = M_y \cdot 2^{-\Delta}.
\end{equation}
If $\Delta \ge Nw$, the contribution of $y$ is below the representable precision
and the operation degenerates to $z \approx x$. Otherwise, both mantissas are
expressed at the shared exponent $e = e_x$.

\subsubsection{Signed Limbwise Accumulation}

After alignment, addition and subtraction are unified by introducing signed limb
arrays
\[
a_i = s_x \cdot d_i^{(x)}, \qquad
b_i = s_y \cdot d_i^{(y')}.
\]
The raw limbwise sum is then
\begin{equation}
t_i = a_i + b_i,
\end{equation}
which may lie outside the canonical range $[0,2^w)$. This computation is
embarrassingly parallel and maps directly to GPU threads, with each thread
computing one or more limb sums independently.

\subsubsection{Carry Structure and Prefix Reformulation}

Each intermediate sum $t_i$ can be decomposed as
\begin{equation}
t_i = r_i + c_i \cdot 2^w,
\end{equation}
where
\[
r_i = t_i \bmod 2^w, \qquad
c_i = \left\lfloor \frac{t_i}{2^w} \right\rfloor.
\]
The carry $c_i$ contributes to limb $i+1$, inducing a dependency chain that is
sequential in the worst case.

To parallelize this process, carry propagation is reformulated as a prefix
problem. Each limb defines a carry transfer operator
\begin{equation}
T_i(x) = x + c_i,
\end{equation}
where $x$ is the incoming carry from lower-order limbs. The total carry entering
limb $i$ is given by the prefix composition
\begin{equation}
C_i = (T_{i-1} \circ T_{i-2} \circ \cdots \circ T_0)(0).
\end{equation}
Because the operator composition
\[
(T_a \circ T_b)(x) = x + c_b + c_a
\]
is associative, carry propagation reduces to a parallel prefix sum over the
carry values $\{c_i\}$, generalizing classical carry-lookahead techniques to
arbitrary-precision arithmetic.

\subsubsection{Single-Pass Parallel Prefix on the GPU}

On GPUs, conventional multi-pass prefix scans incur excessive global
synchronization or kernel-launch overhead. Instead, we employ a single-pass
parallel prefix formulation with decoupled look-back
\cite{merrill2016single}. Each thread block computes its local carry prefix
independently, while a lightweight look-back mechanism resolves inter-block
dependencies only when required.

This approach allows blocks to make forward progress without global barriers,
stalling only when an upstream carry must be observed. Long carry chains, while
possible, are rare in practice and are handled efficiently when they occur. The
resulting carry propagation exhibits near-linear throughput with logarithmic
critical path length, making it well-suited to large limb counts.

\subsubsection{Finalization and Normalization}

The corrected result limbs are obtained as
\begin{equation}
d_i^{(z)} = (r_i + C_i) \bmod 2^w.
\end{equation}
A carry beyond the most significant limb increments the exponent,
\[
d_N^{(z)} \neq 0 \;\Rightarrow\; e \leftarrow e + 1,
\]
while leading-zero cancellation triggers renormalization and a corresponding
decrement of the exponent.

\subsubsection{Correctness and Complexity}

The algorithm is mathematically equivalent to exact integer addition of aligned
mantissas,
\[
z = 2^e \cdot (M_x + M_y'),
\]
and the prefix formulation preserves the strict associativity of integer
addition. All operations are integer-valued and deterministic, yielding
bitwise-identical results across executions.

For $N$ limbs, the algorithm performs $O(N)$ work with $O(\log N)$ span and
$O(N)$ global memory traffic, making it asymptotically optimal for high-precision
addition on massively parallel architectures.

\paragraph{Comparison with MPIR/GMP Serial Carry Propagation}

Conventional high-precision libraries such as MPIR and GMP implement addition
using strictly serial carry propagation. After exponent alignment, limbs are
processed sequentially from least to most significant, with each carry-out
immediately applied to the next limb. This approach is optimal for scalar CPUs:
it minimizes memory traffic, exploits tight data locality, and benefits from
branch prediction and instruction-level parallelism. For moderate precisions,
the constant factors are small and the simplicity of the algorithm dominates.

However, the serial carry dependency enforces an $O(N)$ critical path that
fundamentally limits parallelism. Even when vector instructions are used to
accelerate limbwise addition, the carry chain itself remains sequential and
cannot be overlapped across independent execution units. As a result, MPIR/GMP
addition scales primarily in throughput with clock frequency rather than in
latency with available parallel resources.

In contrast, the GPU-oriented formulation presented here trades a modest
increase in algorithmic complexity for a dramatic reduction in critical path
length. By expressing carry propagation as an associative prefix operation and
resolving dependencies using a single-pass, decoupled look-back scheme, the
effective span is reduced from $O(N)$ to $O(\log N)$. This shift aligns the
algorithm with the strengths of massively parallel architectures, allowing
thousands of threads to participate in carry resolution while preserving exact
arithmetic semantics.

Importantly, the GPU approach does not outperform MPIR/GMP for small limb
counts, where launch overhead and synchronization dominate. Its advantage
emerges at large precisions, where serial carry latency becomes comparable to
or exceeds the cost of multiplication. In this regime, parallel carry
propagation is essential for sustaining end-to-end performance in
high-precision iterative workloads.

\paragraph{Role of Merrill--Garland in Parallel Carry Propagation}

The approach adopted here is closely informed by the work of
\cite{merrill2016single} on single-pass parallel prefix scans with decoupled
look-back. Their contribution is not a numerical
algorithm per se, but a general execution strategy for evaluating long,
associative prefix dependencies efficiently on GPUs.

In the context of high-precision addition, carry propagation is naturally
expressed as a prefix composition over per-limb transfer functions. However,
naively mapping this formulation to a GPU using classical parallel scans would
require multiple global synchronization phases or kernel launches, introducing
significant overhead and limiting scalability. The Merrill--Garland framework
addresses this by separating the computation into two logically distinct
components: (1) local prefix evaluation within a thread block, and (2)
resolution of inter-block dependencies through a lightweight, dynamically
resolved look-back mechanism.

Crucially, the decoupled look-back design allows blocks to proceed independently
until a true dependency on an upstream block is encountered. This property
aligns well with the statistical structure of carry propagation in large
arbitrary-precision additions, where long carry chains are relatively rare and
most limbs can be finalized without waiting on distant predecessors. The
algorithm therefore achieves high average throughput while retaining correctness
in worst-case carry scenarios.

It is important to emphasize that Merrill--Garland does not prescribe how carry
information should be represented, nor does it assume a particular numerical
domain. In this work, their scan framework is specialized to the algebra of
carry transfer functions arising from signed limbwise addition. The numerical
semantics---exact integer arithmetic with deterministic behavior---remain
entirely independent of the scan mechanism. In this sense, Merrill--Garland
provides the parallel control structure, while the arithmetic formulation
determines the correctness and stability of the result.

By combining a mathematically exact carry formulation with a single-pass prefix
execution model, the resulting algorithm bridges a gap between classical
high-precision arithmetic and modern GPU execution. This synthesis enables
carry-dominated operations, traditionally viewed as inherently serial, to scale
efficiently on massively parallel hardware.

\paragraph{Contrast with Blelloch-Style Parallel Prefix Scans}

Classical parallel prefix algorithms, most notably the Blelloch scan, provide a
well-established framework for evaluating associative operators with
$O(\log N)$ depth and $O(N)$ work. Blelloch-style scans proceed in two distinct
phases: an up-sweep (reduce) phase that computes partial aggregates over a
balanced tree, followed by a down-sweep phase that distributes prefix values to
all elements. This structure is well suited to SIMD and shared-memory parallel
models and forms the basis of many CPU and GPU prefix implementations.

However, on GPUs, Blelloch scans impose rigid global synchronization points
between phases. When applied to large limb arrays, this typically requires
either multiple kernel launches or explicit grid-wide synchronization barriers.
As a result, the execution is dominated by synchronization latency rather than
arithmetic throughput, particularly when the scan is embedded inside a larger
iterative algorithm such as high-precision reference orbit computation.

In contrast, the single-pass parallel prefix algorithm
of \cite{merrill2016single} eliminates the global phase separation inherent in
Blelloch scans. Instead of enforcing a strict up-sweep/down-sweep structure,
each thread block computes its local prefix independently and resolves
inter-block dependencies dynamically using a decoupled look-back mechanism. This
allows blocks to make forward progress asynchronously and avoids global barriers
in the common case.

For high-precision carry propagation, this distinction is critical. Carry chains
often terminate locally, and only a small fraction of limbs depend on distant
predecessors. Blelloch-style scans nevertheless force all blocks to participate
in every global synchronization phase, regardless of whether a true dependency
exists. Merrill--Garland, by contrast, pays synchronization cost only when a
carry must cross block boundaries, yielding substantially better utilization of
GPU resources.

From a numerical perspective, both approaches compute the same associative
prefix over carry transfer operators and are therefore equally correct. The
difference lies entirely in execution strategy. Blelloch scans provide a
deterministic, phase-structured evaluation with predictable synchronization,
while Merrill--Garland offers a latency-tolerant, demand-driven execution model
that is better aligned with the irregular dependency structure of carry
propagation in large arbitrary-precision additions.

In this work, the Merrill--Garland formulation is preferred not because it
changes the arithmetic, but because it minimizes synchronization on massively
parallel hardware, allowing carry-dominated operations to scale to precisions
where serial or multi-pass approaches become prohibitive.


\subsubsection{Discussion}

By expressing carry propagation as a parallel prefix problem and employing a
single-pass, decoupled look-back strategy, high-precision floating-point
addition becomes compatible with GPU execution. The formulation cleanly
separates numerical correctness from execution strategy and composes naturally
with NTT-based multiplication and fused arithmetic pipelines. As a result,
fully high-precision arithmetic becomes practical for demanding workloads such
as GPU-accelerated reference orbit computation.


\subsection{Number Theoretic Transform and Multiply}
\label{sec:ntt-multiply}

High-precision floating-point multiplication ultimately reduces to multiplying
two large integers (the significands), followed by normalization and exponent
adjustment. A standard way to accelerate large-integer multiplication is to
compute the convolution of digit-limbs using a fast transform. Over the reals,
one would use an FFT; over modular arithmetic, the analogous tool is the
\emph{Number Theoretic Transform} (NTT).

\subsection{From Large-Integer Multiplication to Convolution}

Let the two nonnegative integers to be multiplied be represented in base $B$ as

\[
A = \sum_{i=0}^{n-1} a_i B^i,\qquad
C = \sum_{i=0}^{n-1} c_i B^i,
\]

with $0 \le a_i, c_i < B$. Their product is

\[
A\cdot C = \sum_{k=0}^{2n-2} \left(\sum_{i=0}^{k} a_i c_{k-i}\right) B^k.
\]

The coefficients

\[
s_k = \sum_{i=0}^{k} a_i c_{k-i}
\]

form the \emph{discrete convolution} of the sequences $(a_i)$ and $(c_i)$. If we
can compute $(s_k)$ quickly, then we can recover the product (with subsequent
carry propagation in base $B$).

\subsection{NTT as an FFT Over a Finite Field}

The NTT computes a discrete Fourier transform, but with all operations performed
modulo a prime $p$ rather than over $\mathbb{C}$. Choose a prime modulus $p$ and
an $N$-th primitive root of unity $\omega \in \mathbb{Z}_p$ such that

\[
\omega^N \equiv 1 \pmod p,\qquad
\omega^k \not\equiv 1 \pmod p \text{ for } 0<k<N.
\]

Then the forward NTT of a length-$N$ sequence $x = (x_0,\dots,x_{N-1})$ is

\[
X_k = \sum_{j=0}^{N-1} x_j \,\omega^{jk} \pmod p,\qquad k=0,\dots,N-1,
\]

and the inverse NTT is

\[
x_j = N^{-1} \sum_{k=0}^{N-1} X_k \,\omega^{-jk} \pmod p,\qquad j=0,\dots,N-1,
\]

where $N^{-1}$ is the multiplicative inverse of $N$ modulo $p$.

Just as with the complex FFT, the key property is that the transform diagonalizes convolution:

\[
\mathrm{NTT}(x \star y) \;=\; \mathrm{NTT}(x)\odot \mathrm{NTT}(y),
\]

where $\odot$ denotes pointwise multiplication and $\star$ denotes cyclic
convolution modulo $x^N-1$. To compute the \emph{linear} convolution needed for
multiplication, we pad both inputs with zeros to length $N \ge 2n$ so that the
cyclic convolution coincides with the linear convolution.

\subsection{Why a Special Prime Helps: \texttt{MagicPrime}}

A practical NTT requires:

\begin{enumerate}

\item $N$ to be highly composite (typically a power of two) so that
Cooley--Tukey style butterflies apply efficiently;

\item a modulus $p$ such that $N \mid (p-1)$, guaranteeing the existence of an
$N$-th primitive root of unity in $\mathbb{Z}_p$;

\item fast modular multiplication on the target hardware (here, GPUs with
efficient 64-bit integer operations).

\end{enumerate}

The prime used here, denoted \texttt{MagicPrime}, is chosen specifically to
satisfy these constraints. The critical mathematical implication of using a
prime modulus is that $\mathbb{Z}_p$ is a field, so every nonzero element has a
multiplicative inverse and the NTT is well-defined and invertible when $\omega$
exists.

The most important structural requirement is:

\[
N \mid (p-1).
\]

When $N$ is a power of two, say $N = 2^m$, this becomes

\[
2^m \mid (p-1).
\]

Thus, the larger the power of two dividing $(p-1)$, the larger the supported
transform sizes.

\subsection{The ``Goldilocks'' Form and Power-of-Two Roots}

A particularly convenient choice on 64-bit hardware is a prime of the form

\[
p = 2^{64} - 2^{32} + 1,
\]

often called a ``Goldilocks'' prime. (In the codebase this role is played by
\texttt{MagicPrime}.) Two consequences make this form attractive:

\paragraph{(1) Large power-of-two factor in $p-1$.}

We have

\[
p - 1 = 2^{64} - 2^{32} = 2^{32}\left(2^{32}-1\right),
\]

so $2^{32}$ divides $(p-1)$. Therefore, for any $N = 2^m$ with $m \le 32$, there
exists an $N$-th root of unity in $\mathbb{Z}_p$. This enables power-of-two NTTs
up to length $2^{32}$ in principle (practically limited by memory and
implementation constraints), which is ample for large-limb convolutions.

\paragraph{(2) Efficient modular arithmetic with 64-bit operations.}

While modular multiplication in $\mathbb{Z}_p$ conceptually involves products up
to $p^2$, this modulus is engineered so reductions can be implemented
efficiently using 64-bit and 128-bit intermediates and/or Montgomery reduction.
The prime is close to $2^{64}$, which aligns well with native unsigned integer
ranges, and its special structure admits reduction strategies that avoid slow
division.

\subsection{NTT-Based Multiplication at a High Level}

Given limb sequences $(a_i)$ and $(c_i)$:

\begin{enumerate}

  \item Choose $N$ as a power of two with $N \ge 2n$, and choose a primitive
  $N$-th root $\omega \in \mathbb{Z}_p$.

  \item Zero-pad both sequences to length $N$ (interpreting limbs as elements of
  $\mathbb{Z}_p$).

  \item Compute $A_k = \mathrm{NTT}(a)_k$ and $C_k = \mathrm{NTT}(c)_k$.

  \item Compute pointwise products $P_k = A_k \cdot C_k \pmod p$.

  \item Compute $p_j = \mathrm{NTT}^{-1}(P)_j$ to obtain the convolution
  coefficients modulo $p$.

\end{enumerate}

Finally, because the convolution coefficients are computed modulo $p$, we must
ensure they represent the \emph{true integer} convolution values, not values
wrapped modulo $p$. This is achieved by choosing the limb base $B$ and transform
length $N$ so that each exact coefficient $s_k$ satisfies

\[
0 \le s_k < p,
\]

(or more generally, can be reconstructed from one or more moduli). With a single
sufficiently large prime (as is typical with a 64-bit Goldilocks prime and
appropriately sized limbs), the coefficients can be recovered directly and then
normalized via carry propagation in base $B$.

\medskip

This section establishes the mathematical foundation: \texttt{MagicPrime} is
selected so that large power-of-two NTTs exist (because $2^m \mid p-1$) and so
that modular arithmetic is efficient on 64-bit GPU hardware. Subsequent sections
will describe how this is specialized for high-precision floating-point
significands, including limb packing, Montgomery-domain multiplication, twiddle
scheduling, and normalization/carry handling.

\subsection{NTT in \FractalShark{}}

Fundamentally, the NTT implementation in \FractalShark{} is somewhat naive.  It
doesn't make much effort at memory layout or optimized coalescing, which is
likely costing it in performance.  This section describes how the NTT-based
multiplication is realized in practice, following the structure of
\FractalShark{}'s CUDA implementation. The overall goal is to compute the exact
convolution of two large significand arrays using modular arithmetic modulo
\texttt{MagicPrime}, and then to recover a canonical high-precision result via
normalization and carry propagation.

\subsubsection{Plan Construction and Parameter Selection}

Before launching any GPU kernels, an NTT ``plan'' is constructed. The plan
determines:

\begin{itemize}

  \item the coefficient bit-width $b$ used to pack the original 32-bit limbs
  into NTT coefficients,

  \item the packed coefficient length $L$,

  \item the transform size $N$, chosen as a power of two with $N \ge 2L$.

\end{itemize}

Correctness requires that no convolution coefficient overflow the prime modulus.
If the coefficients are bounded by $2^b$ and the transform length is $N$, then a
conservative bound on the largest convolution term is

\[
\max_k s_k \;\le\; N \cdot 2^{2b}.
\]

The plan builder enforces

\[
2b + \log_2 N + \delta \;\le\; 64,
\]

for a small safety margin $\delta$, ensuring that all exact convolution values
lie strictly below \texttt{MagicPrime}. This guarantees that the modular
convolution coincides with the true integer convolution.

\subsubsection{Roots of Unity and Montgomery Domain}

The CUDA setup phase precomputes all roots of unity required for the radix--2
NTT. For each stage $s$, a primitive $2^s$-th root $\omega_s$ and its inverse
are generated, along with:

\begin{itemize}

  \item powers of a twist root $\psi^i$ and $\psi^{-i}$,

  \item the modular inverse $N^{-1}$.

\end{itemize}

All constants are stored in Montgomery representation. As a result, every
modular multiplication inside the NTT butterflies is implemented as a Montgomery
multiply, avoiding explicit modular reduction by division.

\subsubsection{Packing, Twisting, and Forward NTT}

The input significands are initially stored as arrays of 32-bit limbs. These are
packed into base-$2^b$ coefficients

\[
A(x) = \sum_{i=0}^{L-1} a_i x^i,\qquad
C(x) = \sum_{i=0}^{L-1} c_i x^i,
\]

with $0 \le a_i, c_i < 2^b$. During packing, each coefficient is:

\begin{enumerate}

  \item mapped into $\mathbb{Z}_p$,

  \item multiplied by the twist factor $\psi^i$,

  \item converted into Montgomery form.

\end{enumerate}

After packing, an in-place radix--2 forward NTT is performed. A bit-reversal
permutation places coefficients into the correct order, followed by iterative
butterfly stages:

\[
(u, v) \;\mapsto\; (u + \omega v,\; u - \omega v),
\]

with all operations performed modulo \texttt{MagicPrime}. A grid-stride loop is
used so that threads collectively cover all $N$ coefficients, independent of the
exact grid dimensions.

\subsubsection{Multiway Pointwise Multiplication}

In the transform domain, convolution reduces to pointwise multiplication. To
amortize the cost of the NTT, the implementation computes three related
convolutions simultaneously using a standard three-multiply decomposition.
Conceptually, if

\[
X = X_r + iX_i,\qquad Y = Y_r + iY_i,
\]

then the products

\[
X_rY_r,\quad X_iY_i,\quad (X_r+X_i)(Y_r+Y_i)
\]

are sufficient to reconstruct both real and imaginary components. Accordingly,
three frequency-domain products are computed per transform index:

\[
\widehat{XX}_k,\quad \widehat{YY}_k,\quad \widehat{XY}_k,
\]

each via a Montgomery modular multiplication modulo \texttt{MagicPrime}.

\subsubsection{Inverse NTT and Untwisting}

Following pointwise multiplication, inverse radix--2 NTTs are applied using the
inverse roots of unity. The inverse transform yields coefficients still in
Montgomery form and still containing the twist factor. These are corrected by:

\begin{enumerate}
\item multiplying by $\psi^{-i}$,
\item multiplying by $N^{-1}$,
\item converting out of Montgomery representation.
\end{enumerate}

After this step, the data represents the exact integer convolution coefficients in base $2^b$.

\subsubsection{Unpacking and Normalization}

The final stage converts the convolution coefficients back into the original
limb representation. Base-$2^b$ digits are unpacked into wide accumulators
(typically 128-bit), after which carry propagation and normalization are
performed to produce a canonical high-precision result. This normalization step
is handled separately and is optimized using parallel-prefix techniques as
described in earlier sections.

\medskip

Together, these steps implement \FractalShark{}'s NTT-based multiplication
pipeline: from high-precision significand limbs, through modular convolution in
$\mathbb{Z}_{\texttt{MagicPrime}}$, and back to an exact, normalized
high-precision product suitable for subsequent arithmetic in reference orbit
computation.

\subsection{Amortizing Synchronization via Simultaneous Products}

In the presented GPU implementation, a dominant cost of large-scale NTT-based
multiplication is not the modular arithmetic itself, but the required grid-wide
synchronization. Each major phase of the pipeline— packing and twisting of
inputs, forward NTT, pointwise multiplication, inverse NTT, and untwisting with
normalization—requires that all threads observe a consistent global state. This
is enforced using explicit grid-level barriers (e.g.,
\texttt{cooperative\_groups::grid\_group::sync()}), whose latency grows with
grid size and is largely independent of the amount of arithmetic performed
between barriers.

To reduce the amortized cost of these synchronization points, the implementation
computes three related convolution products, $X\cdot Y$, $X^2$, and $Y^2$,
within a single NTT pipeline. All three products share the same forward
transforms, inverse transforms, twiddle scheduling, temporary buffers, and
synchronization boundaries. As a result, the fixed cost of grid-wide
coordination and memory visibility is incurred once, while producing three
mathematically independent convolution results. The additional arithmetic
required for the extra pointwise products consists only of a small number of
Montgomery multiplications per frequency index and is negligible compared to the
cost of global synchronization and memory traffic.

At the code level, this strategy is realized by a fused front-end that packs the
input limb arrays, applies twist factors, and converts values into Montgomery
form while producing multiple frequency-domain streams in a single pass. A
single grid-stride pointwise multiplication phase then computes the three
products concurrently. These are followed by a single multiway inverse radix--2
NTT, after which a unified untwisting, scaling by $N^{-1}$, and conversion out
of Montgomery representation are performed. Explicit grid-wide barriers appear
only at true phase boundaries, serving as producer--consumer separators between
global-memory stages rather than as fine-grained synchronization.

This design is well matched to the target workload. In high-precision reference
orbit computation, both cross terms and squared terms arise naturally and
repeatedly. By folding these operations into a single multiway NTT, the
implementation reduces the total number of kernel phases and synchronization
events, increases arithmetic intensity per barrier, and improves overall GPU
utilization. Consequently, the effective performance of the multiplication
pipeline is governed primarily by arithmetic throughput and memory bandwidth,
rather than by synchronization overhead.

\subsection{Usage in \FractalShark{}}
The key design is a persistent \emph{combo} object returned by initialization:

\begin{enumerate}
  \item \texttt{InitHpSharkReferenceKernel}: allocates device/host state for the reference orbit and sets
        \((c_x,c_y)\), max radius, and launch configuration.
  \item \texttt{InvokeHpSharkReferenceKernel}: advances the orbit in bounded batches of at most
        \(\texttt{MaxOutputIters}\), storing results in \texttt{OutputIters}.
  \item \texttt{ShutdownHpSharkReferenceKernel}: frees persistent resources.
\end{enumerate}

Because the precision must be fixed at compile time for the \texttt{
SharkFloatParams} specialization, \texttt{DispatchByPrecision} rounds the
requested precision to a power of two and chooses from a fixed set (\(\{256,512,
\dots,524288\}\) bits). Each invocation appends the emitted \texttt{OutputIters}
records into the CPU-side \texttt{PerturbationResults}. Periodicity and escape
are reported through \texttt{PeriodicityStatus} and handled similarly to the CPU
paths.

\subsection{High-Precision Floating-Point Representation in \FractalShark{}}
\label{subsec:hpfloat-model}

The GPU reference orbit backend operates on a custom high-precision
floating-point format implemented by the \texttt{HpSharkFloat} family of types.
This format is intentionally \emph{not} IEEE~754--compatible. Instead, it is
designed to support exact arithmetic on large significands, deterministic
behavior across CPU and GPU backends, and efficient execution on massively
parallel architectures.

Conceptually, an \texttt{HpSharkFloat} value represents a real number of the
form
\[
x = s \cdot M \cdot 2^E,
\]
where $s \in \{-1,+1\}$ is the sign, $M$ is a nonnegative integer significand,
and $E$ is a signed integer exponent. The sign, significand, and exponent are
tracked independently, and no implicit normalization or rounding is performed
during arithmetic operations.

\paragraph{Significand representation.}
The significand $M$ is stored as a fixed-size array of 32-bit limbs. The number
of limbs is determined at compile time by the
\texttt{SharkFloatParams} specialization and remains constant throughout the
lifetime of the computation. As a result, precision is explicit and static:
all arithmetic kernels operate on limb arrays of identical size, and no dynamic
reallocation or precision growth occurs during iteration.

The significand represents a nonnegative integer magnitude. Negative values are
handled by explicit sign logic rather than two’s-complement arithmetic, which
simplifies carry propagation and normalization.

\paragraph{Exponent handling and deferred normalization.}
The exponent $E$ is maintained separately from the significand and updated
explicitly by arithmetic kernels. In contrast to IEEE~754 arithmetic,
normalization is not performed eagerly after every operation. Intermediate
results may remain temporarily denormalized, with exponent adjustments and
carry propagation deferred until well-defined normalization phases.

This separation is essential for GPU execution. Immediate normalization would
introduce serial dependencies and global synchronization at every arithmetic
step, whereas deferred normalization allows large batches of arithmetic to be
performed in parallel before a single, optimized carry-propagation pass.

\paragraph{Normalization and canonical form.}
A value is considered to be in canonical form when:
\begin{itemize}
  \item the significand has been fully carry-propagated,
  \item the most significant limb is nonzero (except for the zero value),
  \item the exponent reflects the exact binary scaling of the significand.
\end{itemize}
Normalization is implemented as a separate phase and is optimized using
parallel-prefix carry propagation techniques, as described in earlier
sections.

\paragraph{Rounding, exceptional values, and determinism.}
No rounding is performed during addition, subtraction, or multiplication. All
operations are exact with respect to the fixed significand size. If an
intermediate result were to exceed the representable precision, this would
constitute a logic error rather than silently rounded behavior.

The format does not represent NaNs, infinities, or subnormal values. These
concepts are unnecessary for reference orbit computation, where arithmetic
ranges are structurally controlled and exceptional conditions (such as escape)
are handled at the algorithmic level.

Because the precision is fixed, arithmetic is exact, and no data-dependent
rounding occurs, the \texttt{HpSharkFloat} model provides deterministic,
bitwise-reproducible results across runs and across CPU and GPU backends,
subject only to correct synchronization.

\paragraph{Implications for reference orbit computation.}
This floating-point model is best viewed as high-precision integer arithmetic
with an explicit exponent rather than as an extension of IEEE floating-point.
The design directly enables NTT-based multiplication, parallel carry
propagation, and predictable numerical behavior at extreme precisions, making
it well suited for authoritative reference orbit generation on the GPU.


\section{Checksum-Guided Debugging via Host--Device Cross-Validation}
\label{sec:checksums}

Correctness bugs in GPU high-precision arithmetic are notoriously difficult to
localize: a single off-by-one carry, lane-misaligned shuffle, or missing
synchronization can corrupt results far downstream, often without an obvious
local symptom. To shorten the feedback loop, we instrument the arithmetic
pipeline (\cref{sec:hp-add,sec:ntt-multiply}) with lightweight \emph{stage checksums} that can be computed on the GPU,
computed independently on the host reference implementation, and compared to
pinpoint the first divergence. This converts a diffuse ``wrong final answer''
into a small set of candidate stages, dramatically reducing the search space
during development.  These checksums are disabled in production builds to avoid
any performance impact but can be enabled selectively during debugging.

\subsection{Stage Checksums as Homomorphic Summaries}

Let $A \in (\mathbb{Z}/2^w\mathbb{Z})^N$ denote a limb array representing an
intermediate mantissa or scratch buffer (e.g., aligned addends, raw limbwise
sums, carry descriptors, normalized limbs). We associate to each such array a
64-bit checksum
\begin{equation}
  H(A) \in \{0,1\}^{64},
\end{equation}
computed deterministically over the ordered limb sequence. In our implementation
we employ 64-bit rolling checksums (e.g., CRC-64 or Fletcher-64) because they are
(1) inexpensive relative to high-precision arithmetic, (2) order-sensitive, and
(3) easily reducible in parallel.

Each \emph{debug checkpoint} records a tuple
\begin{equation}
  S_k = \bigl(\texttt{Purpose}_k,\, \texttt{Depth}_k,\, \texttt{CallIndex}_k,\,
  \texttt{Conv}_k,\, H(A_k)\bigr),
\end{equation}
where $\texttt{Purpose}$ identifies which logical stage or buffer is being
summarized (e.g., \texttt{ADigits}, \texttt{BDigits}, \texttt{FinalAdd1},
\texttt{Result\_offsetXX}, etc.), and the remaining metadata disambiguates
dynamic execution contexts such as recursion depth or multiple invocations of
the same primitive within a larger operation. This metadata ensures that host
and device checkpoints can be matched without ambiguity even when the same
kernel path is exercised repeatedly in a pipeline.

\subsection{Parallel Computation with Order Preservation}

The checksum must match between host and device, so the GPU computation must be
\emph{equivalent} to the sequential definition. This requires two properties:
\begin{enumerate}
  \item \textbf{Deterministic segmentation}: the array is partitioned into
  contiguous chunks assigned to threads in a deterministic manner.
  \item \textbf{Order-preserving reduction}: partial checksums are combined in
  the same left-to-right order as the sequential traversal.
\end{enumerate}

To make this concrete, consider a checksum family whose internal state can be
represented as a small tuple (e.g., for Fletcher-64, $(s_1,s_2)$ with implicit
length). Each thread processes a contiguous chunk $A[\ell:r)$ and produces a
local state $\sigma_j$. To recover the global checksum, we require an
associative \emph{combine} operator $\oplus$ such that
\begin{equation}
  \sigma(A[0:n)) \;=\; \sigma(A[0:k)) \;\oplus\; \sigma(A[k:n)).
\end{equation}
Associativity enables hierarchical reduction, while order preservation requires
that the reduction tree respects the left-to-right concatenation order. On the
GPU, we implement this reduction hierarchically:
\begin{enumerate}
  \item Each thread computes $\sigma_j$ for its chunk.
  \item Within each warp, we compute an order-preserving prefix/reduction using
  warp shuffles and $\oplus$.
  \item Warp results are combined in increasing warp order to form a per-block
  checksum.
  \item Block results are reduced across the grid using repeated pairwise
  left-to-right combinations, yielding a single 64-bit checksum.
\end{enumerate}
This structure ensures that the device result is \emph{definitionally identical}
to the host's sequential traversal, modulo the checksum's arithmetic rules.

\subsection{Host Reference and Cross-Comparison}

On the host, we compute the same checksum $H(A_k)$ over the corresponding
reference buffers produced by the MPIR/GMP-based implementation (or a trusted
CPU implementation). Because both sides share the identical stage labeling
(\texttt{Purpose}, \texttt{Depth}, \texttt{CallIndex}, \texttt{Conv}), we can
compare checkpoint sequences as keyed records:
\begin{equation}
  \Delta_k \;=\;
  \bigl(H_{\mathrm{gpu}}(A_k) \stackrel{?}{=} H_{\mathrm{host}}(A_k)\bigr).
\end{equation}

A mismatch at checkpoint $k$ implies that the first divergence occurred at or
before the computation that produces $A_k$. By inserting checkpoints at
strategically chosen boundaries (e.g., after exponent alignment, after signed
limbwise accumulation, after carry resolution, after normalization), we obtain a
coarse-to-fine localization mechanism. In practice this behaves like a binary
search over the pipeline: we start with a small number of high-level checkpoints
and then refine around the earliest failing stage by adding more granular
summaries (e.g., per-buffer or per-substep).

\subsection{Practical Debugging Workflow}

The checksum mechanism supports a fast iterative workflow:
\begin{enumerate}
  \item \textbf{Instrument}: enable checksum capture for selected stages and
  record the resulting $(\texttt{key}, H)$ tuples from GPU execution.
  \item \textbf{Replay on host}: run the corresponding host reference path over
  the same inputs, generating the same keyed tuples.
  \item \textbf{Diff}: locate the earliest key for which
  $H_{\mathrm{gpu}} \ne H_{\mathrm{host}}$.
  \item \textbf{Refine}: insert additional checkpoints in the immediate
  neighborhood of the first mismatch (e.g., split ``carry propagation'' into
  descriptor publish, look-back resolution, digit transfer, and final writeback).
\end{enumerate}

This approach is especially effective for bugs that only manifest at scale:
race conditions, mis-specified synchronization, and rare carry-chain corner
cases. Even when the final numerical error is small or intermittent, checksum
mismatches provide a crisp and reproducible signature of divergence.

\subsection{Why Checksums Help for High-Precision Arithmetic}

High-precision arithmetic pipelines are composed of large, structured arrays
whose semantics evolve across stages (aligned limbs $\rightarrow$ raw sums
$\rightarrow$ carry-resolved digits $\rightarrow$ normalized mantissa). A
checksum provides a compact witness of correctness for each stage without
requiring full array dumps or expensive per-element comparisons. While a checksum
does not identify the exact index of the first wrong limb, it reliably answers a
more valuable question during early debugging: \emph{which transformation first
produced an incorrect state?} Once localized, targeted assertions, partial dumps,
or reduced test cases can isolate the exact defect with far less effort.

Taken together, stage checksums act as a low-overhead correctness scaffold that
bridges device execution and a trusted host reference, enabling rapid fault
localization in complex GPU high-precision arithmetic kernels.


\section{Per-Iteration Periodicity Checking under High-Precision Arithmetic on the GPU}
\label{sec:hp-periodicity}

The \FractalShark{} GPU reference-orbit kernel (\cref{subsec:ref-orbit-gpu}) evaluates the Mandelbrot recurrence
\(
z_{n+1} = z_n^2 + c
\)
using high-precision arithmetic for the state \(z_n\), while interleaving a
low-overhead periodicity/termination test between the (expensive) high-precision
multiply and add phases. This interleaving is essential: high-precision
multiplication dominates runtime at large limb counts, so an early stop
condition that can be evaluated with negligible additional cost yields
substantial savings when a cycle is detected or the orbit escapes.

\paragraph{State extraction and logging.}

At iteration \(n\), the high-precision state \((x_n, y_n)\) is available in the
multiplication output registers/structures. The periodicity checker converts
these values into a reduced high-dynamic-range scalar format,
\(\mathrm{HDR}(\cdot)\), via a lossy but monotone extraction
(\texttt{ToHDRFloat}) and stores the resulting pair in a per-iteration trace
buffer:

\begin{equation}
  \widehat{x}_n \leftarrow \mathrm{HDR}(x_n), \quad
  \widehat{y}_n \leftarrow \mathrm{HDR}(y_n), \quad
  \texttt{OutputIters}[n] \leftarrow (\widehat{x}_n, \widehat{y}_n).
\end{equation}

This trace is used both for downstream diagnostics and to provide a compact,
device-side record of the authoritative orbit prefix.

\paragraph{Derivative-based periodicity criterion.}

In addition to the orbit state, we propagate the complex derivative
\(
\frac{dz}{dc}
\)
along the orbit (cf.\ the CPU-side counterpart in \cref{subsec:ref-orbit-periodicity}). For the Mandelbrot map \(f(z)=z^2+c\), the chain rule yields

\begin{equation}
  \frac{d z_{n+1}}{d c} = f'(z_n)\frac{d z_n}{d c} + 1
  = 2 z_n \frac{d z_n}{d c} + 1,
  \label{eq:dzdc-recurrence}
\end{equation}

with initialization \(\frac{dz_0}{dc}=0\) when \(z_0=0\).
Writing \(z_n = x_n + i y_n\) and introducing the shorthand
\(d_n \equiv \frac{\partial z_n}{\partial c}\) with real and imaginary parts
\(d_{x,n}\) and \(d_{y,n}\), expanding
\eqref{eq:dzdc-recurrence} into real and imaginary parts gives:

\begin{align}
  d_{x,n+1} &= 2\,(x_n\, d_{x,n} - y_n\, d_{y,n}) + 1, \label{eq:dzdc-real}\\
  d_{y,n+1} &= 2\,(x_n\, d_{y,n} + y_n\, d_{x,n}). \label{eq:dzdc-imag}
\end{align}

In our implementation, the derivative accumulators \texttt{dzdcX} and
\texttt{dzdcY} are stored in the same high-precision float type used by the
orbit state, but the periodicity decision itself is performed on reduced HDR
magnitudes to minimize overhead:

\begin{equation}
  \|z_n\|_\infty \approx \max(|\widehat{x}_n|, |\widehat{y}_n|), \qquad
  \left\|\frac{dz_n}{dc}\right\|_\infty \approx \max(|\widehat{d x}_n|, |\widehat{d y}_n|),
\end{equation}

where hats denote reduced HDR representations after a normalization/reduction
step (\texttt{HdrReduce}) to ensure stable comparisons.

The checker uses a sufficient condition of the form

\begin{equation}
  \|z_n\|_\infty < 2\,R\,\left\|\frac{dz_n}{dc}\right\|_\infty,
  \label{eq:periodicity-ineq}
\end{equation}

where \(R\) is a problem-dependent bound (stored as \texttt{RadiusY} in the
reference structure). Intuitively, \eqref{eq:periodicity-ineq} compares the
current orbit magnitude against a scaled sensitivity radius implied by the
derivative growth: when the orbit state becomes small relative to the local
linearization bound, further iteration is treated as converging toward a
previous state and the algorithm flags a periodic cycle.
Concretely, the device computes

\begin{equation}
  n_2 \leftarrow \max(|\widehat{x}_n|, |\widehat{y}_n|), \quad
  r_0 \leftarrow \max(|\widehat{d x}_n|, |\widehat{d y}_n|), \quad
  n_3 \leftarrow 2\,R\,r_0,
\end{equation}

and terminates when \(n_2 < n_3\), setting \texttt{PeriodicityStatus =
PeriodFound} and recording \texttt{OutputIterCount = n+1}.

\paragraph{Escape test.}
We additionally apply a conventional escape-radius test in the same reduced
domain. After forming the next-state candidates
\(
\widehat{x}_n + \widehat{c}_x
\)
and
\(
\widehat{y}_n + \widehat{c}_y
\),
the checker evaluates
\begin{equation}
  \|\widehat{z}_n + \widehat{c}\|_2^2
  = (\widehat{x}_n + \widehat{c}_x)^2 + (\widehat{y}_n + \widehat{c}_y)^2,
\end{equation}
and declares \texttt{Escaped} when this value exceeds a fixed threshold
(\texttt{256.0} in the current implementation). As with the periodicity
criterion, all comparisons are performed on reduced positive HDR values to avoid
high-precision control-flow costs on the device.

\paragraph{Control-flow placement and synchronization.}
Periodicity checking is invoked once per iteration at the top of the reference
step, before launching the high-precision multiplication and addition kernels
that compute \(z_{n+1}\). In the current design, a single distinguished thread
(\texttt{block 0, thread 0}) executes the checker and updates the shared
termination flag stored in the global \texttt{reference} structure. The kernel
then executes a cooperative-grid barrier:
\begin{equation}
  \texttt{grid.sync()},
\end{equation}
ensuring that (i) all blocks observe a consistent \texttt{PeriodicityStatus} and
(ii) any updates to \texttt{dzdcX}, \texttt{dzdcY}, and the trace buffer are
visible before subsequent work proceeds. If termination is requested, the
iteration loop exits immediately; otherwise, the kernel proceeds into the
multiply-add pipeline that produces the next high-precision state.

This organization isolates the control-intensive termination logic from the
throughput-oriented NTT multiplication and carry-propagating addition stages,
while maintaining deterministic, grid-wide early-exit behavior. The only global
synchronization introduced by periodicity checking is the single \texttt{grid.sync()}
already required by the cooperative multi-block arithmetic pipeline, making the
incremental overhead of periodicity checking negligible relative to the cost of
high-precision multiplication at large precisions.

\paragraph{Alternative host-side periodicity checking.}
An alternative design would be to retain the authoritative high-precision orbit
state entirely on the device, periodically copy the full-precision values
\((x_n, y_n, dz_n/dc)\) back to the host, and perform periodicity and escape
testing on the CPU using existing MPIR-based logic. While conceptually simpler,
this approach is impractical at large precisions: each iteration would require
transferring hundreds to thousands of limbs per component over the PCIe bus,
and the cumulative bandwidth and synchronization costs would dominate the total
runtime. Moreover, host-side checking would introduce additional latency and
force a tighter coupling between device execution and CPU control flow. By
performing periodicity detection directly on the GPU using reduced HDR
representations, the implementation avoids repeated large data transfers while
preserving an early-termination mechanism that is both inexpensive and tightly
integrated with the cooperative-grid execution model.



\section{Perturbation-only Mandelbrot rendering (without linear approximation)}
\label{sec:perturb-only}

This kernel can render the Mandelbrot set using \emph{perturbation alone}, i.e.,
without taking any LA v2 linear-approximation steps. The same CUDA entry point
\code{mandel\_1xHDR\_float\_perturb\_lav2<IterType,T,SubType,Mode,PExtras>} is
used; the behavior is selected at compile time via \code{LAv2Mode}. In
particular, when \code{Mode} includes only the perturbation path (e.g.\
\code{LAv2Mode::PO}), the kernel skips the LA stage traversal and runs only the
perturbation loop against a stored reference orbit (\cref{sec:ref-orbit-calc}).

\subsection{Rendering objective}
\label{sec:perturb-only-goal}

The goal remains standard escape-time rendering for
\[
z_{n+1} = z_n^2 + c,\qquad z_0=0,
\]
with bailout \(|z|^2 \ge 4\). For each pixel, the kernel computes the parameter
\(c\), iterates until escape or \code{n\_iterations}, and stores the resulting
iteration count in \code{OutputIterMatrix[idx]}.

In perturbation rendering, the expensive high-precision orbit evaluation for
each pixel is avoided by reusing a \emph{reference orbit} computed at a
reference parameter \(c_\star\), then evolving only the \emph{delta orbit} for
nearby pixels.

\subsection{Reference orbit and delta formulation}
\label{sec:perturb-only-deltas}

Let the reference parameter be \(c_\star\), with stored reference orbit
\(\{z_n^\star\}\) satisfying
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a pixel parameter \(c\) near \(c_\star\), define the parameter delta
\[
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star,
\]
and the orbit delta
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star.
\]
Substituting \(z_n = z_n^\star + \Delta z_n\) into the Mandelbrot recurrence
yields the \emph{exact} delta recurrence:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right) \\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
Perturbation uses this recurrence directly: it is not a linearization. The work
per iteration is reduced because \(z_n^\star\) is fetched from storage rather
than recomputed in high precision.

\subsection{Pixel parameter delta \texorpdfstring{$\Delta c$}{Delta c}}
\label{sec:perturb-only-deltac}

For each pixel \((X,Y)\), the kernel constructs a delta parameter relative to a
chosen reference center (sign conventions incorporate the image mapping):
\begin{align}
\Delta c_x &= dx \cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy \cdot Y - \texttt{centerY},
\end{align}
and stores this as \code{DeltaSub0} (with scalar components \code{DeltaSub0X},
\code{DeltaSub0Y}). The perturbation state starts at
\[
\Delta z_0 = 0,
\]
so \code{DeltaSubN} is initialized to zero.

\subsection{Perturbation recurrence used in the kernel}
\label{sec:perturb-only-update}

Writing the reference sample as \(z^\star = x^\star + i y^\star\) and the delta
as \(\Delta z = \Delta x + i \Delta y\), the delta recurrence can be expressed
in a factored form convenient for implementation:
\begin{equation}
\Delta z \leftarrow \Delta z \cdot (2 z^\star + \Delta z) + \Delta c.
\label{eq:perturb-factor}
\end{equation}
This identity is equivalent to
\(\Delta z_{n+1} = 2 z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c\), because
\(\Delta z\cdot(2z^\star+\Delta z) = 2z^\star\Delta z + (\Delta z)^2\).

The kernel implements \cref{eq:perturb-factor} in real arithmetic by forming
the sums
\[
(2x^\star + \Delta x),\qquad (2y^\star + \Delta y),
\]
then updating:
\begin{align}
\Delta x' &= \Delta x\,(2x^\star + \Delta x) - \Delta y\,(2y^\star + \Delta y) + \Delta c_x, \\
\Delta y' &= \Delta x\,(2y^\star + \Delta y) + \Delta y\,(2x^\star + \Delta x) + \Delta c_y.
\end{align}
Note that both right-hand sides use the \emph{pre-update} values of
\(\Delta x\) and \(\Delta y\).  In the code, these originals are saved as
\code{DeltaSubNXOrig} and \code{DeltaSubNYOrig} before either component is
overwritten.  The intermediate quantities correspond to:
\begin{verbatim}
tempSum1 = 2*zy + DeltaSubNYOrig;  // (2 y^\star + \Delta y)
tempSum2 = 2*zx + DeltaSubNXOrig;  // (2 x^\star + \Delta x)
\end{verbatim}
followed by the real/imag updates. For extended or HDR types (\cref{sec:hdr-float}), the kernel routes
the same math through type-specialized implementations
(\code{T::custom\_perturb2} / \code{T::custom\_perturb3}) to keep the inner loop
tight and to enforce the type's reduction/normalization rules.

\subsection{Reconstructing the absolute orbit for escape testing}
\label{sec:perturb-only-reconstruct}

Perturbation evolves \(\Delta z_n\), but escape-time rendering requires a
bailout test on the absolute orbit \(z_n\). Each iteration reconstructs:
\[
z_n \approx z_n^\star + \Delta z_n,
\]
using the stored reference sample \(z_n^\star\) and the current delta. In the
kernel, this appears as:
\[
x = x^\star + \Delta x,\qquad y = y^\star + \Delta y.
\]
The bailout test is then performed on
\[
|z|^2 = x^2 + y^2,
\]
with the canonical threshold \(4\). For HDR and related types, the norm and the
comparison are performed on reduced values using specialized reduced
comparators to keep the escape decision stable at deep zoom.

\subsection{Reference index management and rebasing}
\label{sec:perturb-only-rebase}

The perturbation loop advances a reference-orbit index \code{RefIteration} in
lockstep with \code{iter}, fetching \(z^\star\) samples from \code{Perturb}. The
kernel includes a \emph{rebasing} mechanism that resets the delta
representation when it becomes ill-conditioned. Conceptually, if the delta
becomes comparable to or larger than the reconstructed orbit, the decomposition
\(z = z^\star + \Delta z\) stops being numerically advantageous. In that case,
the kernel folds the delta into the base by setting:
\[
\Delta z \leftarrow z,\qquad \text{and restart the reference index.}
\]
Operationally, the code compares the reconstructed orbit magnitude proxy
against the delta magnitude proxy, and also rebasing if the reference index
reaches the end of the stored orbit samples (to avoid out-of-range sampling).
After rebasing, perturbation continues from the new base representation.

This mechanism keeps the perturbation method usable across a wide range of
pixels and iteration depths while preserving the core rendering objective:
compute escape-time using a stable bailout on the reconstructed orbit.

\subsection{Using \code{LAv2Mode} to select perturbation-only execution}
\label{sec:perturb-only-mode}

The kernel is structured as two compile-time phases:
\begin{itemize}
\item an LA v2 phase guarded by \code{Mode == Full || Mode == LAO},
\item a perturbation phase guarded by \code{Mode == Full || Mode == PO}.
\end{itemize}
Therefore, perturbation-only rendering is achieved by instantiating the kernel
with a mode that includes only the perturbation path (e.g.\ \code{LAv2Mode::PO}).
In this configuration:
\begin{itemize}
\item \code{DeltaSub0} is computed from the pixel location,
\item \code{DeltaSubN} remains initialized to \(\Delta z_0 = 0\),
\item the kernel runs the perturbation loop, reconstructing \(z\) each step for
      bailout tests,
\item the final escape-time count is written to \code{OutputIterMatrix}.
\end{itemize}
This provides a complete Mandelbrot renderer based purely on reference-orbit
perturbation, without relying on any linear-approximation hierarchy.


\section{Approximation-Based Orbit Acceleration}
\label{sec:approx-accel}

At deep zoom, the cost of iterating high-precision types for every pixel can be
dominant. The remaining components described below are still in service of the
same rendering goal: compute escape-time for \(z_{n+1}=z_n^2+c\), but by reusing
a \emph{reference orbit} and evolving \emph{deltas} for nearby pixels.

\subsection{Bilinear approximation (BLA) for orbit deltas}
\label{sec:bilinear-approx}

\subsubsection{Reference orbit and delta formulation}
Let the reference parameter be \(c_\star\) with orbit
(see \cref{sec:perturbation-concept} for the general perturbation framework):
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a nearby pixel parameter \(c = c_\star + \Delta c\), define:
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star,\qquad
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star.
\]
Expanding the recurrence gives:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right)\\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
When \(\Delta z_n\) remains small, the quadratic term can be neglected,
yielding a linearized update:
\begin{equation}
\Delta z_{n+1} \approx A_n\,\Delta z_n + B_n\,\Delta c,\qquad
A_n = 2z_n^\star,\quad B_n = 1.
\end{equation}
BLA generalizes this into precomputed multi-step maps that \emph{jump} multiple
iterations while maintaining an explicit validity bound.

\subsubsection{What \code{BLA<T>} stores}
A \code{BLA<T>} instance stores two complex coefficients:
\[
A = A_x + iA_y,\qquad B = B_x + iB_y,
\]
plus:
\begin{itemize}
  \item \code{r2}: a squared-radius validity bound used during lookup,
  \item \code{l}: the number of Mandelbrot iterations summarized by this step.
\end{itemize}
These objects exist to accelerate \emph{escape-time evaluation} by evolving
\(\Delta z\) cheaply for many pixels, then reconstructing \(z \approx z^\star +
\Delta z\) to perform bailout checks consistent with Mandelbrot rendering.

\subsubsection{Applying a step: complex multiply-add}
The method \code{getValue(RealDeltaSubN, ImagDeltaSubN, RealDeltaSub0, ImagDeltaSub0)}
applies:
\[
\Delta z \leftarrow A\,\Delta z + B\,\Delta c,
\]
expanded into real arithmetic:
\begin{align}
\Re(\Delta z') &= A_x \Re(\Delta z) - A_y \Im(\Delta z) + B_x \Re(\Delta c) - B_y \Im(\Delta c), \\
\Im(\Delta z') &= A_x \Im(\Delta z) + A_y \Re(\Delta z) + B_x \Im(\Delta c) + B_y \Re(\Delta c).
\end{align}

\subsubsection{Composing steps to build longer jumps}
If a first step maps \(\Delta z \mapsto A_1\Delta z + B_1\Delta c\) and a second maps
\(\Delta z' \mapsto A_2\Delta z' + B_2\Delta c\), the composition is:
\[
A_{\text{new}} = A_2 A_1,\qquad B_{\text{new}} = A_2 B_1 + B_2.
\]
In the code (\code{BLA.h}), these two steps are passed as the parameters
\code{x} and \code{y} to \code{getNewA} and \code{getNewB}, corresponding to
step~1 and step~2 respectively.
This supports a hierarchy of step sizes (often powers of two) for quickly
advancing delta orbits while rendering the escape-time field.

\subsubsection{GPU lookup: selecting a valid aligned step}
A GPU-side helper such as \code{GPU\_BLAS} stores the hierarchy and selects a
step that is both \emph{aligned} with the current iteration index and
\emph{valid} under the current bound check (typically comparing a computed
squared-magnitude proxy \code{z2} against \code{r2}). When a step is valid, the
renderer can advance the orbit by \code{l} iterations at a cost far below
performing \code{l} full high-precision Mandelbrot updates.

\subsection{LA v2 linear approximation with perturbation (HDR kernel)}
\label{sec:la-v2-perturb}

This kernel family combines staged linear-approximation steps with a
perturbation finisher loop against a stored reference orbit
(see \cref{sec:perturb-only} for the perturbation-only variant). The rendering
objective remains escape-time Mandelbrot evaluation; the kernel accelerates
that evaluation by evolving deltas and periodically reconstructing \(z\) to
perform bailout checks.

\subsubsection{Parameter delta per pixel}
Each pixel constructs a parameter offset \(\Delta c\) relative to a selected
reference center (sign conventions may incorporate the image \(Y\)-flip):
\begin{align}
\Delta c_x &= dx\cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy\cdot Y - \texttt{centerY},
\end{align}
and packs \(\Delta c=\Delta c_x+i\Delta c_y\) into \code{DeltaSub0}.

\subsubsection{Delta state and reconstruction}
The kernel maintains \(\Delta z_n\) in \code{DeltaSubN} and reconstructs an
absolute orbit estimate using the stored reference orbit sample
\(z_j^\star\):
\[
z \approx z_j^\star + \Delta z.
\]
Escape-time rendering then proceeds by applying approximation steps when valid,
or performing perturbation updates otherwise, while periodically testing the
bailout condition on the reconstructed \(z\).

\subsubsection{Perturbation update}
Given a reference sample \(z^\star=x^\star+iy^\star\) and \(\Delta z=\Delta
x+i\Delta y\), the perturbation form is:
\[
\Delta z \leftarrow \Delta z\cdot(2z^\star+\Delta z) + \Delta c.
\]
In real arithmetic, with temporaries corresponding to \((2x^\star+\Delta x)\)
and \((2y^\star+\Delta y)\), this yields:
\begin{align}
\Delta x' &= \Delta x\,(2x^\star+\Delta x) - \Delta y\,(2y^\star+\Delta y) + \Delta c_x, \\
\Delta y' &= \Delta x\,(2y^\star+\Delta y) + \Delta y\,(2x^\star+\Delta x) + \Delta c_y,
\end{align}
where both right-hand sides use the pre-update values of \(\Delta x\) and
\(\Delta y\) (the code saves these as \code{DeltaSubNXOrig} and
\code{DeltaSubNYOrig}).

\subsubsection{Escape test}
After updating \(\Delta z\), reconstruct \(z\approx z^\star+\Delta z\) and test:
\[
|z|^2 = x^2+y^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\]
For HDR/expanded types, norms and comparisons are performed using reduced values
and specialized reduced comparators, preserving meaningful bailout decisions at
deep zoom.



\section{Disk-Backed Growable Vectors with Stable Virtual Addresses}
\label{sec:disk_backed_growable_vectors}

High-resolution Mandelbrot rendering and reference-orbit techniques (\cref{sec:ref-orbit-calc}) impose
extreme demands on memory management. At deep zoom levels, a single authoritative
reference orbit may contain tens of billions of elements; for example, an orbit
with $2.7\times10^{10}$ iterations already implies a logical memory footprint far
beyond what can be held resident in RAM, even with compact per-iteration storage.
At this scale, both the cost of allocation and the cost of relocation become
dominant design constraints.

Several subsystems in \FractalShark{}—including orbit logging, metadata streams,
debug outputs, and the custom heap allocator—require \emph{growable} storage
while simultaneously maintaining \emph{stable virtual addresses} for previously
returned pointers. This requirement arises naturally when the program hands out
interior pointers into a buffer (e.g., into an orbit record array or a heap
arena). Any reallocation-and-copy strategy would invalidate those pointers and
would require copying enormous amounts of data, which is computationally
infeasible at multi-billion-element scales.

In addition to scale and pointer stability, persistence is a first-class
concern. Reference orbits are expensive to compute but comparatively cheap to
reuse. If orbit data can be left resident on disk and later reloaded simply by
mapping the same backing file back into the process address space, recomputation
can be avoided entirely. This enables efficient multi-pass rendering workflows,
interactive debugging, and reuse of authoritative reference orbits across program
invocations.

To address these requirements, we implement a
\texttt{GrowableVector<EltT>} abstraction that combines (i) a fixed,
virtually-contiguous address range reserved up front and (ii) incremental growth
within that range by committing additional pages on demand. Growth is performed
in place, so the base address never changes and no copying is required as
capacity increases. The abstraction supports two complementary backing modes:

\begin{enumerate}
  \item \textbf{Anonymous (pagefile-backed) growth.} A large virtual region is
  reserved once via \texttt{VirtualAlloc(..., MEM\_RESERVE, ...)} and pages are
  subsequently committed within that region using
  \texttt{VirtualAlloc(base, bytes, MEM\_COMMIT, ...)}. Because each commit
  targets the original base address, successful growth preserves
  \texttt{m\_Data} and does not require relocation.

  \item \textbf{File-backed (disk-backed) growth.} The reserved address range is
  backed by a disk file and mapped into the process using a section object. In
  this mode, pages are demand-paged from the backing file and become resident
  only as accessed, substantially reducing peak commit requirements when the
  logical capacity greatly exceeds physical memory. When persistence is enabled,
  the same backing file can later be remapped to recover the stored data without
  recomputation.
\end{enumerate}

\subsection{Stable-address growth via reserve-and-commit}
\label{sec:reserve_commit}

At initialization time, the vector establishes an address anchor:
\begin{itemize}
  \item In anonymous mode, the anchor is created by reserving a large region
  (typically sized to a fraction of physical memory, or an explicit override)
  and recording the returned base pointer \texttt{m\_Data}.
  \item In file-backed mode, the anchor is created by mapping a section at a
  base address and thereafter requiring all remaps/extends to preserve that base.
\end{itemize}

Growth proceeds by increasing the logical capacity \texttt{m\_CapacityInElts}
without moving data. In anonymous mode, this corresponds to committing
additional pages within the reserved region. In file-backed mode, it corresponds
to extending the underlying section and allowing the view to cover the expanded
range. Critically, in both cases we \emph{never} allocate a second buffer and
copy: the address stability is enforced by explicitly requesting the existing
base address and treating any base-address change as a hard error.

\subsection{File-backed mapping for low-commit large buffers}
\label{sec:file_backed_mapping}

When a filename is provided (or a temporary filename is generated), the vector
opens or creates a backing file and constructs a section object over it. We use
native NT section APIs (e.g., \texttt{NtCreateSection},
\texttt{NtMapViewOfSection}, and \texttt{NtExtendSection}) to obtain a
reserve-style mapping suitable for later extension.

Conceptually, the pipeline is:
\begin{enumerate}
  \item \textbf{Open/Create backing file.} The file can be persistent (saved
  results) or temporary (e.g., created with delete-on-close semantics when the
  data is not meant to be retained).
  \item \textbf{Create section.} A section is created with read/write
  protection. For growable vectors, the section is configured so that the view
  can be reserved and later extended.
  \item \textbf{Map a view to establish the base address.} The first mapping
  produces the stable anchor pointer \texttt{m\_Data}. Subsequent growth must
  preserve this base.
  \item \textbf{Extend on demand.} When capacity increases, the section is
  extended to the new byte size. Because the base mapping remains fixed, all
  previously returned pointers remain valid.
\end{enumerate}

This approach has two key practical benefits:
\begin{itemize}
  \item \textbf{Stable pointers with large logical capacity.} The program may
  reserve and grow to very large capacities (hundreds of GiB or more) without
  the pointer invalidation inherent in \texttt{realloc}-style strategies.
  \item \textbf{Reduced peak commit pressure.} The OS can keep most of the
  logical buffer non-resident until touched; pages are faulted in as accessed
  and can be evicted back to disk under memory pressure. This is particularly
  effective when the workload has a large addressable dataset but a much smaller
  active working set.
\end{itemize}

\subsection{Trimming and persistence policies}
\label{sec:trim_persist}

The vector supports multiple policies that control whether data is persisted:
\begin{itemize}
  \item \textbf{Persistent save (\texttt{EnableWithSave}).} Upon \texttt{Trim()},
  the backing file is truncated to match the used size, ensuring the on-disk
  representation reflects the logical content exactly.
  \item \textbf{Temporary file-backed (\texttt{EnableWithoutSave}).} Upon
  \texttt{Trim()}, the view can be remapped to match the used size while
  \emph{reusing the same base address}. This reduces the mapped span without
  sacrificing pointer stability, and the file is automatically deleted when the
  handle closes.
  \item \textbf{Anonymous (\texttt{DontSave}).} Trimming is a logical operation
  (used size changes) while the reserved region remains as the stable anchor.
\end{itemize}

\subsection{Using the same mechanism for the heap arena}
\label{sec:heap_uses_growable_vector}

The custom heap allocator builds its arena on top of the same
\texttt{GrowableVector<uint8\_t>} abstraction. The heap is initialized by
creating a file-backed growable vector for the heap arena and committing an
initial chunk of memory. When the allocator needs more space, it expands the
arena by growing the underlying vector in-place and then updating the heap's
wilderness block (coalescing if possible). Because the arena grows within a
reserved and stable virtual range, all heap pointers returned to the program
remain valid across heap expansions, and expansion does not require copying or
relocating the heap.

\subsection{Portability and backing-file considerations}
\label{sec:disk_backed_portability_notes}

The implementation described above relies on Windows-specific virtual memory and
section-mapping semantics, in particular the ability to reserve a virtually
contiguous address range and to grow into that range while preserving a fixed
base address. On Windows, this is achieved using native section objects backed by
ordinary (non-sparse) files whose size is explicitly extended as the logical
capacity grows.

An important consequence of this design choice is that the backing file itself
physically grows on disk as the vector expands, even if large portions of the
mapped address space are never touched. Windows mitigates the runtime memory
impact by demand-paging pages into RAM and evicting them back to disk under
memory pressure, but the on-disk footprint still reflects the full logical extent
of the section. This behavior is acceptable—and often desirable—on Windows,
where file-backed sections are tightly integrated with the operating system's
paging and commit model.

More specifically, Windows treats a section backed by a regular (non-sparse)
file with a well-defined size as a complete and deterministic backing store for
any page in the mapped range. Page faults are resolved by reading from the file,
and evicted pages can be written back to the same file without requiring
additional allocation or metadata decisions at fault time. By explicitly
growing the backing file as capacity increases, the implementation ensures that
the backing storage for every potential page already exists. This aligns with
Windows-native paging expectations and simplifies correctness guarantees around
section growth, page-in, and page-out behavior.

On other platforms, or in alternative designs, a \emph{large sparse file} may be
a more appropriate backing store for growable, disk-backed vectors. Sparse files
allow the logical file size to greatly exceed the actual disk space consumed,
allocating physical blocks only for regions that are written. When combined with
memory-mapped I/O, this can preserve the core advantages of the approach—stable
virtual addresses and low resident memory usage—while also minimizing disk space
consumption for unused regions.

The present implementation intentionally does not rely on sparse-file behavior,
favoring predictability and robustness of paging semantics over minimizing
nominal disk usage. This choice, while well aligned with Windows-native behavior,
has implications for portability: environments that do not support extending a
mapped view in place, or that impose stricter constraints on file-backed mappings
(such as Wine or certain Unix-like kernels), may require alternative strategies.
Possible approaches include explicit sparse-file usage, preallocation of large
sparse sections, or fallback allocators that relax the stable-address requirement
in exchange for broader compatibility.



\section{Periodic Point Detection (Feature Finder)}
\label{sec:feature-finder}

Given a viewer location and a search radius, \FractalShark{} can automatically
locate the center of a nearby mini-Mandelbrot copy---i.e.\ the unique parameter
$c$ at which the critical orbit is exactly periodic.  This capability is
implemented by the \code{FeatureFinder} class (files
\code{FeatureFinder.h}/\code{FeatureFinder.cpp}), whose design is heavily
influenced by the periodic-point finder in the Imagina fractal viewer.  The
algorithm proceeds in two phases:
\begin{enumerate}
\item \textbf{Phase~A (T-space).}
  Detect the candidate period $p$ and perform a coarse Newton iteration using
  the renderer's native floating-point type~$T$ (which may be \code{float},
  \code{double}, or \code{HDRFloat}).
\item \textbf{Phase~B (MPF refinement).}
  Polish the candidate to arbitrary precision using GMP \code{mpf\_t}
  arithmetic, employing a mixed Newton--Halley iteration with precision
  management modeled after Imagina.
\end{enumerate}
Three evaluation back-ends are available for Phase~A---direct iteration,
perturbation theory (PT), and linear approximation (LA)---so that the finder
can operate at any zoom depth supported by the renderer.

\subsection{Motivation: mini-Mandelbrot copies}
\label{subsec:ff-motivation}

The Mandelbrot set contains infinitely many small-scale copies of itself,
commonly called \emph{mini-brots}.  Each copy is the nucleus of a hyperbolic
component of period~$p$: a connected region of the parameter plane in which the
critical orbit $z_0=0,\;z_{n+1}=z_n^2+c$ converges to an attracting cycle of
exact period~$p$.  The center of the component---the \emph{nucleus}---is the
unique parameter $c^*$ satisfying
\begin{equation}\label{eq:ff-nucleus}
z_p(c^*) = 0,\qquad z_0 = 0,
\end{equation}
where $z_p$ denotes the $p$-fold iterate of the critical point.

Locating these nuclei is useful for several reasons:
\begin{itemize}
\item \textbf{Reference orbit placement.}  A reference orbit (\cref{sec:ref-orbit-calc}) centered at a
  nucleus is exactly periodic, eliminating long-term drift and improving the
  accuracy of perturbation-based rendering (\cref{sec:perturbation-concept}) nearby.
\item \textbf{Automatic navigation.}  Given a user click, the finder can snap
  to the nearest mini-brot center and report its period and intrinsic size,
  enabling one-click deep-zoom targeting.
\item \textbf{Scale estimation.}  The derivative $\partial z_p/\partial c$ at
  the nucleus determines the local magnification scale of the mini-brot
  relative to the full set, which informs precision requirements for rendering.
\end{itemize}
At extreme zoom depths the nucleus coordinates may require thousands of bits of
precision, so the finder must combine fast low-precision search with
high-precision refinement.

\subsection{Design and implementation}
\label{subsec:ff-design}

\paragraph{Class template.}
\code{FeatureFinder<IterType, T, PExtras>} is a final class template
parameterized by the iteration-count type (\code{uint32\_t} or
\code{uint64\_t}), the working floating-point type~$T$, and a
\code{PerturbExtras} tag that selects compression behavior.  It inherits
convenience typedefs from \code{TemplateHelpers} and defines a nested
\code{Params} struct with tunable constants (maximum Newton iterations,
relative step tolerances $2^{-40}$ and $2^{-80}$, an optional absolute
residual accept threshold, and a print flag).

\paragraph{Evaluator policies (strategy pattern).}
Phase~A must iterate the critical orbit to detect the period and compute
derivatives.  Three evaluator structs encapsulate this:
\begin{itemize}
\item \code{DirectEvaluator} --- iterates $z\leftarrow z^2+c$ from scratch,
  suitable when no reference orbit is available.
\item \code{PTEvaluator} --- uses a precomputed reference orbit and
  perturbation delta $\Delta c$ to evaluate the orbit, decompressing
  reference data on the fly via \code{RuntimeDecompressor}.
\item \code{LAEvaluator} --- uses the linear-approximation (LA) coefficients
  from \code{LAReference} to skip early iterations, falling back to
  perturbation for the remaining tail.
\end{itemize}
Each evaluator exposes a single templated method \code{Eval<FindPeriod>}.
When \code{FindPeriod=true}, it searches for the smallest $n$ at which a
periodicity trigger fires; when \code{FindPeriod=false}, it evaluates at a
known period and returns the residual, $\partial z/\partial c$, and
$z_{\mathrm{coeff}}$.

\paragraph{Common Newton loop (\code{FindPeriodicPoint\_Common}).}
All three \code{FindPeriodicPoint} overloads delegate to this template,
which:
\begin{enumerate}
\item Calls the evaluator with \code{FindPeriod=true} to obtain a candidate
  period~$p$.
\item Applies an initial Newton correction
  $c \leftarrow c - z_p / (\partial z_p/\partial c)$ in high precision
  (the canonical $c$ is maintained as a \code{HighPrecision} value and
  projected to $T$ for each evaluation).
\item Runs up to \code{MaxNewtonIters} Newton steps in $T$-space, stopping when
  either
  $|\Delta c|^2 \le |c|^2 \cdot (2^{-40})^2$ (relative step) or
  $|z_p|^2 \le |c|^2 \cdot (2^{-40})^2$ (relative residual).
\item Performs a final correction pass and stores the result as a
  \code{PeriodicPointCandidate} inside \code{FeatureSummary}, along with the
  MPF precision needed for Phase~B.
\end{enumerate}

\paragraph{Phase~B: MPF refinement (\code{RefinePeriodicPoint\_HighPrecision}).}
This method retrieves the candidate from \code{FeatureSummary}, converts
its coordinates to GMP \code{mpf\_t} at the candidate's stored precision,
and calls the Imagina-style mixed Newton--Halley polisher described in
\cref{subsec:ff-newton,subsec:ff-halley,subsec:ff-mixed-prec}.  On
completion the refined coordinates are written back to \code{FeatureSummary}
and the candidate is marked as refined.

\paragraph{Parallel orbit evaluation.}
The high-precision forward evaluation
(\code{EvaluateCriticalOrbitAndDerivs}) must perform several large
\code{mpf\_mul} operations per iteration.  To overlap these, the code spawns
seven spin-based worker threads (four for derivative-precision products,
three for coordinate-precision products).  Workers spin on per-slot
\code{std::atomic<uint64\_t>} generation counters; the main thread
publishes operand pointers, bumps the generation, and spin-waits on a
matching done counter.  Each slot is cache-line-aligned (\code{alignas(64)})
to avoid false sharing.

\subsection{Newton--Raphson iteration}
\label{subsec:ff-newton}

\paragraph{Root-finding formulation.}
Finding a period-$p$ nucleus amounts to solving $F(c)=0$ where
\begin{equation}\label{eq:ff-Fc}
F(c) \;=\; z_p(c),\qquad
z_0 = 0,\quad z_{n+1} = z_n^2 + c.
\end{equation}
Newton's method produces the update
\begin{equation}\label{eq:ff-newton-step}
\Delta c \;=\; \frac{F(c)}{F'(c)}
         \;=\; \frac{z_p}{\partial z_p / \partial c},
\qquad c \leftarrow c - \Delta c.
\end{equation}
Because $z_{n+1}=z_n^2+c$, the derivative satisfies the linear recurrence
\begin{equation}\label{eq:ff-dzdc-recurrence}
\frac{\partial z_{n+1}}{\partial c}
  = 2\,z_n\,\frac{\partial z_n}{\partial c} + 1,
\qquad \frac{\partial z_0}{\partial c} = 0,
\end{equation}
so both $z_n$ and $\partial z_n/\partial c$ are available at the end of a
single forward pass of $p$~iterations.

\paragraph{The \code{zcoeff} product.}
In addition to $\partial z_p/\partial c$, the code tracks a multiplicative
coefficient
\begin{equation}\label{eq:ff-zcoeff}
w_n = \prod_{k=0}^{n-1} 2\,z_k,
\end{equation}
with the convention $w_0=1$.  This product satisfies
$\partial z_n/\partial c = w_n + \text{lower-order terms}$ and is used later
to compute the intrinsic scale of the mini-brot (see
\cref{subsec:ff-intrinsic-radius}).  In the code it is updated as
\code{zcoeff *= 2*z} at each iteration, matching the recurrence for the
leading factor of the derivative.

\paragraph{Convergence.}
Standard Newton--Raphson converges quadratically near a simple root.  The
Phase~A loop in \code{FindPeriodicPoint\_Common} uses two independent
stopping criteria, whichever triggers first:
\begin{enumerate}
\item \emph{Relative step:}
  $|\Delta c|^2 \le |c|^2 \cdot \epsilon_{\mathrm{step}}^2$
  with $\epsilon_{\mathrm{step}} = 2^{-40}$.
\item \emph{Relative residual:}
  $|z_p|^2 \le |c|^2 \cdot \epsilon_{\mathrm{diff}}^2$
  with $\epsilon_{\mathrm{diff}} = 2^{-40}$.
\end{enumerate}
An optional absolute residual accept (\code{Eps2Accept}) provides a third
early-out that can be enabled via \code{Params}.

\paragraph{Period detection trigger.}
Before Newton iteration begins, the evaluator must identify the candidate
period.  The criterion used is: at iteration~$n$, if
\begin{equation}\label{eq:ff-period-trigger}
|z_n|^2 \;<\; R^2 \cdot \left|\frac{\partial z_n}{\partial c}\right|^2,
\end{equation}
then $p = n$ is accepted as the candidate period, where $R$ is the search
radius.  This test detects the moment the orbit passes close to the origin
relative to the local stretching factor $|\partial z/\partial c|$, which is
a hallmark of near-periodic behavior.  A dynamic tightening mechanism
(implemented in the \code{PeriodicityPP} helper) progressively shrinks the
effective $R^2$ during the scan to reduce false triggers.

\subsection{Halley's method}
\label{subsec:ff-halley}

The second derivative $F'' = \partial^2 z_p/\partial c^2$ is computed during
every forward evaluation, even when the Halley step itself is not taken.  This
is because $F''$ serves two distinct purposes in the refinement loop:

\begin{enumerate}
\item \textbf{Halley correction.}  When the Halley gate (\cref{eq:ff-halley-gate})
  is satisfied, $F''$ enables a cubic-convergence step that can save expensive
  forward evaluations compared to plain Newton (see below).
\item \textbf{Error estimator.}  The stopping criterion
  (\cref{eq:ff-err}) uses $|F''|^2$ to estimate the residual after
  each step.  Without the second derivative, the code would have no reliable
  way to judge convergence short of comparing successive iterates---which
  requires an extra forward evaluation per check.
\end{enumerate}

Because $F''$ is carried in \code{HDRFloat<double>} (${\sim}53$~bits of
mantissa with a wide exponent), it adds only cheap low-precision arithmetic to
the forward evaluation: a few \code{HDRFloat} multiplies and additions per
iteration of the period loop, compared with the expensive \code{mpf\_t}
multiplies needed for $z$ and $\partial z/\partial c$.  The cost is therefore
negligible relative to the rest of the evaluation, making it worthwhile to
compute unconditionally.

Phase~B optionally upgrades from Newton to Halley iteration for cubic
convergence.  The Halley step for $F(c) = z_p(c)$ is
\begin{equation}\label{eq:ff-halley-step}
\Delta_H \;=\;
  \frac{2\,F(c)\,F'(c)}
       {2\bigl(F'(c)\bigr)^2 - F(c)\,F''(c)},
\qquad c \leftarrow c - \Delta_H,
\end{equation}
where $F'=\partial z_p/\partial c$ and $F''=\partial^2 z_p/\partial c^2$.

\paragraph{Second derivative recurrence.}
Differentiating \cref{eq:ff-dzdc-recurrence} once more gives
\begin{equation}\label{eq:ff-d2zdc2}
\frac{\partial^2 z_{n+1}}{\partial c^2}
  = 2\left(\frac{\partial z_n}{\partial c}\right)^{\!2}
  + 2\,z_n\,\frac{\partial^2 z_n}{\partial c^2},
\qquad \frac{\partial^2 z_0}{\partial c^2} = 0.
\end{equation}
In the code this quantity is carried as a pair of \code{HDRFloat<double>}
values (real and imaginary parts), which provide a wide exponent range at low
mantissa precision.  The coordinate-precision quantities $z$ and
$\partial z/\partial c$ are stored in \code{mpf\_t}, while
$\partial^2 z/\partial c^2$ remains in HDR throughout the forward
evaluation.  When the Halley step is actually computed, the HDR second
derivative is promoted to \code{mpf\_t} at coordinate precision.

\paragraph{Halley gate.}
Halley's step is only beneficial when $F''$ is well-conditioned relative to
$F$ and $F'$.  The code uses the dimensionless ratio
\begin{equation}\label{eq:ff-halley-gate}
\rho^2 \;=\;
  \frac{|F|^2\,|F''|^2}{|F'|^4}
\end{equation}
and activates Halley only when $\rho^2 < 2^{-12}$.  When $\rho^2$ is
small the denominator $2(F')^2 - F\,F''$ is dominated by the $(F')^2$ term,
so the Halley correction is a small, well-determined perturbation of the
Newton step.  If the Halley denominator turns out to be exactly zero (or the
gate is not satisfied), the code falls back to a plain Newton step for that
iteration.

\paragraph{Is Halley worthwhile given low-precision $F''$?}
The second derivative $F''$ is accumulated in \code{HDRFloat<double>}
throughout the forward evaluation, providing only ${\sim}53$~bits of
mantissa (with a wide exponent range), while $F$ and $F'$ are carried in
\code{mpf\_t} at hundreds or thousands of bits.  This precision mismatch
limits---but does not eliminate---the benefit of the Halley correction.

The Halley step can be decomposed as
\begin{equation}\label{eq:ff-halley-decomp}
\Delta_H \;=\; \Delta_N \cdot \frac{1}{1 - \frac{F \cdot F''}{2(F')^2}}
\;=\; \Delta_N \cdot \frac{1}{1 - \tfrac{1}{2}\rho^2 e^{i\theta}},
\end{equation}
where $\Delta_N = F/F'$ is the Newton step and $\rho^2$ is the dimensionless
ratio from \cref{eq:ff-halley-gate}.  Since $F''$ is recomputed from scratch
at each Newton iterate via the recurrence~\cref{eq:ff-d2zdc2}, it provides a
fresh ${\sim}53$-bit approximation of the true second derivative at the
current scale.  The Halley correction factor therefore adjusts the Newton step
by a relative amount of order~$\rho^2$, with that correction accurate to
${\sim}53$~bits.

In terms of convergence, suppose Newton has produced $n$~correct bits of the
nucleus coordinate.  A plain Newton step would yield ${\sim}2n$~correct bits
(quadratic convergence), while a full-precision Halley step would yield
${\sim}3n$ (cubic convergence).  With $F''$ limited to ${\sim}53$~bits of
mantissa, the Halley step yields ${\sim}\min(3n,\; 2n+53)$~correct bits.
That is, Halley contributes up to ${\sim}53$~additional correct bits per step
beyond what Newton alone provides.  This bonus applies every iteration, not
just in early convergence: each forward evaluation produces a fresh $F''$ at
the current residual's scale, so the ${\sim}53$-bit correction remains
meaningful throughout.

Whether this trade-off is net positive depends on the period~$p$.  Each
Newton/Halley iteration requires a full forward evaluation ($p$~iterations of
$z = z^2 + c$ plus the derivative recurrence), so the per-iteration cost
scales with~$p$.  For large~$p$, saving even one forward evaluation by
converging in fewer Halley steps is significant, and the ${\sim}53$ extra bits
per step can eliminate one or more expensive iterations compared to pure
Newton.  For small~$p$, the additional arithmetic for the second derivative and
Halley denominator is modest but the savings from faster convergence are also
small.  A full-precision $F''$ (carried in \code{mpf\_t}) would recover true
cubic convergence at the cost of additional high-precision multiplies per
iteration; whether that cost is justified remains an open question.

\subsection{Mixed-precision refinement}
\label{subsec:ff-mixed-prec}

The Phase~B polisher (\code{RefinePeriodicPoint}) operates entirely in GMP
\code{mpf\_t} arithmetic but uses two distinct precision levels to balance cost
and accuracy, following the strategy used in Imagina.

\paragraph{Coordinate vs.\ derivative precision.}
The candidate coordinate $c$ and the orbit value $z_p$ are maintained at full
\emph{coordinate precision} $P_c$ (typically hundreds to tens of thousands of
bits, determined by the zoom depth).  The derivative
$\partial z_p/\partial c$ is maintained at a reduced \emph{derivative
precision} $P_d$ chosen by
\begin{equation}\label{eq:ff-deriv-prec}
P_d = \operatorname{clamp}\!\left(
  \frac{-E_{\mathrm{scale}} + 32}{4},\;
  256,\;
  P_c + E_{\max}^{|c|} + 32
\right),
\end{equation}
where $E_{\mathrm{scale}}$ is the base-2 exponent of the scale factor
$\text{Scale} \approx 1/|w_p \cdot \partial z_p/\partial c|$ and
$E_{\max}^{|c|}$ is the exponent of $\max(|\Re c|,|\Im c|)$.
The rationale is that the derivative need only be accurate enough to produce
a Newton/Halley step whose relative error is smaller than the current
residual; carrying it at full coordinate precision would roughly double the
cost of each forward evaluation without improving the convergence rate.

\paragraph{Error estimator and stopping criterion.}
After each step $\Delta c$, the code computes an error proxy entirely in
\code{HDRFloat<double>}:
\begin{equation}\label{eq:ff-err}
\mathrm{err} \;=\;
  \frac{|\Delta c|^4 \cdot |F''|^2}{|F'|^2}.
\end{equation}
This quantity approximates the leading-order truncation error of the Newton
step (or, more precisely, the residual that would remain after two successive
Newton steps of similar quality).  Iteration stops when
\begin{equation}\label{eq:ff-stop}
-\lfloor \log_2(\mathrm{err}) \rfloor \;\ge\; 2\,P_c,
\end{equation}
meaning the estimated error occupies fewer than $2P_c$ bits---well below the
representable precision of the coordinate.

\paragraph{Final correction and accept/reject.}
After the main loop converges (or exhausts the iteration budget), a single
additional Newton step is applied as a final correction.  The refined $c$ is
then checked against the initial seed $c_0$: if
$|c - c_0|^2 > R^2$ (the squared search radius, carried in \code{mpf\_t}),
the result is rejected and $c$ is reset to~$c_0$.  This guard prevents the
iteration from wandering to a distant nucleus outside the user's region of
interest.

\subsection{Intrinsic radius and scale}
\label{subsec:ff-intrinsic-radius}

Once the nucleus $c^*$ and its period~$p$ are known, the local magnification
of the mini-brot copy is characterized by
\begin{equation}\label{eq:ff-scale}
\mathrm{Scale} \;=\;
  \frac{1}{\bigl|w_p \cdot \partial z_p/\partial c\bigr|},
\end{equation}
where $w_p = \prod_{k=0}^{p-1} 2\,z_k$ is the \code{zcoeff} product
evaluated at $c^*$.  The code reports an \emph{intrinsic radius}
\begin{equation}\label{eq:ff-intrinsic-radius}
r_{\mathrm{int}} = 4 \cdot \mathrm{Scale}
  = \frac{4}{\bigl|w_p \cdot \partial z_p/\partial c\bigr|}.
\end{equation}
Geometrically, $r_{\mathrm{int}}$ estimates the diameter of the
period-$p$ hyperbolic component in the parameter plane.  It is stored
as a \code{HighPrecision} value in \code{FeatureSummary} and can be used by
the renderer to set an appropriate zoom level or to determine the additional
bits of precision needed to render the component's interior.

\subsection{Threading model and GPU prospects}
\label{subsec:ff-threading}

\paragraph{Current multi-threaded implementation.}
The dominant cost of the Feature Finder is the forward evaluation
\code{EvaluateCriticalOrbitAndDerivs}, which iterates $z_{n+1}=z_n^2+c$ and
the derivative $\partial z_p/\partial c$ for $p$ iterations at high precision
using GMP \code{mpf\_t} arithmetic.  Each iteration requires several
independent high-precision multiplies: three at coordinate precision
($z_{\mathrm{re}}^2$, $z_{\mathrm{im}}^2$, $z_{\mathrm{re}} \cdot
z_{\mathrm{im}}$) and four at derivative precision (the complex product
$\mathrm{dzdc} \cdot 2z$).  Because these multiplies are independent within a
single iteration, \FractalShark{} dispatches them to a pool of seven spin-based
worker threads (\code{NWORK~=~7}).  Synchronization uses cache-line-aligned
(\code{alignas(64)}) generation counters with atomic acquire/release
semantics---no mutexes or condition variables---so the main thread and workers
coordinate with minimal latency.  The main thread composes the results after
each batch completes and advances to the next iteration.

A single-threaded fallback (\code{EvaluateCriticalOrbitAndDerivsST}) with
identical logic is also provided.

The Newton outer loop in \code{FindPeriodicPoint\_Common} is inherently
sequential: each Newton step $c \leftarrow c - z_p / (\partial z_p / \partial
c)$ depends on the preceding evaluation's output, so successive steps cannot
overlap.

\paragraph{GPU acceleration via \code{HpSharkFloat}.}
The arithmetic bottleneck that motivates the seven-worker CPU pool is the same
bottleneck that the GPU reference orbit backend (\cref{sec:gpu-ref-orbit})
already addresses: high-precision multiplication at thousands of limbs.
\FractalShark{}'s existing \code{HpSharkFloat} pipeline performs NTT-based
multiplication on the GPU and has demonstrated roughly $10\times$ speedups over
multithreaded MPIR at 16384 limbs.

A natural extension would replace the CPU \code{mpf\_mul} calls inside the
forward evaluation with GPU kernel launches through the same
\code{HpSharkFloat} infrastructure.  The host would continue to drive the
Newton outer loop, but each forward evaluation---$p$ iterations of $z=z^2+c$
plus the derivative recurrence---would execute as GPU work.  This mirrors the
architecture of the reference orbit GPU backend, where the host orchestrates
iteration batches and the GPU performs the heavy arithmetic.

At high periods and deep zooms (thousands of 32-bit limbs), the per-multiply
GPU advantage would dominate the overall wall-clock time.  The Newton loop
typically converges in tens of steps, each requiring $p$ high-precision
multiplies, so the total arithmetic volume scales as $O(p \cdot N \log N)$ per
Newton step (where $N$ is the limb count).  For large $p$ and $N$, offloading
this to the GPU would yield substantial speedups over both the single-threaded
and seven-worker CPU implementations.
