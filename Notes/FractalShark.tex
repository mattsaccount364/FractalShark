\documentclass[12pt]{article}

% --- Encoding / fonts (fixes OT1 issues + improves monospace metrics) ---
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Math ---
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}

% --- Layout ---
\usepackage{geometry}
\geometry{a4paper, margin=1in}

% --- Better line breaking / fewer overfull boxes ---
\usepackage{microtype}
\emergencystretch=2em % last-resort "be less picky" knob for line breaks

% --- Links and cross-refs ---
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}

% --- Convenience for inline code that may need breaks ---
\newcommand{\code}[1]{\texttt{#1}}

\newcommand{\FractalShark}{FractalShark}
\newcommand{\FractalSharkItalic}{\textit{FractalShark}}

\title{\FractalSharkItalic{}: High-Performance Mandelbrot Rendering}
\author{Matthew Renzelmann}
\date{\today}

\begin{document}

\vspace*{\fill}

\begin{center}
\textit{These notes are a work-in-progress summary of
\FractalShark{} but are currently not in a well-edited or organized state.
Moreover, much of it is presently AI-generated and thus may contain errors or
inconsistencies.  That said, I've made some effort to review and correct it, so
it should be better than nothing.  I'll remove this warning once I'm better
satisfied with the content.}
\end{center}

\vspace*{\fill}

\maketitle

\makeatletter
\renewcommand\numberline[1]{#1\enspace}
\makeatother

\tableofcontents
\clearpage

\section{\FractalShark{} Overview}
\label{sec:goal}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{mandelbrot.png}
  \caption{The Mandelbrot set.}
  \label{fig:mandelbrot}
\end{figure}

\FractalShark{} is an interactive Mandelbrot set renderer focused on
\textbf{extreme deep-zoom exploration}, numerical correctness, and
algorithmic experimentation.  It supports zoom depths ranging from
conventional views to magnifications exceeding $10^{10000}$, while
exposing internal rendering choices and precision tradeoffs to the user.

\paragraph{Interactive navigation and view control.}
\FractalShark{} provides direct mouse-driven navigation, including centering
the view at an arbitrary point, zooming in and out at the cursor location,
stepping backward through navigation history, and invoking automatic zoom
modes.  Window management options allow toggling between windowed and
full-screen modes, including square-aspect rendering for precise analysis.

\paragraph{Built-in views for demonstration and validation.}
The application includes an extensive set of built-in views covering a wide
range of numerical regimes: GPU precision limits, high-period locations,
known hard points, historical bug cases, regression tests, and ultra-deep
zoom demonstrations.  These views serve both as showcases of capability and
as repeatable test cases for performance and correctness.

\paragraph{Explicit algorithm selection and transparency.}
A central design goal of \FractalShark{} is to make rendering algorithms
explicit and selectable.  Users may rely on an automatic algorithm selector
or manually choose among many rendering paths, including:
\begin{itemize}
  \item GPU-based low-zoom renderers with selectable iteration precision and
        multiple numeric formats (single-, dual-, and quad-limb 32- and 64-bit
        variants, as well as deeper representations discussed subsequently).
  \item Scaled perturbation algorithms for extending low-precision arithmetic
        to deeper zooms.
  \item Bilinear approximation (BLA v1) perturbation paths.
  \item Linear approximation (LAv2) pipelines, including full rendering,
        linear-approximation-only, and perturbation-only variants.
  \item Reference-compression–aware algorithms, including Imagina-compatible
        ``max'' compression formats.
  \item CPU-only algorithms for very high precision, verification, and
        fallback scenarios.
\end{itemize}

Many of these modes are exposed explicitly for testing and comparison, and
not all are intended as default or production-quality paths.

\paragraph{Linear approximation configuration.}
Linear approximation behavior can be configured for multithreaded or
single-threaded execution and tuned via presets that prioritize accuracy,
performance, or memory usage.  These controls reflect ongoing development
and experimentation with LA parameter tradeoffs.

\paragraph{Image quality and coloring.}
\FractalShark{} supports GPU antialiasing at multiple sample levels and a
flexible palette system.  Palettes may be selected from predefined themes,
generated randomly, and rendered at configurable color depths.  Some palette
features are intentionally limited or disabled where they are known to be
incomplete.

\paragraph{Iteration limits and precision control.}
Users can dynamically adjust iteration limits, switch between 32-bit and
64-bit iteration counters, and modify shallow-zoom iteration precision.
These controls are primarily relevant for low-zoom or testing scenarios and
are not universally applicable to all rendering algorithms.

\paragraph{Perturbation and reference-orbit management.}
\FractalShark{} places strong emphasis on reference-orbit reuse and
perturbation-based rendering.  It supports multiple perturbation strategies
(single-threaded, multithreaded, periodicity-assisted, and experimental GPU
variants), along with tools to clear, inspect, save, reload, and reuse
reference orbits.  Automatic and manual reference-orbit persistence is
supported via file-backed storage.

\paragraph{Memory management and scalability.}
To enable extreme zoom depths without exhausting system memory, \FractalShark{}
employs file-backed storage, optional reference compression, configurable
memory limits, and automatic cleanup policies.  These mechanisms are closely
tied to recent allocator and reference-orbit refactors discussed in the
project development history.

\paragraph{Diagnostics, benchmarking, and testing.}
The application includes facilities for displaying detailed rendering
parameters, running repeatable benchmarks, executing regression tests, and
comparing reference orbits (including Imagina-compatible formats).  These
features reflect FractalShark’s dual role as both a visualization tool and a
development platform.

\paragraph{Data export and interoperability.}
\FractalShark{} can save rendered images, high-resolution bitmaps, iteration
counts, and reference orbits in multiple text and compressed formats.  It
supports loading external reference orbits and matching Imagina-compatible
data for cross-tool validation.

\medskip
\noindent
Overall, \FractalShark{} functions as both a \textbf{deep-zoom Mandelbrot
explorer} and a \textbf{research and experimentation platform} for
high-precision fractal rendering, prioritizing transparency, correctness, and
performance.  It is not intended as a polished end-user application, but rather
as an experimental platform for exploring and validating advanced fractal
rendering algorithms.

\section{User Interface and Popup Menu Guide}
\label{sec:ui-popup-menu}

FractalShark can be driven \emph{entirely} from the keyboard and mouse, with the
right-click popup menu acting as the primary discovery and configuration
surface.  Day-to-day navigation (zooming, recentering, going back, iteration
adjustment, palette cycling, and several recomputation paths) is also exposed
through single-key shortcuts, so common workflows do not require opening the
menu.

The popup menu is context-sensitive and rebuilt dynamically each time it is
shown, so that checked, enabled, and disabled states accurately reflect the
current application settings and rendering state.  The menu can be invoked by
mouse (\textbf{right-click}) or from the keyboard via the standard Windows
context-menu invocation (\textbf{Shift+F10} / \textbf{Menu} key).

\subsection{Basic Interaction}

The most common interactions are:
\begin{itemize}
  \item \textbf{Right-click}: Open the popup menu at the mouse cursor.
  \item \textbf{Left-click and drag}: Zoom into a rectangular region.
  \item \textbf{Keyboard shortcuts}:
    \begin{itemize}
      \item \texttt{z}: Zoom in at the mouse cursor.
      \item \texttt{Shift+Z}: Zoom out slightly.
      \item \texttt{b}: Return to the previous view.
      \item \texttt{- / =}: Decrease or increase the iteration limit.
    \end{itemize}
\end{itemize}

\subsection{Menu Structure and Behavior}

The popup menu is organized into logical groups and submenus. Items fall into
three distinct behavioral categories:

\begin{description}
  \item[Radio groups] Mutually exclusive choices where exactly one option is
  active at a time (for example, selecting a render algorithm or GPU antialiasing level).
  \item[Commands] Immediate actions that execute when selected and do not
  maintain a checked state.
\end{description}

Radio groups reflect the configuration in use, with the currently active option
checked. Selecting a different option automatically deselects the previous
choice and triggers any necessary reconfiguration.

\subsection{Top-Level Menu}

\begin{itemize}
  \item \emph{Show Help} (command): Displays an on-screen help dialog summarizing keyboard shortcuts.

  \item \emph{Navigate} (submenu):
    \begin{itemize}
      \item \emph{Back} (command): Navigate back to the previous view in history.
      \item \emph{Center View Here} (command): Centers the current view on the
      point under the cursor.
      \item \emph{Zoom In Here} (command): Zooms in toward the point under the cursor.
      \item \emph{Zoom Out} (command): Zooms out from the current view.
      \item \emph{Autozoom Default} (command): See \ref{par:autozoom} for details.
      \item \emph{Autozoom Max} (command): See \ref{par:autozoom} for details.
    \end{itemize}

  \item \emph{Built-In Views} (submenu): Loads a predefined view/location used
  for demonstrations, debugging, or performance testing.
    \begin{itemize}
      \item \emph{Help} (command): Pops up a message box that simply says this
      submenu contains built-in test views.
      \item \emph{Standard View} (command): Loads the default starting view: the
      classic Mandelbrot.
      \item \emph{\#1 -- \#whatever} (commands): Loads a specific predefined
      test view. Many entries encode expected precision limits, kernel behavior,
      or known failure cases in their label text.  Several views require intense
      computation and take a long time to render.  View \#27, for example, is a
      period ~28 billion point that takes hours to render and requires reference
      compression even with 128GB of RAM.  Exercise caution when selecting deep-zoom
      views, as they may cause long delays or high memory usage.

    \end{itemize}

  \item \emph{Recalculate, Reuse Reference} (command): Forces an immediate
  re-render using the current settings, reusing the existing reference orbit if
  possible.

  \item \emph{Toggle Repainting}: Enables or disables progressive
  repainting while a render is in progress. When disabled, no recalculation
  takes place on window resize or related, which can be helpful if you want a
  new window size but don't want to trigger a re-render.
  \item \emph{Toggle Window Size}: Toggles between windowed and
  full-screen behavior.  The window stays at the same aspect ratio by default.
  \item \emph{Toggle Window Size (Square)}: Toggles an alternate sizing
  mode constrained to a square aspect ratio.
  \item \emph{Minimize Window} (command): Minimizes the application window.

  \item \emph{Antialiasing} (radio group submenu): Selects the antialiasing
  level used for rendering.
    \begin{itemize}
      \item \emph{1x (fast)}
      \item \emph{4x}
      \item \emph{9x}
      \item \emph{16x (better quality)}
    \end{itemize}

  Note that antiasliasing as used here is not traditional GPU antialiasing.
  Instead, 4, 9, or 16 samples are explicitly computed per pixel and averaged
  together either on the CPU or GPU.  This approach is known as
  \emph{supersampling} and produces high-quality results at the cost of
  increased computation time and memory usage.  It's a naive approach---in
  principle, more sophisticated techniques could be implemented in the future,
  such as edge-aware sampling or adaptive sampling.

  \item \emph{Choose Render Algorithm} (radio group submenu): Selects the core
  Mandelbrot rendering strategy. This is a radio group: selecting one algorithm
  automatically deselects the others. Available choices may include CPU-based
  reference and linear-approximation renderers, multiple CUDA-based GPU
  renderers, and high-dynamic-range and experimental kernels. On systems without
  a working CUDA setup, CPU renderers can be selected as a fallback. In the
  event of a CUDA initialization failure, switching to a CPU algorithm followed
  by a recalculation may restore a usable image.

    \begin{itemize}
      \item \emph{Help} (command): Displays help for algorithm selection and
      related concepts.

      \item \emph{Auto (Default)} (radio): Automatically selects the
      default/recommended renderer.  Recommended.

      \emph{Note}: this option exhibits potentially-confusing behavior.  When
      selected, it does not lock in a specific algorithm; rather, it allows the
      system to choose the most appropriate algorithm based on current view
      parameters. The radio button for "Auto" will not be checked; instead, the
      actual algorithm in use will be checked.  Thus, selecting \emph{Auto} may
      appear to have no effect but if you navigate the submenus you'll find the
      algorithm currently in use is checked.  This design allows users to see
      what algorithm is active.

      At present, only three algorithms are eligible for automatic selection:
      \begin{itemize}
        \item \emph{1x32 GPU}: The default low-zoom GPU linear-approximation
              renderer.  No perturbation or linear approximation is used.
        \item \emph{1x32 GPU -- Perturbation Only}: The default
              perturbation-only GPU renderer for moderate zooms uses
              perturbation but no linear approximation.
        \item \emph{1x32 GPU -- LAv2}: A bit deeper, no high-dynamic range
              floats but does use linear-approximation renderer and
              perturbation.
        \item \emph{HDRx32 GPU -- LAv2}: Arbitrary depth, combines all known
              algorithmic optimizations.  This one can result in rendering
              artifacts at some deep locations due to numeric precision
              limitations.  "Auto" mode can't tell, so if you see artifacts, try
              HDRx2x32 or HDRx64 variants instead.  View \#26 is an example of a
              point that fails with HDRx32 but works correctly with HDRx64.

              See figure \ref{fig:auto-renderer-artifact} for an example of
              artifacts that can arise when using HDRx32 at extreme depths.
      \end{itemize}

      \item \emph{LA Parameters} (submenu): Controls linear-approximation (LA) configuration and tuning.
        \begin{itemize}
          \item \emph{Multithreaded (Default)} (radio): Uses multithreaded LA processing.
          \item \emph{Single Threaded} (radio): Uses single-threaded LA processing.
          \item \emph{Max Accuracy (Default)} (command): \textit{[TODO: unclear behavior/description]}
          \item \emph{Max Performance (Accuracy Loss)} (command): \textit{[TODO: unclear behavior/description]}
          \item \emph{Min Memory} (command): \textit{[TODO: unclear behavior/description]}
        \end{itemize}

      \item \emph{CPU-Only} (submenu): CPU renderers and CPU-driven perturbation variants.
        \begin{itemize}
          \item \emph{Very High Precision CPU} (radio)
          \item \emph{1x64 CPU} (radio)
          \item \emph{HDRx32 CPU} (radio)
          \item \emph{HDRx64 CPU} (radio)

          \item \emph{1x64 CPU -- Perturbation BLA} (radio)
          \item \emph{HDRx32 CPU -- Perturbation BLA} (radio)
          \item \emph{HDRx64 CPU -- Perturbation BLA} (radio)

          \item \emph{HDRx32 CPU -- Perturbation LAv2} (radio)
          \item \emph{HDRx64 CPU -- Perturbation LAv2} (radio)
          \item \emph{HDRx32 RC CPU -- Perturbation LAv2} (radio)
          \item \emph{HDRx64 RC CPU -- Perturbation LAv2} (radio)
        \end{itemize}

      \item \emph{Low-Zoom Depth} (submenu): GPU renderers intended for relatively modest zoom depths and lower precision requirements.
        \begin{itemize}
          \item \emph{Iteration Precision} (radio group submenu): Selects the iteration precision mode used by low-zoom GPU paths.
            \begin{itemize}
              \item \emph{4x (fast)} (radio)
              \item \emph{3x} (radio)
              \item \emph{2x} (radio)
              \item \emph{1x (better quality)} (radio)
            \end{itemize}

          \item \emph{1x32 GPU} (radio)
          \item \emph{2x32 GPU} (radio)
          \item \emph{4x32 GPU} (radio)
          \item \emph{1x64 GPU} (radio)
          \item \emph{2x64 GPU} (radio)
          \item \emph{4x64 GPU} (radio)
          \item \emph{HDRx32 GPU} (radio)
        \end{itemize}

      \item \emph{Scaled} (submenu): Perturbation renderers using a scaled formulation.
        \begin{itemize}
          \item \emph{1x32 GPU -- Perturbation Scaled} (radio)
          \item \emph{2x32 GPU -- Perturbation Scaled (broken)} (radio)
          \item \emph{HDRx32 GPU -- Perturbation Scaled} (radio)
        \end{itemize}

      \item \emph{Bilinear Approximation V1} (submenu): Perturbation renderers using the original bilinear approximation approach.
        \begin{itemize}
          \item \emph{1x64 GPU -- Perturbation BLA} (radio)
          \item \emph{HDRx32 GPU -- Perturbation BLA} (radio)
          \item \emph{HDRx64 GPU -- Perturbation BLA} (radio)
        \end{itemize}

      \item \emph{LA Only (for testing)} (submenu): Test configurations that run only the linear-approximation portion of the pipeline.
        \begin{itemize}
          \item \emph{1x32 GPU -- LAv2 -- LA only} (radio)
          \item \emph{2x32 GPU -- LAv2 -- LA only} (radio)
          \item \emph{1x64 GPU -- LAv2 -- LA only} (radio)
          \item \emph{HDRx32 GPU -- LAv2 -- LA only} (radio)
          \item \emph{HDRx2x32 GPU -- LAv2 -- LA only} (radio)
          \item \emph{HDRx64 GPU -- LAv2 -- LA only} (radio)
        \end{itemize}

      \item \emph{Perturbation Only} (submenu): Test configurations that run only the perturbation portion of the pipeline.
        \begin{itemize}
          \item \emph{1x32 GPU -- Perturb only} (radio)
          \item \emph{2x32 GPU -- Perturb only} (radio)
          \item \emph{1x64 GPU -- Perturb only} (radio)
          \item \emph{HDRx32 GPU -- Perturb only} (radio)
          \item \emph{HDRx2x32 GPU -- Perturb only} (radio)
          \item \emph{HDRx64 GPU -- Perturb only} (radio)
        \end{itemize}

      \item \emph{Reference Compression} (submenu): Algorithms using reference-orbit compression as part of the perturbation pipeline.
        \begin{itemize}
          \item \emph{1x32 GPU -- RC LAv2} (radio)
          \item \emph{2x32 GPU -- RC LAv2} (radio)
          \item \emph{1x64 GPU -- RC LAv2} (radio)
          \item \emph{HDRx32 GPU -- RC LAv2} (radio)
          \item \emph{HDRx2x32 GPU -- RC LAv2} (radio)
          \item \emph{HDRx64 GPU -- RC LAv2} (radio)

          \item \emph{Perturbation Only} (submenu): Reference-compressed perturbation-only variants.
            \begin{itemize}
              \item \emph{1x32 GPU -- RC Perturb Only} (radio)
              \item \emph{2x32 GPU -- RC Perturb Only} (radio)
              \item \emph{1x64 GPU -- RC Perturb Only} (radio)
              \item \emph{HDRx32 GPU -- RC Perturb Only} (radio)
              \item \emph{HDRx2x32 GPU -- RC Perturb Only} (radio)
              \item \emph{HDRx64 GPU -- RC Perturb Only} (radio)
            \end{itemize}

          \item \emph{LA Only} (submenu): Reference-compressed LA-only variants.
            \begin{itemize}
              \item \emph{1x32 GPU -- RC LAv2} (radio)
              \item \emph{2x32 GPU -- RC LAv2} (radio)
              \item \emph{1x64 GPU -- RC LAv2} (radio)
              \item \emph{HDRx32 GPU -- RC LAv2} (radio)
              \item \emph{HDRx2x32 GPU -- RC LAv2} (radio)
              \item \emph{HDRx64 GPU -- RC LAv2} (radio)
            \end{itemize}
        \end{itemize}

      \item \emph{LAv2 (Full Pipeline)} (radio entries): Main LAv2 renderers combining LA, perturbation, and reference handling.
        \begin{itemize}
          \item \emph{1x32 GPU -- LAv2} (radio)
          \item \emph{2x32 GPU -- LAv2} (radio)
          \item \emph{1x64 GPU -- LAv2} (radio)
          \item \emph{HDRx32 GPU -- LAv2} (radio)
          \item \emph{HDRx2x32 GPU -- LAv2} (radio)
          \item \emph{HDRx64 GPU -- LAv2} (radio)
        \end{itemize}

      \item \emph{Tests} (submenu): Development and diagnostic test actions.
        \begin{itemize}
          \item \emph{Run Basic Test (saves files in local dir)} (command)
          \item \emph{Run View \#27 test} (command)
        \end{itemize}

    \end{itemize}

  \item \emph{Iterations} (submenu): Adjusts the iteration limit and iteration counter width.
    \begin{itemize}
      \item \emph{Default Iterations} (command): Resets iteration count to defaults.
      \item \emph{+1.5x} (command): Multiplies iteration limit by 1.5.
      \item \emph{+6x} (command): Multiplies iteration limit by 6.
      \item \emph{+24x} (command): Multiplies iteration limit by 24.
      \item \emph{Decrease Iterations} (command): Decreases the current iteration limit.
      \item \emph{32-Bit Iterations} (radio): Uses 32-bit iteration counters (lower memory, smaller range).
      \item \emph{64-Bit Iterations} (radio): Uses 64-bit iteration counters (larger range).
    \end{itemize}

  \item \emph{Perturbation} (radio group submenu): Controls perturbation rendering and related reference-orbit management. FractalShark supports perturbation-based rendering driven by a high-precision reference orbit. Menu options in this area control how the reference orbit is computed and reused. GPU-accelerated reference orbits are intended for extremely deep zooms and may be slower than CPU computation at modest precision levels. They are marked as experimental and should be used accordingly.
    \begin{itemize}
      \item \emph{Clear Perturbation References -- All} (command): Clears all cached perturbation references.
      \item \emph{Clear Perturbation References -- Med} (command): Clears medium-class cached references.
      \item \emph{Clear Perturbation References -- High} (command): Clears high-class cached references.
      \item \emph{Show Perturbation Results} (command): Displays diagnostic/summary results for perturbation state.

      \item \emph{Auto (default)} (radio)
      \item \emph{Single Thread (ST)} (radio)
      \item \emph{Multi Thread (MT)} (radio)
      \item \emph{ST + Periodicity} (radio)
      \item \emph{MT2 + Periodicity} (radio)
      \item \emph{MT2 + Periodicity + Perturb ST} (radio)
      \item \emph{MT2 + Periodicity + Perturb MT v1} (radio)
      \item \emph{MT2 + Periodicity + Perturb MT v2} (radio)
      \item \emph{MT2 + Periodicity + Perturb MT v3} (radio)
      \item \emph{MT2 + Periodicity + Perturb MT v4 (broken)} (radio)
      \item \emph{MT5 + Periodicity} (command): Only enabled when perturbation support is available.

      \item \emph{GPU-Accelerated (see README)} (command): Selects or enables a GPU-accelerated perturbation path. \textit{[TODO: clarify whether this is a mode toggle, a mode selection, or a one-shot action]}

      \item \emph{Clear and Reload Reference Orbits} (command): Clears reference-orbit state and reloads it from persistent storage.
      \item \emph{Save Reference Orbits} (command): Saves reference-orbit state to persistent storage.
    \end{itemize}

  \item \emph{Palette Color Depth} (submenu): Controls palette selection, palette bit depth, and optional palette rotation. Some entries may be enabled only when palette rotation support is available.
    \begin{itemize}
      \item \emph{Basic} (radio): \textit{[TODO: clarify palette type ``Basic'']}
      \item \emph{Default} (radio)
      \item \emph{Patriotic} (radio)
      \item \emph{Summer} (radio)
      \item \emph{Random} (radio)

      \item \emph{Create Random Palette} (command): Generates a new random palette.

      \item \emph{5-bit} (radio)
      \item \emph{6-bit} (radio)
      \item \emph{8-bit} (radio)
      \item \emph{12-bit} (radio)
      \item \emph{16-bit} (radio)
      \item \emph{20-bit} (radio)

      \item \emph{Palette Rotation} (command): \textit{[TODO: clarify whether this is a toggle or a one-shot action; enabled only when rotation is supported]}
    \end{itemize}

  \item \emph{Memory Management} (submenu): Controls memory-limit policy and whether reference orbit data is automatically saved.
    \begin{itemize}
      \item \emph{Enable Auto-Save Orbit (delete when done, default)} (radio): Automatically saves orbit data temporarily and deletes it when no longer needed.
      \item \emph{Enable Auto-Save Orbit (keep files)} (radio): Automatically saves orbit data and retains files on disk.
      \item \emph{Disable Auto-Save Orbit} (radio): Disables automatic orbit saving.

      \item \emph{Remove Memory Limits} (radio): Disables memory limiting behavior.
      \item \emph{Leave max of (1/2*ram, 8GB) free (default)} (radio): Constrains allocation to preserve system memory headroom.
    \end{itemize}

  \item \emph{Show Rendering Details} (command): Displays current rendering status and diagnostics (e.g., position/state details). \textit{[TODO: refine what fields are shown]}

  \item \emph{Save} (submenu): Output and benchmarking commands.
    \begin{itemize}
      \item \emph{Save Location...} (command): Saves the current location/view parameters.
      \item \emph{Save High Res Bitmap} (command): Saves a high-resolution bitmap render. \textit{[TODO: clarify how resolution is selected]}
      \item \emph{Save Iterations as Text} (command): Saves iteration counts in a text format.
      \item \emph{Save Bitmap Image} (command): Saves the current bitmap image.

      \item \emph{Save Reference Orbit as Text} (command): Saves the reference orbit in text form.
      \item \emph{Save Compressed Orbit as Text (simple)} (command)
      \item \emph{Save Compressed Orbit as Text (max)} (command)
      \item \emph{Save Compressed Orbit (Imagina/max)} (command)

      \item \emph{Benchmark (5x, full recalc)} (command): Runs a benchmark performing full recalculation passes.
      \item \emph{Benchmark (5x, intermediate only)} (command): Runs a benchmark emphasizing intermediate stages. \textit{[TODO: clarify what ``intermediate'' means here]}

      \item \emph{Diff Imagina Orbits (choose two)} (command): Compares two Imagina-format saved orbits.
    \end{itemize}

  \item \emph{Load} (submenu): Load view parameters and reference orbits.
    \begin{itemize}
      \item \emph{Load Location...} (command): Loads a previously saved location/view.
      \item \emph{Enter Location} (command): Manually enters a location/view. \textit{[TODO: clarify input format]}
      \item \emph{Load Imagina Orbit (Match)...} (command): Loads an Imagina orbit by matching parameters. \textit{[TODO: clarify match criteria]}
      \item \emph{Load Imagina Orbit (Use Saved)...} (command): Loads a previously saved Imagina orbit directly.
    \end{itemize}

  \item \emph{Exit} (command): Exits the application.
\end{itemize}

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{auto-renderer-artifact.png}
  \caption{Rendering artifacts arising from insufficient precision in the
  HDRx32 GPU renderer. Switching to HDRx64 or a CPU-based renderer resolves these
  artifacts.}
  \label{fig:auto-renderer-artifact}
\end{figure}

\subsection{Recalculation Commands}

Commands such as \emph{Recalculate} or \emph{Recalculate, Reuse Reference} trigger
an immediate re-render of the current view. These commands do not change any
persistent settings; they simply instruct the renderer to restart computation
using the current configuration. Reusing the reference orbit can significantly
reduce recomputation time when only display parameters change.

\subsection{Experimental and Debug Options}

Some menu entries expose experimental features, debugging aids, or partially
implemented functionality. These options may be unstable, incomplete, or
subject to removal. FractalShark is an experimental research project rather than
a polished end-user application. Users should expect occasional crashes,
rendering failures, or unresponsive behavior when exercising less common menu
options.

\subsection{Practical Usage Notes}

\begin{itemize}
  \item There is currently no reliable way to cancel a long-running render once
  it has begun. If necessary, the application must be terminated externally.
  \item Some menu options may be temporarily disabled depending on the current
  render state or selected algorithm.
  \item If the application exits unexpectedly, a crash dump may be generated in
  the executable directory and can be used for debugging or issue reporting.
\end{itemize}

Despite its rough edges, the popup menu provides a unified control surface for
exploring FractalShark’s many rendering paths and numerical experiments, making
it the primary interface for both casual exploration and deep technical
investigation.


\subsection{Keyboard Shortcuts}
\label{subsec:keyboard-shortcuts}

In addition to the popup menu, FractalShark supports a set of single-key
shortcuts for navigation, recalculation, palette exploration, and tuning of
perturbation/linear-approximation parameters.  Most shortcuts operate relative
to the \emph{current mouse position} (converted into client coordinates at the
time the key is pressed), which makes them feel similar to context-menu actions.

Unless otherwise noted, shortcuts are \emph{case-sensitive} in the sense that
uppercase variants are invoked by holding \texttt{Shift}.  Many actions also
explicitly clear cached perturbation results prior to recomputation, trading
additional work for correctness or a ``clean'' run.

\paragraph{Navigation and view control.}
\begin{itemize}
  \item \texttt{z}: Zoom in by a fixed amount, centered at the mouse cursor.
  \item \texttt{Z} (\texttt{Shift+z}): Zoom out by a fixed amount, centered at the mouse cursor.
  \item \texttt{c}: Center the view at the mouse cursor (no forced reference recomputation).
  \item \texttt{C} (\texttt{Shift+c}): Center the view and clear all perturbation results first.
  \item \texttt{b}: Go back to the previous view.
  \item \texttt{r}: Square the current view (aspect normalization).
  \item \texttt{R} (\texttt{Shift+r}): Clear all perturbation results, then square the current view.
  \item Mouse \textbf{left-click and drag}: Zoom into a dragged rectangle.
        Holding \texttt{Shift} while dragging disables aspect-ratio enforcement.
\end{itemize}

\paragraph{Autozoom (experimental / buggy).}
\label{par:autozoom}

\begin{itemize}
  \item \texttt{a}: Autozoom using the default heuristic (currently experimental and may misbehave).
  \item \texttt{A} (\texttt{Shift+a}): Autozoom using the ``Max'' heuristic.
\end{itemize}
Both autozoom variants first center on the mouse position before executing the
autozoom routine.  (The on-screen hotkey help also notes that holding \texttt{Ctrl}
may abort autozoom, but the abort mechanism is not part of this key handler and
is implemented elsewhere.)

\paragraph{Recalculation and benchmarking.}
Several keys force a recalculation; uppercase forms often clear cached results
first.
\begin{itemize}
  \item \texttt{i}: Force recalculation and show the current position (copies details to clipboard).
  \item \texttt{I} (\texttt{Shift+i}): Clear \emph{medium-res} perturbation results, then recalc and show position.
  \item \texttt{o}: Force recalculation and show the current position.
  \item \texttt{O} (\texttt{Shift+o}): Clear \emph{all} perturbation results, then recalc and show position.
  \item \texttt{p}: Force recalculation and show the current position.
  \item \texttt{P} (\texttt{Shift+p}): Clear \emph{LA-only} perturbation results, then recalc and show position.
\end{itemize}
\emph{Note:} The in-application hotkey dialog groups \texttt{i/I}, \texttt{o/O},
\texttt{p/P}, and \texttt{r/R} under ``Recalculating and Benchmarking'' with
slightly different wording.  The authoritative behavior is the code path: these
keys primarily differ by which cached perturbation result class is cleared
before forcing recomputation.

\paragraph{Reference compression tuning.}
These keys adjust compression error exponents and then repaint, typically after
clearing cached perturbation results.
\begin{itemize}
  \item \texttt{e}: Clear all perturbation results, reset compression error exponents to defaults,
        and repaint.
  \item \texttt{q}: Clear all perturbation results; \emph{increase} intermediate compression error
        exponent by a step (more error, less memory).
  \item \texttt{Q} (\texttt{Shift+q}): Clear all perturbation results; \emph{decrease} intermediate compression
        error exponent by a step (less error, more memory).
  \item \texttt{w}: Clear all perturbation results; \emph{increase} low-error compression exponent by a step.
  \item \texttt{W} (\texttt{Shift+w}): Clear all perturbation results; \emph{decrease} low-error compression exponent by a step.
\end{itemize}
\emph{Practical interpretation:} increasing the error exponent generally favors
memory reduction and speed at the expense of fidelity in reconstructed reference
segments; decreasing it pushes toward higher fidelity and higher memory use.

\paragraph{Linear approximation parameter tuning (powers-of-two adjustments).}
These keys adjust linear-approximation thresholds and period-detection thresholds.
After changing parameters, FractalShark clears LA-only perturbation results and
forces a recalc.
\begin{itemize}
  \item \texttt{h}: Increase LA threshold scale exponents (less accurate / faster per-pixel).
  \item \texttt{H} (\texttt{Shift+h}): Decrease LA threshold scale exponents (more accurate / slower per-pixel).
  \item \texttt{j}: Increase LA period-detection threshold exponents.
  \item \texttt{J} (\texttt{Shift+j}): Decrease LA period-detection threshold exponents.
\end{itemize}

\paragraph{Palettes.}
\begin{itemize}
  \item \texttt{d}: Advance to the next palette lookup-table depth and redraw.
  \item \texttt{D} (\texttt{Shift+d}): Create a new random palette, select it, and redraw.
  \item \texttt{t}: Increase auxiliary palette depth (cycling auxiliary palette behavior) and redraw.
  \item \texttt{T} (\texttt{Shift+t}): Decrease auxiliary palette depth and redraw.
\end{itemize}

\paragraph{Iteration count.}
These keys scale the maximum iteration count aggressively for quick exploration:
\begin{itemize}
  \item \texttt{=} : Multiply max iterations by 24.
  \item \texttt{-} : Multiply max iterations by $2/3$.
\end{itemize}

\paragraph{Window manipulation and menu invocation.}
\begin{itemize}
  \item \textbf{Right-click}: Open the popup menu at the cursor position.
  \item \textbf{Shift+F10 / Menu key}: Also opens the popup menu (Windows context-menu invocation).
  \item \textbf{Alt + left-click drag} (windowed mode): Drag the window by treating the client area as a caption.
  \item Mouse wheel: Zoom in/out (scroll up zooms in; scroll down zooms out).
\end{itemize}

\paragraph{Responsiveness note.}
Many shortcuts trigger a recomputation or redraw immediately.  If FractalShark
is executing a long-running render (especially a large GPU kernel), keyboard
input may appear delayed until the render completes.  In practice, the popup
menu and hotkeys are best used as \emph{configuration and relaunch} controls,
not as guaranteed real-time interaction during heavy computation.

\subsection{Commentary}

\begin{itemize}
\item One fun thing you can try is running with LAv2 + ``LA only''. This 
approach actually works pretty well once Linear Approximation kicks in at 
deeper zooms --- it gives you an idea of what the actual image should look like 
but is very fast, since it does no perturbation. The images it produces are not 
precise, and often leave out the fine detail; however, it's fun to play with 
when zooming in on a specific point.

\item The most interesting reference orbit calculation is at \texttt{
AddPerturbationReferencePointMT3}. It includes a ``bad'' calculation which is 
used for the ``scaled'' CUDA kernels. The multithreaded approach handily beats 
the single-threaded implementation on my CPU in all scenarios.

\item There are CPU renderers, but they were mostly to learn/debug more easily, 
and aren't optimized heavily. They're much easier to understand and reason 
about though.
\end{itemize}

\section{Kernel List}

This section simply lists all the CUDA kernels in \FractalShark{}.

\begin{itemize}

\item \textbf{Mandelbrot base kernels}
  \begin{itemize}
    \item \texttt{mandel\_1x\_float}
    \item \texttt{mandel\_1x\_double}
    \item \texttt{mandel\_2x\_float}
    \item \texttt{mandel\_2x\_double}
    \item \texttt{mandel\_4x\_float}
    \item \texttt{mandel\_4x\_double}
    \item \texttt{mandel\_hdr\_float}
  \end{itemize}

\item \textbf{Mandelbrot perturbation kernels}
  \begin{itemize}
    \item \texttt{mandel\_1x\_double\_perturb\_bla}
    \item \texttt{mandel\_1xHDR\_float\_perturb\_bla}
    \item \texttt{mandel\_1xHDR\_float\_perturb\_lav2}
  \end{itemize}

\item \textbf{High-precision reference kernels}
  \begin{itemize}
    \item \texttt{HpSharkReferenceGpuKernel}
    \item \texttt{HpSharkReferenceGpuLoop}
  \end{itemize}

\item \textbf{Utility kernels}
  \begin{itemize}
    \item \texttt{antialiasing\_kernel}
    \item \texttt{max\_reduce}
    \item \texttt{max\_kernel}
  \end{itemize}

\item \textbf{Addition kernels}
  \begin{itemize}
    \item \texttt{AddKernel}
    \item \texttt{AddKernelTestLoop}
  \end{itemize}

\item \textbf{Multiplication (NTT) kernels}
  \begin{itemize}
    \item \texttt{MultiplyKernelNTT}
    \item \texttt{MultiplyKernelNTTTestLoop}
  \end{itemize}

\item \textbf{Disabled kernels}
  \begin{itemize}
    \item \texttt{mandel\_2x\_float\_perturb\_setup}
    \item \texttt{mandel\_2x\_float\_perturb}
  \end{itemize}

\end{itemize}

\subsection{Commentary}

\begin{itemize}

\item All the per-pixel CUDA kernels are in \texttt{gpu\_render.cu}. The two
most interesting are probably \texttt{mandel\_1xHDR\_float\_perturb\_bla} and
\texttt {mandel\_1xHDR\_float\_perturb\_lav2}, which are the ones I've spent the
most time on lately. For better performance at low zoom levels, you could look
at \texttt{mandel\_1x\_float\_perturb}, which leaves out linear approximation
and just does straight perturbation up to $\sim 10^{30}$, which corresponds with
the 32-bit float exponent range.

\item The \texttt{mandel\_1x\_float} is the classic 32-bit float Mandelbrot and 
is screaming fast on a GPU. This one is optimized with fused multiply-add for 
fun even though it's kind of useless because you can barely zoom in before you 
get pixellation.

\end{itemize}

\paragraph{High-Dynamic-Range (HDR) scalar representation.}
In this work, the term \emph{HDR} refers to a reduced \emph{high-dynamic-range}
numeric representation used exclusively for control-flow decisions, not for
authoritative arithmetic. An HDR value preserves a wide exponent range—sufficient
to compare magnitudes spanning many orders of magnitude—while intentionally
discarding most mantissa precision. This design enables inexpensive, robust
magnitude comparisons (e.g., norms, maxima, and inequality tests) without
incurring the cost of full arbitrary-precision operations. HDR values are derived
from the underlying high-precision state via a monotone reduction step and are
used solely for periodicity detection, escape testing, and other termination
criteria. They never participate in state updates and therefore cannot influence
the numerical correctness of the reference orbit itself. The use of the term HDR
here is purely numerical and is unrelated to high-dynamic-range imaging.


\section{The Mandelbrot set and escape-time rendering}
\label{sec:mandelbrot-intro}

The Mandelbrot set \(M\subset\mathbb{C}\) is defined as the set of complex
parameters \(c\) for which the orbit of
\begin{equation}
z_{n+1} = z_n^2 + c,\qquad z_0 = 0
\end{equation}
remains bounded. A common rendering method is \emph{escape-time}: iterate until
the orbit magnitude exceeds a bailout threshold, or until a maximum iteration
count is reached.

A standard bailout uses \(|z_n|^2 \ge 4\), since \(|z|>2\) implies divergence:
\begin{equation}
|z_n|^2 = \Re(z_n)^2 + \Im(z_n)^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}
For each pixel, the stored value is the smallest \(n\) (or a bounded proxy) at
which escape occurs, or the maximum iteration limit if escape never occurs.


\section{Pixel-to-parameter mapping}
\label{sec:mapping}

Each CUDA thread computes a pixel coordinate \((X,Y)\) and maps it to a complex
parameter \(c = x_0 + i y_0\). A typical kernel starts with:
\begin{verbatim}
int X = blockIdx.x * blockDim.x + threadIdx.x;
int Y = blockIdx.y * blockDim.y + threadIdx.y;
if (X >= width || Y >= height) return;
size_t idx = ConvertLocToIndex(X, height - Y - 1, width);
\end{verbatim}

\subsection{Thread-to-pixel mapping}
A 2D CUDA grid of 2D blocks covers the image. Each thread is responsible for one
pixel. The bounds check prevents out-of-range threads from writing.

\subsection{Y-axis convention}
The expression \code{height - Y - 1} flips \(Y\). This is common when the image
buffer uses a top-left origin but the complex-plane mapping assumes a
bottom-left origin (or vice versa).

\subsection{Affine map into the complex plane}
The parameter \(c\) is computed by an affine transform:

\begin{align}
x_0 &= cx + dx \cdot X, \\
y_0 &= cy + dy \cdot Y,
\end{align}

where \code{cx,cy} anchor the plane (e.g., the coordinate at pixel \((0,0)\)),
and \code{dx,dy} are per-pixel increments. Different kernels compute these
expressions in different numeric types; the intent is always the same: map each
pixel to its associated complex parameter \(c\).


\section{Mandelbrot recurrence in real arithmetic}
\label{sec:real-form}

Writing \(z = x + i y\) and \(c = x_0 + i y_0\), the iteration becomes:

\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + x_0, \\
y_{n+1} &= 2 x_n y_n + y_0.
\end{align}

The escape test is:

\begin{equation}
x_n^2 + y_n^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}

Many kernels cache squares:
\[
zrsqr = x^2,\quad zisqr = y^2,
\]
so that \(x^2-y^2\) and \(x^2+y^2\) can be formed cheaply as \code{zrsqr - zisqr}
and \code{zrsqr + zisqr}. This is especially valuable when \(x\) and \(y\) are
represented by multi-component expansion types.


\section{Mandelbrot base kernels}
\label{sec:mandel-base-kernels}

The goal of these kernels in \FractalShark{} is to \emph{render the Mandelbrot set}
by computing an \emph{escape-time} iteration count per pixel over a 2D image
grid efficiently. Each CUDA thread evaluates a single complex parameter \(c\)
corresponding to one pixel, iterates the Mandelbrot recurrence, and stores the
iteration count into an output buffer. Separate (or fused) stages can map
iteration counts to colors, apply palettes, and perform anti-aliasing.

Across these base kernels, the primary variation is the numeric representation
used for the orbit arithmetic: from IEEE-754 \code{float}/\code{double} up
through expansion types (float-float, double-double, quad-float, quad-double)
and HDR-normalized formats. The shared objective remains the same: compute the
escape-time for the Mandelbrot iteration as accurately and efficiently as needed
for a desired zoom depth.

Each kernel variant renders the same Mandelbrot escape-time field; the only
difference is the numeric type used for mapping and orbit iteration. The
following sections describe how each type realizes the same recurrence and
escape test.  This set of kernels does not use linear approximation or
perturbation; they simply evaluate the Mandelbrot iteration directly in the
chosen numeric format.

Throughout, \code{IterType} is the integer type used to store the escape-time
iteration count (e.g., \code{uint32\_t} or \code{uint64\_t}).

\subsection{Kernel: \code{mandel\_1x\_float}}
\label{sec:mandel-1x-float}

\subsubsection{Numeric type}
This variant uses IEEE-754 single precision \code{float}. It provides the
highest throughput but limits usable zoom depth due to rounding error and loss
of significance in \(c\) and the orbit.

\subsubsection{FMA-based orbit update}
The implementation uses fused multiply-add intrinsics:

\begin{verbatim}
ytemp = __fmaf_rd(-y, y, x0);     // x0 - y^2
xtemp = __fmaf_rd(x, x, ytemp);   // x^2 - y^2 + x0
xtemp2 = 2.0f * x;
y = __fmaf_rd(xtemp2, y, y0);     // 2xy + y0
x = xtemp;
\end{verbatim}

These intrinsics correspond exactly to:

\begin{align}
x &\leftarrow x^2 - y^2 + x_0,\\
y &\leftarrow 2xy + y_0.
\end{align}

Using FMA reduces intermediate rounding and can improve performance. The
\code{\_\_fmaf\_rd} variant rounds downward; if IEEE round-to-nearest is desired,
use \code{\_\_fmaf\_rn} (or plain \code{fmaf}).

\subsubsection{Escape test}
The kernel tests \(x^2+y^2 < 4\), which implies recomputing squares each loop in
this simplest variant.  Perhaps this kernel could explicitly cache the squares
for better performance.

\subsection{Kernel: \code{mandel\_1x\_double}}
\label{sec:mandel-1x-double}

\subsubsection{Numeric type}
This variant uses IEEE-754 \code{double} and therefore carries substantially
more mantissa precision than float. The Mandelbrot recurrence is identical, but
performance depends strongly on GPU FP64 throughput.  Consumer GPUs often
have much lower FP64 throughput than FP32, so this kernel may be slower than
\code{mandel\_1x\_float} on such hardware.

\subsubsection{Orbit update and escape test}
The update uses double-precision FMA intrinsics (e.g., \code{\_\_fma\_rd}) or
equivalent arithmetic to compute:
\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]
with the same bailout condition \(|z|^2 \ge 4\).

\subsection{Kernel: \code{mandel\_2x\_float}}
\label{sec:mandel-2x-float}

\subsubsection{Numeric type: float-float expansion}
This variant uses a float-float expansion type \code{dblflt} to represent a
value as:
\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]
where \code{head} (hi) carries the leading magnitude and \code{tail} (lo) is a
correction term. Arithmetic uses compensated routines such as
\code{add\_dblflt}, \code{sub\_dblflt}, \code{mul\_dblflt}, \code{sqr\_dblflt},
and often a specialized \code{mul\_dblflt2x(x,y)} to compute \(2xy\) with good
accuracy.

All double- and quad- float/double implementations are based on work from Andrew
Thall \cite{andrew-thall-dblflt}.  The implementations are modified here to
support both float and double base types.  These changes are unique to
\FractalShark{}.

\subsubsection{Mapping and orbit iteration}

The affine mapping for \(c\) is performed in \code{dblflt}:

\[
x_0 = cx + dx \cdot X,\qquad y_0 = cy + dy \cdot Y,
\]

and the orbit update follows the same real-form recurrence using expansion
operations. Cached squares are typically maintained as \code{dblflt}:

\[
zrsqr = x^2,\quad zisqr = y^2.
\]

\subsubsection{Escape test}
The implementations compares only the leading component (e.g., \code{head}) for
speed:

\[
zrsqr.\mathrm{head} + zisqr.\mathrm{head} < 4.
\]

This approach is fast but can misclassify points extremely near the boundary. A
fully robust bailout can incorporate both components (or a conservative bound).

\subsection{Kernel: \code{mandel\_2x\_double}}
\label{sec:mandel-2x-double}

\subsubsection{Numeric type: double-double expansion}
This variant uses a double-double type \code{dbldbl} representing:

\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]

with both components in double precision, yielding \(\sim\)106 bits of precision
in favorable cases. This approach enables deeper zoom while retaining a
structure similar to the float-float kernel.

\subsubsection{Mapping, orbit update, and escape test}

The kernel computes the same affine mapping and iterates the same recurrence,
but with double-double arithmetic. The escape predicate also uses the leading
component for speed.

\subsection{Kernel: \code{mandel\_4x\_float}}
\label{sec:mandel-4x-float}

\subsubsection{Numeric type: quad-float (4-term expansion)}

This variant uses a four-float expansion type \code{GQF::gqf\_real}:

\[
a \approx a_0 + a_1 + a_2 + a_3,
\]

with decreasing-magnitude components. Pixel coordinates and constants are lifted
into this type (e.g., \code{make\_qf(X,0,0,0)}), then the affine mapping and
orbit update are performed in quad-float arithmetic.  The implementation of this
numeric type is also based on Andrew Thall's work \cite{andrew-thall-dblflt}
with some minor changes specific to \FractalShark{}.

\subsubsection{Orbit update and escape test}
The update implements:

\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]

using quad-float operations (including specialized square and power-of-two
multiply helpers). The escape test can be performed in the full quad-float type:

\[
zrsqr + zisqr \le 4.
\]

\subsection{Kernel: \code{mandel\_4x\_double}}
\label{sec:mandel-4x-double}

\subsubsection{Numeric type: quad-double (4-term expansion)}
This variant uses a four-double expansion type \code{GQD::gqd\_real}:
\[
a \approx a_0 + a_1 + a_2 + a_3,\qquad a_k\in\mathbb{R}_{double}.
\]
It supports very deep zoom rendering with high numerical stability.

\subsubsection{Mapping, orbit update, and escape test}
The affine mapping and orbit update are evaluated in quad-double arithmetic. A
literal bailout constant (e.g., \code{4.0}) is promoted via overloads, so the
escape compare remains a full-precision comparison in the quad-double domain.

\subsection{Kernel: \code{mandel\_hdr\_float}}
\label{sec:mandel-hdr-float}

\subsubsection{Numeric type: HDR-normalized expansion}
This variant uses an HDR wrapper around an expansion type, e.g.:

\[
\code{HDRFloat<CudaDblflt<dblflt>>}.
\]

This implementation has no practical value because precision is limited by the
base expansion type; however, it serves as a testbed for HDR arithmetic in the
Mandelbrot context.  Thus, this kernel is primarily of academic interest.

\subsubsection{Reduction and stable comparisons}

The kernel frequently calls \code{HdrReduce()} on intermediate values. These
reductions are part of the numeric contract: norms and comparisons are assumed
to be applied to reduced/normalized values, enabling specialized comparators
without repeatedly materializing primitive scalars.

Instead of testing \(x^2+y^2 < 4\) in primitive form, the kernel maintains:

\[
zsq\_sum = zrsqr + zisqr
\]

and checks escape via a reduced comparator:

\begin{verbatim}
while (zsq_sum.compareToBothPositiveReduced(Four) < 0)
\end{verbatim}

This directly supports escape-time Mandelbrot rendering in HDR arithmetic while
keeping comparisons meaningful and stable.

\section{Iteration chunking via \code{iteration\_precision}}
\label{sec:chunking}

A few of the base kernels just described, which exclude linear approximation or
perturbation, are templated on an integer \code{iteration\_precision}
(\(1,2,4,8,16\)) and unroll multiple Mandelbrot steps inside the loop:

\begin{itemize}
  \item Each loop iteration performs \code{iteration\_precision} updates.
  \item The counter \code{iter} increases by that amount.
  \item The maximum iteration \code{n\_iterations} is adjusted so \code{iter}
        does not exceed the requested limit.
\end{itemize}

This approach reduces loop overhead. The trade-off is that the escape predicate
is typically checked only once per chunk; therefore the reported escape
iteration can be larger than the true first-escape iteration by up to
\code{iteration\_precision - 1}. For strict escape iteration counts (e.g., for
continuous/smooth coloring based on the first bailout), use a chunk size of 1 or
insert bailout checks within the unrolled body.

This optimization was mainly explored for educational purposes; in practice, the
benefit is small compared to other optimizations such as perturbation and linear
approximation.

\section{Perturbation Rendering of the Mandelbrot Set}
\label{sec:perturbation-concept}

Rendering the Mandelbrot set at extreme zoom levels presents a fundamental
numerical challenge. As the image is magnified, the parameters \(c\) associated
with individual pixels differ by increasingly small amounts, while the number
of iterations required to determine escape behavior typically grows. Accurately
tracking these orbits therefore requires both high numerical precision and a
large number of iterations, making naive per-pixel evaluation prohibitively
expensive.

Perturbation provides a mathematical and algorithmic framework that addresses
this challenge by exploiting the strong coherence between nearby orbits. Rather
than evaluating each pixel independently at full precision, perturbation
separates the computation into a shared high-precision component and many
pixel-local low-precision components. This section introduces the perturbation
approach and explains how it enables efficient deep-zoom Mandelbrot rendering.

\subsection{Baseline iteration and its limitations}

The Mandelbrot set is defined by the iteration
\begin{equation}
z_{n+1} = z_n^2 + c, \qquad z_0 = 0,
\end{equation}
where \(c \in \mathbb{C}\) is the parameter associated with a pixel and the orbit
\(\{z_n\}\) is iterated until either escape is detected or a maximum iteration
count is reached.

At modest zoom levels, this iteration can be evaluated accurately using standard
floating-point arithmetic. At deep zoom levels, however, the differences between
nearby parameters \(c\) may be many orders of magnitude smaller than the values
of \(z_n\) encountered during iteration. In such regimes, rounding error and
loss of significance make fixed-precision arithmetic unreliable. While arbitrary
precision arithmetic can restore accuracy, applying it independently to every
pixel scales poorly.

\subsection{Coherence of nearby orbits}

A key observation underlying perturbation is that nearby parameters generate
orbits that remain close for many iterations. In a typical image tile, all pixel
parameters \(c\) lie within a small neighborhood of a central value. Their
orbits therefore share a common large-scale structure, differing only by small
corrections that grow gradually over time.

Perturbation makes this structure explicit by selecting a single
\emph{reference parameter} and expressing all nearby orbits relative to it. This
transforms the problem from one of many independent high-precision computations
into one of a single high-precision computation plus many low-precision updates.

\subsection{Reference orbit}

Let \(c_\star\) denote a reference parameter, typically chosen as the center of
the image region. Its orbit is computed using sufficiently high precision to
serve as a reliable baseline:
\begin{equation}
z_{n+1}^\star = (z_n^\star)^2 + c_\star, \qquad z_0^\star = 0.
\end{equation}
The sequence \(\{z_n^\star\}\) is stored and reused for all pixels within the
region. Because this orbit underpins the evaluation of many pixels, its
accuracy is critical.

\subsection{Perturbation as a mixed-precision decomposition}
\label{subsec:perturbation-mixed-precision}

For a nearby pixel parameter \(c = c_\star + \Delta c\), the orbit can be written
as a small deviation from the reference orbit:
\begin{equation}
z_n = z_n^\star + \Delta z_n.
\end{equation}
Substituting this expression into the Mandelbrot recurrence yields an exact
update rule for the deviation:
\begin{equation}
\Delta z_{n+1}
= 2 z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturb-exact}
\end{equation}
No approximation has been introduced: as long as the reference orbit is computed
accurately, this formulation preserves the exact Mandelbrot dynamics.

This decomposition has an important numerical consequence. The reference orbit
\(z_n^\star\) typically grows in magnitude and requires arbitrary precision to be
represented accurately at deep zoom. In contrast, the deviation \(\Delta z_n\)
remains small for many iterations and can usually be evolved using much lower
precision arithmetic. Perturbation therefore reorganizes the computation into a
mixed-precision pipeline: expensive high-precision arithmetic is concentrated in
a single shared orbit, while the majority of per-pixel work is performed using
lower precision with significantly higher throughput.

\subsection{Real-valued formulation for implementation}

For practical implementation, particularly on GPUs, it is convenient to express
the perturbation update in real arithmetic. Writing
\begin{align*}
z_n^\star &= x_n^\star + i y_n^\star, \\
\Delta z_n &= \Delta x_n + i \Delta y_n, \\
\Delta c &= \Delta c_x + i \Delta c_y,
\end{align*}
and expanding \cref{eq:perturb-exact} yields
\begin{align}
\Delta x_{n+1}
&= 2(x_n^\star \Delta x_n - y_n^\star \Delta y_n)
   + (\Delta x_n^2 - \Delta y_n^2)
   + \Delta c_x, \\
\Delta y_{n+1}
&= 2(x_n^\star \Delta y_n + y_n^\star \Delta x_n)
   + 2 \Delta x_n \Delta y_n
   + \Delta c_y.
\end{align}
These equations map directly onto scalar arithmetic and can be evaluated
efficiently in parallel for many pixels.

\subsection{Reconstruction and escape testing}

Although perturbation evolves only the deviation \(\Delta z_n\), escape-time
rendering requires testing the magnitude of the full orbit value. At each
iteration, the current iterate is reconstructed as
\begin{equation}
z_n = z_n^\star + \Delta z_n,
\end{equation}
and the standard Mandelbrot bailout condition is applied:
\begin{equation}
|z_n|^2 \ge 4.
\end{equation}
The iteration at which this condition is first satisfied determines the pixel’s
escape time.

\subsection{Numerical stability and rebasing}

Perturbation remains efficient only while the deviation \(\Delta z_n\) remains
small relative to the reference orbit. As the deviation grows, numerical
cancellation and loss of significance may occur. Practical implementations
therefore employ \emph{rebasing},\footnote{I believe Zhuoran discovered this
technique but don't have a definitive reference} in which the current deviation
is folded into the reference state and perturbation continues relative to this
new baseline. This process preserves accuracy while maintaining numerical
stability.

\subsection{Relation to other acceleration techniques}

Perturbation preserves the exact dynamics of the Mandelbrot map and forms the
foundation of many advanced deep-zoom rendering techniques. It can be used by
itself or combined with additional approximations, such as linear or higher-order
methods, to skip multiple iterations at once. In all cases, perturbation provides
the conceptual framework that makes efficient deep-zoom Mandelbrot rendering
possible.

\section{HDR Floating-Point Representation}
\label{sec:hdr-float}

Deep-zoom Mandelbrot rendering places unusual demands on numerical
representation. Orbit values may span an extreme dynamic range over the course
of iteration, while small relative differences between nearby orbits must be
tracked accurately. Standard floating-point formats are poorly matched to this
combination of requirements: fixed-precision mantissas limit relative accuracy
at large magnitudes, while arbitrary-precision formats incur prohibitive
computational cost when used per pixel.

To address this gap, we employ a custom \emph{high dynamic range (HDR)}
floating-point representation that explicitly separates scale and precision.
This representation is designed to preserve relative accuracy across a wide
range of magnitudes while remaining efficient on GPUs.

\subsection{Decoupling scale and precision}

In standard IEEE floating-point arithmetic, each value is represented as a
mantissa multiplied by a power of two, with both components stored implicitly in
a single word. While convenient, this tightly couples scale and precision: as
values grow in magnitude, fewer bits remain available to represent small
relative differences.

The HDR representation instead stores values in the form
\begin{equation}
x = m \cdot 2^{e},
\end{equation}
where:
\begin{itemize}
\item \(m\) is a fixed-precision mantissa stored in standard floating-point or
      double-double format, and
\item \(e\) is an explicitly managed integer exponent.
\end{itemize}

By storing the exponent separately, the mantissa can remain centered near unit
magnitude regardless of the overall scale of the value. This preserves relative
precision even when the absolute value becomes extremely large or small.

\subsection{Normalization and arithmetic}

HDR arithmetic maintains the invariant that the mantissa remains within a fixed
normalized range. After each arithmetic operation, the mantissa is renormalized
and any excess scaling is folded into the exponent. Conceptually, this mirrors
the behavior of floating-point normalization, but with explicit control over the
process.

Arithmetic operations are performed as follows:
\begin{itemize}
\item \textbf{Multiplication} combines mantissas and adds exponents.
\item \textbf{Addition and subtraction} align exponents explicitly before
      operating on mantissas.
\item \textbf{Renormalization} ensures that mantissas remain well-scaled and
      numerically stable.
\end{itemize}

Because mantissas are stored using fixed-precision types, these operations map
naturally to GPU hardware while avoiding the overhead of general-purpose
arbitrary-precision arithmetic.

\subsection{HDR complex numbers}

Mandelbrot iteration operates on complex numbers. In the HDR formulation, each
complex value
\[
z = x + i y
\]
is represented by storing separate HDR values for the real and imaginary
components. Both components share the same conceptual structure but may have
independent exponents.

This representation allows the magnitude of \(z\) to grow or shrink over many
orders of magnitude while maintaining accurate relative phase and magnitude
information, which is critical for both escape testing and perturbation-based
methods.

\subsection{Role of HDR in perturbation and reference orbits}

The HDR representation plays a complementary role to perturbation. In the
perturbation framework, the reference orbit typically grows rapidly in magnitude
and therefore demands high dynamic range, while perturbation deltas remain small
and can often be represented using lower precision.

HDR arithmetic is used primarily for:
\begin{itemize}
\item computing reference orbits at deep zoom levels,
\item evaluating linear and nonlinear terms involving large reference values,
\item maintaining numerical stability across long iteration sequences.
\end{itemize}

By decoupling scale from precision, HDR arithmetic allows the reference orbit to
be computed efficiently without sacrificing accuracy, while still enabling the
mixed-precision structure exploited by perturbation rendering.

\subsection{Advantages over conventional representations}

Compared to standard floating-point formats, HDR arithmetic offers:
\begin{itemize}
\item substantially increased dynamic range,
\item stable relative precision independent of magnitude,
\item predictable and controllable numerical behavior.
\end{itemize}

Compared to arbitrary-precision libraries, it avoids dynamic memory allocation,
irregular control flow, and large per-operation overheads, making it well suited
for massively parallel execution on GPUs.

\subsection{Summary}

The HDR floating-point representation provides a numerically robust and
computationally efficient foundation for deep-zoom Mandelbrot rendering. By
explicitly separating scale and precision, it bridges the gap between standard
floating-point arithmetic and full arbitrary-precision methods. In combination
with perturbation, it enables accurate, high-performance rendering across
extreme zoom levels while remaining compatible with GPU execution models.


\section{Reference Orbit Calculation}
\label{sec:ref-orbit-calc}

This section describes how \texttt{RefOrbitCalc} constructs (and optionally 
reuses) a high-precision \emph{reference orbit} for perturbation rendering of 
the quadratic map

\begin{equation}
  z_{n+1} = z_n^2 + c,\qquad z,c\in\mathbb{C},
\end{equation}

with an implementation that supports single-threaded CPU, multi-threaded CPU,
and GPU backends, plus several storage/compression modes that trade memory
footprint against recomputation.

\subsection{High-level pipeline and dispatch}
\label{subsec:ref-orbit-pipeline}

A reference orbit is stored in a \texttt{PerturbationResults<IterType,T,PExtras>}
instance, where:

\begin{itemize}
  \item \texttt{IterType} is the iteration index type (\texttt{uint32\_t} or \texttt{uint64\_t}).
  \item \texttt{T} is the low-precision numeric type used for downstream perturbation math (e.g.\ \texttt{float},
        \texttt{double}, \texttt{HDRFloat<...>}).
  \item \texttt{PExtras} selects the storage format for per-iteration orbit data:
        uncompressed (\texttt{Disable}), a lightweight compressor (\texttt{SimpleCompression}),
        or additional diagnostic bookkeeping (e.g.\ \texttt{Bad}).
\end{itemize}

Orbit construction is initiated through \texttt{AddPerturbationReferencePoint()}, which:

\begin{enumerate}
  \item Picks an initial guess \((c_x,c_y)\) (center of the current view if unset).
  \item Chooses an algorithm (\texttt{ST}, \texttt{MT}, reuse-based hybrids, or \texttt{GPU}) based on
        \texttt{m\_PerturbationAlg} and zoom factor heuristics.
  \item Allocates a new \texttt{PerturbationResults} slot, initializes metadata and bounds, and runs the chosen
        orbit kernel until escape, periodicity detection, or the maximum iteration count is reached.
\end{enumerate}

To control memory pressure, \texttt{OptimizeMemory()} monitors process commit 
usage and opportunistically drops cached orbits that are not of the currently 
demanded variant type when the working set exceeds a configurable threshold.

\subsection{Single-threaded authoritative orbit}
\label{subsec:ref-orbit-st}

The single-threaded path (\texttt{AddPerturbationReferencePointST}) computes 
the authoritative orbit directly using MPIR/GMP \texttt{mpf\_t} for the 
recurrence, while simultaneously emitting a low-precision shadow copy \((\hat{x}
_n,\hat{y}_n)\in T^2\) for downstream work (compression, bailout checks, 
periodicity tests).

\paragraph{State and initialization.}

Given a selected reference parameter \(c = c_x + i c_y\), the implementation:

\begin{itemize}
  \item Initializes \texttt{mpf\_t} temporaries for \(x,y,x^2\), and scratch products.
  \item Sets the initial iterate \((x_0,y_0) = (c_x,c_y)\) (this code uses the common convention \(z_0=c\)).
  \item Computes low-precision cast values \(\hat{c}_x,\hat{c}_y \in T\) either via \texttt{mpf\_get\_d} for
        native float/double, or via a mantissa/exponent extraction for extended formats.
\end{itemize}

\paragraph{Recurrence.}
Writing \(z_n = x_n + i y_n\), one iteration evaluates
\begin{align}
  x_{n+1} &= x_n^2 - y_n^2 + c_x, \\
  y_{n+1} &= 2 x_n y_n + c_y.
\end{align}
The implementation uses:
\begin{itemize}
  \item \texttt{mpf\_mul} and \texttt{mpf\_sub}/\texttt{mpf\_add} for the high-precision update.
  \item A low-precision snapshot \((\hat{x}_n,\hat{y}_n)\) acquired once per iteration for storage/compression,
        periodicity heuristics, and bailout checks.
\end{itemize}

\paragraph{Bailout.}

The bailout threshold is evaluated in low precision using

\begin{equation}
  \|\hat{z}_n + \hat{c}\|^2 = (\hat{x}_n + \hat{c}_x)^2 + (\hat{y}_n + \hat{c}_y)^2 > 256,
\end{equation}

which matches the code's use of \texttt{TwoFiftySix} and avoids a high-precision
norm each step.  Note that the bailout here is different from the one used in
the lower-precision kernels discussed previously; this is acceptable because the
reference orbit is used only for perturbation, not direct rendering.  The
difference is merely arbitrary.

\subsection{Periodicity tracking via \texorpdfstring{$\partial z/\partial c$}{dz/dc}}
\label{subsec:ref-orbit-periodicity}

Several modes enable periodicity detection. The implementation tracks the complex derivative

\(\frac{\partial z_n}{\partial c}\) in low precision:
\begin{equation}
  d_{n+1} = 2 z_n d_n + 1,\qquad d_0 = 1,
\end{equation}

with \(d_n = d_{x,n} + i d_{y,n}\). Expanding into real and imaginary parts yields:

\begin{align}
  d_{x,n+1} &= 2(x_n d_{x,n} - y_n d_{y,n}) + 1, \\
  d_{y,n+1} &= 2(x_n d_{y,n} + y_n d_{x,n}).
\end{align}

The code applies a radius-based heuristic: let

\begin{equation}
  n_2 = \max(|\hat{x}_n|,|\hat{y}_n|),\qquad
  r_0 = \max(|d_{x,n}|,|d_{y,n}|),
\end{equation}

and define a detection threshold

\begin{equation}
  n_3 = 2\,R_{\max}\,r_0,
\end{equation}

where \(R_{\max}\) is the maximum perturbation radius stored in \texttt{results}
. If \(n_2 < n_3\), the orbit is marked as \emph{maybe periodic} and the
reference loop terminates early (unless periodicity detection is disabled).
Otherwise, \((d_{x,n},d_{y,n})\) is advanced using the update above.

\subsection{Expanded versus factored evaluation of the reference orbit}

The high-precision reference orbit evaluates the quadratic map

\[
z_{n+1} = z_n^2 + c,
\qquad
z_n = x_n + i y_n,
\qquad
c = c_x + i c_y,
\]

with full arbitrary-precision arithmetic.

Writing the iteration in real and imaginary components gives the
\emph{expanded form}:

\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + c_x, \label{eq:ref-expanded-real} \\
y_{n+1} &= 2 x_n y_n + c_y. \label{eq:ref-expanded-imag}
\end{align}

This form follows directly from the algebraic definition of the polynomial.
Each term is evaluated explicitly, requiring two full-precision squares and
one full-precision multiplication per iteration.

The same quadratic map can be evaluated in a mathematically equivalent
\emph{factored form}. In particular, the real component may be written as

\begin{align}
x_{n+1}
&= (x_n - y_n)(x_n + y_n) + c_x, \label{eq:ref-factored-real}
\end{align}
while the imaginary component remains
\begin{align}
y_{n+1} &= 2 x_n y_n + c_y. \label{eq:ref-factored-imag}
\end{align}

In exact arithmetic, \cref{eq:ref-factored-real,eq:ref-factored-imag} are
identical to \cref{eq:ref-expanded-real,eq:ref-expanded-imag}. The difference
lies solely in how the products are formed.

From an implementation perspective, the expanded form evaluates
$x_n^2$ and $y_n^2$ independently and makes the subtraction
$x_n^2 - y_n^2$ explicit. This closely mirrors the mathematical definition
of the Mandelbrot polynomial and exercises the full squaring and
multiplication paths of the high-precision arithmetic.

The factored form reduces the number of full-precision squares by replacing
$x_n^2 - y_n^2$ with a single multiplication of the shared intermediates
$(x_n \pm y_n)$. This can reduce computational cost when multiplication and
squaring have similar expense, but it introduces stronger coupling between
terms and alters the way cancellation and rounding effects manifest in
finite-precision arithmetic.

In all cases, \FractalShark{} uses the expanded form despite the advantages offered
by the factored form. This choice simplifies reasoning about numerical behavior,
ensures that all arithmetic paths are exercised, and maintains consistency with
the perturbation update form used elsewhere in the codebase.  For better
performance it likely makes sense to implement both forms and compare their behavior
empirically.

\section{Compression and Reference Orbit Reuse Modes}
\label{subsec:ref-orbit-compression-reuse}

Two orthogonal storage decisions are made while iterating:

\begin{enumerate}
  \item \textbf{Orbit storage} for perturbation use (\texttt{PExtras}):
  \begin{itemize}
    \item \texttt{Disable}: store every \((\hat{x}_n,\hat{y}_n)\) uncompressed.
    \item \texttt{SimpleCompression}: store a compressed subset of iterations using an error exponent
          determined by \texttt{Fractal::CompressionError}.
    \item \texttt{Bad}: store orbit values plus underflow/diagnostic flags.
  \end{itemize}
  \item \textbf{Reuse storage} for intermediate-precision regeneration (\texttt{ReuseMode}):
  \begin{itemize}
    \item \texttt{SaveForReuse1/2}: store uncompressed \texttt{mpf\_t} reuse entries.
    \item \texttt{SaveForReuse3}: store an intermediate-compressed reuse stream.
    \item \texttt{SaveForReuse4}: store a maximally-compressed intermediate reuse stream.
  \end{itemize}
\end{enumerate}

The next two sections describe these mechanisms in more detail.

\subsection{Reuse-based orbit regeneration}
\label{subsec:ref-orbit-reuse}

We consider iteration of the quadratic map
\[
f_c(z) = z^2 + c,\qquad z,c \in \mathbb{C},
\]
and distinguish between an \emph{authoritative} reference orbit computed in very high precision and a hierarchy of
\emph{intermediate precision} reference orbits constructed via perturbation from that authoritative orbit.
The central motivation for this construction is performance at extreme zoom depths: while the authoritative orbit
is expensive to compute, it need only be regenerated infrequently as the view parameter drifts.
Intermediate precision orbits, by contrast, are substantially cheaper to evaluate and can be reused across many
incremental zoom steps before their accuracy envelope is exceeded.

\subsubsection{Authoritative Reference Orbit}

Let
\[
c_0 \in \mathbb{C}
\]
denote the authoritative reference parameter.
The corresponding authoritative reference orbit is defined by
\begin{equation}
z^{(0)}_0 = 0, \qquad
z^{(0)}_{n+1} = \bigl(z^{(0)}_n\bigr)^2 + c_0,
\label{eq:authoritative_orbit}
\end{equation}
and is computed using sufficiently high precision that it is treated as exact for all practical purposes.
This orbit constitutes the single source of truth for all subsequent perturbative constructions.

At very deep zooms, recomputing~\eqref{eq:authoritative_orbit} at every navigation step would dominate runtime,
as the required precision grows with zoom depth.
The reuse-based framework therefore seeks to minimize how often this authoritative orbit must be recalculated,
while still maintaining numerical correctness.

\subsubsection{Perturbation Formulation}

For any nearby parameter
\[
c = c_0 + \Delta c,
\]
the corresponding orbit may be written as
\[
z_n(c) = z^{(0)}_n + \Delta z_n,
\]
where $\Delta z_n$ represents the perturbation relative to the authoritative orbit.
Substituting into the defining recurrence yields the exact perturbation equation
\begin{align}
\Delta z_0 &= 0, \\
\Delta z_{n+1}
&= 2 z^{(0)}_n \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturbation_exact}
\end{align}
Equation~\eqref{eq:perturbation_exact} is the fundamental relation used both to construct intermediate reference orbits
and to evaluate per-pixel perturbations during rendering.

\subsubsection{Intermediate Precision Reference Orbits}

An intermediate precision reference orbit is defined at a parameter
\[
c_1 = c_0 + \Delta c_1,
\]
where $\Delta c_1$ is chosen such that the perturbation $\Delta z^{(1)}_n$ remains bounded within a fixed target accuracy
(e.g.\ absolute error $\lesssim 10^{-100}$) when represented in a chosen intermediate precision arithmetic.

The intermediate reference orbit is defined by
\[
z^{(1)}_n \equiv z^{(0)}_n + \Delta z^{(1)}_n,
\]
where $\Delta z^{(1)}_n$ is obtained by iterating
\begin{equation}
\Delta z^{(1)}_{n+1}
= 2 z^{(0)}_n \Delta z^{(1)}_n
  + \bigl(\Delta z^{(1)}_n\bigr)^2
  + \Delta c_1
\label{eq:intermediate_perturbation}
\end{equation}
in the intermediate precision format.

Crucially, the cost of evaluating~\eqref{eq:intermediate_perturbation} is dramatically lower than that of recomputing
the authoritative orbit~\eqref{eq:authoritative_orbit}.
As a result, once an intermediate reference orbit has been established, it may be reused across many subsequent zoom
or pan operations, as long as the view parameter remains within its validity radius.
Only when accumulated drift causes the perturbation to exceed the fixed accuracy envelope does a new authoritative
orbit need to be computed.

In the single-thread execution path (beginning at
\texttt{AddPerturbationReferencePointST}), this construction is performed sequentially using the cached authoritative
orbit samples $\{z^{(0)}_n\}$ as coefficients.
No approximation beyond finite-precision rounding is introduced; the intermediate orbit is mathematically equivalent
to directly iterating $f_{c_1}$, subject only to the chosen precision bound.

\subsubsection{Per-Pixel Perturbation from an Intermediate Orbit}

For an individual pixel parameter
\[
c_{\text{px}} = c_1 + \delta c,
\]
with $|\delta c| \ll |\Delta c_1|$, the final orbit is expressed as
\[
z_n(c_{\text{px}}) = z^{(1)}_n + \delta z_n,
\]
where $\delta z_n$ satisfies
\begin{equation}
\delta z_{n+1}
= 2 z^{(1)}_n \delta z_n
  + (\delta z_n)^2
  + \delta c.
\label{eq:pixel_perturbation}
\end{equation}
Because $|\delta c|$ is small, $\delta z_n$ remains well within the same fixed precision envelope used for the
intermediate reference orbit.

From a performance perspective, this two-level perturbation hierarchy is
beneficial when interacting with the Mandelbrot. The expensive high-precision
authoritative orbit is amortized over many intermediate orbits, and each
intermediate orbit in turn supports an entire image worth of per-pixel
perturbations. As zoom depth increases, most navigation steps therefore reuse
existing intermediate data, with authoritative recomputation occurring only
sporadically.

The two-level approach nevertheless does have an important downside: instead of
maintaing only a low-precision copy of the reference orbit, the system must also
store the authoritative orbit samples $\{z^{(0)}_n\}$ to support intermediate
reconstruction.  This increases memory usage and data transfer requirements,
potentially impacting performance.

\subsubsection{SaveForReuse Modes}

Both \texttt{SaveForReuse1} and \texttt{SaveForReuse2} correspond to the mathematical framework described above.
They differ only in execution strategy.

\paragraph{SaveForReuse1.}
In \texttt{SaveForReuse1}, the authoritative orbit samples $\{z^{(0)}_n\}$ are stored after initial computation and reused
whenever intermediate reference orbits are constructed.
This avoids recomputation of~\eqref{eq:authoritative_orbit} and ensures that all perturbative steps are driven by the same
authoritative data.

\paragraph{SaveForReuse2.}
\texttt{SaveForReuse2} is a multithreaded optimization of the same procedure.
It performs the identical perturbation recurrences
\eqref{eq:intermediate_perturbation} and \eqref{eq:pixel_perturbation}, using the same authoritative orbit samples and
producing the same intermediate and per-pixel results.
The distinction lies solely in how data is staged, reused, and synchronized across threads in the multithreaded path.

\paragraph{Equivalence.}
From a mathematical standpoint,
\[
\texttt{SaveForReuse1} \;\equiv\; \texttt{SaveForReuse2}.
\]
Both modes define the same authoritative orbit, the same intermediate reference
orbits, and the same per-pixel perturbation orbits.
Any differences are strictly implementation-level optimizations and do not affect numerical results.
\texttt{SaveForReuse3} and \texttt{SaveForReuse4} are addressed in a subsequent section.

\subsection{Reference Compression}
\label{sec:ref-compression-zhuoran}

The idea in this section was developed by Zhuoran
\cite{Zhuoran2023ReferenceCompression} and \FractalShark{} implements it.

\subsubsection{Motivation}
For deep zoom escape-time fractals, pixel evaluation has become extremely fast
(e.g., via perturbation and GPU acceleration), while \emph{reference-orbit}
construction often remains comparatively expensive and can dominate total render
time.  Additionally, when the reference orbit is large, it can consume
significant amounts of memory.  The goal of \emph{reference compression} is to
store this potentially-large reference orbit in a compact form that can be
transmitted or cached, then \emph{reconstructed} efficiently with a guaranteed
bounded reconstruction error. The key idea is to store only a sparse set of
\emph{waypoints} and fill the gaps by recomputing intermediate iterations in
reduced precision.  This section describes the approach.

\subsubsection{High-precision reference and low-precision surrogate}
Let the (authoritative) high-precision reference orbit be
\(
z_n^{\mathrm{HP}} \in \mathbb{C}
\)
for \(n=0,\dots,N\).
During compression and decompression we also maintain a low-precision surrogate
\(
\hat z_n \in \mathbb{C}
\)
computed by iterating the same recurrence in a cheaper numeric type.

Because the dynamics are initially stable to rounding and only later amplify
numerical differences, \(\hat z_n\) typically tracks \(z_n^{\mathrm{HP}}\) for
many iterations before diverging.  This observation enables a streaming scheme:
store occasional exact anchors (waypoints) and recover all omitted states by
recomputing them in low precision between anchors.

\subsubsection{Waypoint selection criterion}
A \emph{waypoint} stores the iteration index and a corrective payload.
In the simplest mode, the payload is the full authoritative value
\(z_{n_k}^{\mathrm{HP}}\) at iteration \(n_k\).
Waypoints are chosen by simulating \(\hat z_n\) in low precision while comparing
against \(z_n^{\mathrm{HP}}\).  If the relative error exceeds a threshold, the
iteration must be retained.

A convenient test is

\begin{equation}
  \frac{\lVert z_n^{\mathrm{HP}} - \hat z_n \rVert}{\lVert z_n^{\mathrm{HP}} \rVert}
  \;>\; \varepsilon,
  \label{eq:relerr-test}
\end{equation}

where \(\lVert \cdot \rVert\) may be \(\ell_\infty\), \(\ell_2\), or another cheap norm
(consistent between compression and decompression), and \(\varepsilon\) is a
user-controlled tolerance.  Whenever the test fails, we emit a waypoint at \(n\)
and \emph{reset} the low-precision state to the authoritative value:

\begin{equation}
  \text{if waypoint at } n:\qquad \hat z_n \leftarrow z_n^{\mathrm{HP}}.
\end{equation}

Between waypoints, the compressor stores nothing, relying on \(\hat z\)-iteration
to reconstruct the missing values later.

\paragraph{Compressed representation.}
The compressed reference is thus a sparse, ordered list
\(
\mathcal{W} = \{(n_k, w_k)\}_{k=0}^{K-1}
\)
with strictly increasing indices \(0 \le n_0 < n_1 < \cdots < n_{K-1} \le N\),
where the payload \(w_k\) depends on the mode (Section~\ref{sec:refcomp-pert}).

\subsubsection{Decompression by replay}

Decompression replays the same low-precision recurrence and applies waypoints as
hard resets:

\begin{equation}
  \hat z_{n+1} = \hat z_n^2 + c \quad\text{(low precision)},\qquad
  \text{and if } n = n_k:\ \hat z_n \leftarrow w_k.
\end{equation}

If the decompressor uses the same low-precision arithmetic, the same recurrence,
and encounters the same waypoints at the same indices, then the reconstructed
orbit matches what the compressor would have produced by replay; consequently,
the reconstruction error is bounded by the same tolerance logic used to place
waypoints.

\paragraph{On-the-fly use during rendering.}
Because decompression is just a forward scan with occasional resets, it can be
performed \emph{streaming} (no random access required), and can be interleaved
with perturbation-based pixel evaluation by iterating \(\hat z_n\) alongside the
main render loop.

\subsubsection{Perturbation-assisted compression}

\label{sec:refcomp-pert}
Waypointing only the absolute states \(z_n\) is useful but may not achieve high
compression at extreme depths, where the orbit's sensitivity forces frequent
anchors.  Compression improves substantially by switching to a perturbation form
once the orbit becomes suitable.

\paragraph{Self-referenced perturbation.}
Choose a \emph{base} (reference) iteration \(r\) and represent subsequent states as

\begin{equation}
  z_n = z_r + \Delta z_n, \qquad n \ge r,
\end{equation}

where \(z_r\) is treated as a reference and \(\Delta z_n\) is (ideally) small.
In standard perturbation rendering, \(\Delta z\) is evolved using derivatives
around a known reference orbit.  For \emph{reference compression}, the only
available reference is the reference itself, so the scheme starts in the
absolute waypoint mode (previous sections) and transitions to perturbation only
when \(\lVert \Delta z \rVert\) is sufficiently small that perturbation yields
meaningful extra precision in the reduced type.

\paragraph{Switch criterion.}
Let \(\tau\) be a user parameter.  When \(\lVert \Delta z_n \rVert < \tau\), the
compressor may begin emitting waypoints that store \(\Delta z_n\) rather than
\(z_n\).  After the switch, a waypoint payload becomes

\begin{equation}
  w_k = \Delta z_{n_k} \quad\text{(instead of } z_{n_k}\text{)}.
\end{equation}

\paragraph{Rebasing synchronization.}
Perturbation implementations commonly \emph{rebase} to prevent \(\Delta z\) from
growing too large: at some iteration \(b\), the reference is updated so that
\(\Delta z_b \leftarrow 0\) and the base index becomes \(r \leftarrow b\).
During decompression, if rebases occur at different iterations than during
compression, the decompressor will interpret stored \(\Delta z\) against the
wrong base \(z_r\), producing catastrophic corruption.

A robust compressed format therefore must encode enough information to keep
rebases in lockstep.  One approach is to include a single \emph{rebase flag} per
waypoint indicating whether a rebase occurs at that iteration; if a rebase
occurs at an iteration where no waypoint would otherwise be required, the
compressor must still record the iteration (e.g., by forcing a waypoint or
storing an index) so the decompressor can rebase at the same \(b\).

\subsubsection{Error propagation and correction}

Reconstruction introduces small errors.  Without perturbation, such errors are
often tolerable because each pixel's perturbation evaluation can be stable to
small reference drift.  However, once perturbation is used \emph{inside} the
reference reconstruction, errors can feed forward recursively: an error early in
the reconstructed reference becomes part of the base used later, and thus
contaminates subsequent \(\Delta z\) evolution.

\paragraph{Local linear model.}
Let \(e_n = z_n^{\mathrm{HP}} - \tilde z_n\) be the reconstruction error, where
\(\tilde z_n\) denotes the reconstructed (decompressed) value.  Linearizing the
map \(f(z)=z^2+c\) around the reconstructed state gives
\begin{equation}
  e_{n+1}
  \;=\; f(z_n^{\mathrm{HP}}) - f(\tilde z_n)
  \;\approx\; f'(\tilde z_n)\, e_n
  \;=\; 2\tilde z_n\, e_n.
  \label{eq:error-forward}
\end{equation}
Thus, when \(|2\tilde z_n|\) is large, errors amplify rapidly.

\paragraph{Inverse (Newton-style) correction sweep.}
At a waypoint iteration \(m\), the compressor knows the authoritative value
(either \(z_m\) or the perturbation payload consistent with the base).  The
decompressor can compute the current error at \(m\) directly:

\begin{equation}
  e_m = z_m^{\mathrm{HP}} - \tilde z_m
  \quad\text{(or the analogous difference in the perturbation representation).}
\end{equation}

Assuming the dominant source of \(e_m\) is accumulated from the previous segment,
one can approximate the error in earlier iterations by inverting the local model
\eqref{eq:error-forward}:

\begin{equation}
  e_n \;\approx\; \frac{e_{n+1}}{2\tilde z_n},
  \qquad \text{for } n=m-1,m-2,\dots.
  \label{eq:error-backward}
\end{equation}

A single backward sweep applying

\(
\tilde z_n \leftarrow \tilde z_n + e_n
\)

is equivalent to performing one Newton--Raphson-style correction pass over the
segment to enforce consistency with the waypoint constraint at \(m\).  In
practice this substantially reduces recursive drift and allows larger gaps
between waypoints in perturbation mode.

\paragraph{Compressor constraint.}
To keep the scheme well-defined, the compressor must ensure that iterations not
yet eligible for correction are never used as a perturbation base.  Operationally:
when the current perturbation base index approaches the end of the corrected
region, the compressor inserts a new waypoint so the decompressor has an
opportunity to correct that segment before it would be used as a base for future
\(\Delta z\) evolution.

\subsubsection{Algorithm sketch}

The following high-level procedure summarizes the method.

\begin{enumerate}
  \item \textbf{Compression pass:}
    \begin{enumerate}
      \item Initialize low-precision \(\hat z_0 \leftarrow 0\); iterate forward.
      \item At each \(n\), evaluate relative error \eqref{eq:relerr-test}.
      \item If error too large, emit waypoint \((n, z_n^{\mathrm{HP}})\) (or \((n,\Delta z_n)\) after perturbation switch),
            set \(\hat z_n \leftarrow z_n^{\mathrm{HP}}\) (or reset perturbation state), and continue.
      \item If in perturbation mode, record/synchronize any rebase events (bit flag or forced waypoint).
      \item Insert additional waypoints as needed to guarantee correction opportunities before uncorrected
            states would become perturbation bases.
    \end{enumerate}

  \item \textbf{Decompression pass:}
    \begin{enumerate}
      \item Replay the same low-precision recurrence, applying waypoints as resets.
      \item In perturbation mode, rebase exactly when signaled by the compressed stream.
      \item When a waypoint is reached, compute the current error and apply a backward correction sweep
            using \eqref{eq:error-backward} over the segment since the previous waypoint.
    \end{enumerate}
\end{enumerate}

\subsubsection{Practical tradeoffs}

Reference compression exposes a clear accuracy--size--time trade:
\begin{itemize}
  \item Smaller tolerance \(\varepsilon\) \(\Rightarrow\) more waypoints \(\Rightarrow\) larger compressed size but tighter fidelity.
  \item Larger gaps reduce storage but increase decompression work and may require more frequent correction points,
        especially in perturbation mode.
  \item Perturbation-mode waypoints (\(\Delta z\)) can greatly improve compression when \(\Delta z\) remains small for long
        stretches, but require careful rebase synchronization and correction to avoid catastrophic desynchronization.
\end{itemize}

Overall, the method turns an expensive, dense high-precision reference into a
compact, streamable representation whose reconstruction cost is dominated by
cheap low-precision iteration plus sparse waypoint application, enabling reuse
of deep references across sessions, machines, or render nodes.

\subsubsection{Supporting both random and sequential access patterns}
\label{sec:refcomp-runtime-access}

Reference-orbit consumers exhibit two fundamentally different access patterns.
On the one hand, perturbation-based evaluation and on-the-fly decompression
naturally traverse the orbit sequentially, requesting
\(z_0, z_1, z_2, \dots\) in order.  On the other hand, higher-level algorithms
such as linear (or affine) approximation, reuse heuristics, and error analysis
require \emph{direct access} to arbitrary iteration indices in order to evaluate
local behavior around a chosen anchor.  A decompression strategy optimized only
for sequential replay performs poorly for such random probes, while a strategy
optimized only for random access would impose unnecessary overhead on the
dominant sequential case.

To accommodate both use cases efficiently, the runtime decompressor implements a
hybrid access scheme: binary-search-based anchoring for random access, combined
with cached linear scans for sequential access.

\paragraph{Random access via waypoint search.}
The compressed reference orbit is stored as a strictly increasing sequence of
waypoints
\(
\{(i_k, z_{i_k})\}_{k=0}^{K-1}
\),
where \(i_k\) denotes the uncompressed iteration index and \(z_{i_k}\) the saved
state.  Given a request for an arbitrary iteration \(n\), we locate the nearest
preceding waypoint
\begin{equation}
  k^\star = \max\{k \mid i_k \le n\},
\end{equation}
using a binary search over the waypoint indices in
\(\mathcal{O}(\log K)\) time.  Reconstruction then proceeds by replaying the
low-precision recurrence forward from that anchor for
\(\Delta = n - i_{k^\star}\) steps:
\begin{equation}
  z_{t+1} \leftarrow z_t^2 + c,
  \qquad t = i_{k^\star}, \dots, n-1,
\end{equation}
with the same reduction/normalization operations used during compression.  This
provides efficient, bounded-cost random access suitable for linear approximation
and analysis routines that must probe isolated iteration indices.

\paragraph{Sequential access via cached linear scans.}
In contrast, perturbation pipelines typically consume the reference orbit in
monotone order.  Repeating a binary search for every \(n+1\) would be wasteful,
so the decompressor maintains a small cache of recently reconstructed states,
including both their uncompressed and compressed indices.  When a request
targets the immediate successor of a cached iteration, the decompressor advances
the state in-place:
\begin{itemize}
  \item If the next uncompressed index lies strictly between two waypoints, a
        single recurrence step is performed.
  \item If the next index coincides with a waypoint, the cached state is replaced
        by the stored waypoint value and the compressed index is advanced.
\end{itemize}
A dual-entry cache tolerates minor access jitter by retaining both the most
recent and the previously-recent state; exact cache hits are returned
immediately, and near-sequential requests avoid both binary search and replay
from older anchors.

\paragraph{Combined complexity and practical behavior.}
This hybrid design yields the desirable properties of both approaches:
\begin{align}
  \text{sequential access} &: \quad \mathcal{O}(1)\ \text{amortized per iteration}, \\
  \text{random access} &: \quad \mathcal{O}(\log K + \Delta),
\end{align}
where \(\Delta\) is the distance from the nearest preceding waypoint.  In typical
compressed references, \(\Delta\) is small relative to the full orbit length,
while sequential traversal dominates overall access volume.  The result is a
single runtime decompressor that efficiently supports both perturbation-driven
sequential replay and analysis-driven random probing, without duplicating
reference representations or specializing the data structure to a single access
pattern.

\paragraph{Example usage.}
See \ref{fig:view27-1-quadrillion} for an example of a deep-zoom render
that requires reference compression to complete.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{view27-1-quadrillion.png}
  \caption{View \#27, a period ~28-billion point rendered at 1 quadrillion
  iterations per pixel, requires reference compression to render on most
  hardware because of its memory requirements.  This render takes ~6h with an
  RTX 4090 and AMD 5950X and 128GB RAM with the multi-threaded reference orbit
  backend and is effectively the hardest point \FractalShark{} has likely ever
  rendered.}
  \label{fig:view27-1-quadrillion}
\end{figure}


\section{Multi-threaded Reference Orbit Acceleration}
\label{subsec:ref-orbit-mt}

The \texttt{MT3} reference-orbit path accelerates high-cost MPIR arithmetic by
decomposing each iteration across multiple CPU threads.  The design targets
independent, high-latency big-integer operations that naturally overlap in the
perturbation recurrence, while minimizing synchronization overhead.  Two
recurring implementation patterns characterize this approach.

\paragraph{Asynchronous squaring.}
During authoritative orbit evaluation, two worker threads compute the squared
terms \(x_n^2\) and \(y_n^2\) concurrently.  In parallel, the main thread
evaluates the cross term in
\[
  y_{n+1} = 2 x_n y_n + c_y,
\]
performs low-precision periodicity checks, and manages operand reuse and
serialization.  Once the squared terms are returned, the real component update
is completed as
\begin{equation}
  x_{n+1} = x_n^2 - y_n^2 + c_x.
\end{equation}
This decomposition exposes substantial instruction-level and thread-level
parallelism while preserving the exact arithmetic semantics required for
reference orbit computation.

\paragraph{Lock-free handoff with prefetch.}
Inter-thread communication is implemented using a minimal lock-free mailbox
(\texttt{ThreadPtrs<T>}) consisting of atomic \texttt{In} and \texttt{Out}
pointers.  The protocol proceeds as follows:
\begin{enumerate}
  \item The producer publishes a work pointer by storing it into \texttt{In}.
  \item The worker spins until it atomically claims the pointer, prefetches the
        referenced MPIR operands, executes the required arithmetic, and then
        publishes the same pointer via \texttt{Out}.
  \item The producer spins until it retrieves the completed pointer from
        \texttt{Out}, after which the computed results are consumed.
\end{enumerate}
To reduce cache-miss latency when operating on large MPIR limb arrays, worker
threads explicitly prefetch both MPIR metadata and limb data using a fixed
64-byte stride.  This strategy is particularly beneficial at very high
precision, where memory latency can otherwise dominate execution time.

\subsection{Precision-dependent single-thread versus multi-thread trade-offs}
\label{subsec:ref-orbit-mt-tradeoff}

The relative effectiveness of single-threaded and multi-threaded reference-orbit
evaluation depends strongly on the active precision, which in practice is
closely correlated with zoom level and MPIR limb count.  Consequently, the
implementation supports both execution modes and selects between them
automatically when \texttt{PerturbationAlg::Auto} is enabled.

At lower precision (small limb counts), overall performance is often dominated
by fixed overheads such as synchronization, atomic mailbox operations, thread
spinning, and cache traffic.  In this regime, a single-threaded MPIR execution
path benefits from tight instruction locality, reduced fencing, and predictable
cache reuse, and may outperform a parallelized approach despite executing all
arithmetic serially.

As precision increases, the cost structure shifts toward throughput-dominated
big-integer arithmetic.  Large limb counts cause MPIR squaring and multiplication
to dominate total runtime, amortizing synchronization overhead and making
concurrent execution increasingly effective.  Overlapping independent
operations, such as the simultaneous computation of \(x_n^2\) and \(y_n^2\),
yields substantial gains, while explicit limb prefetching further mitigates
memory latency.  Beyond a hardware- and workload-dependent threshold,
multi-threaded reference-orbit computation consistently outperforms the
single-threaded path, with speedups increasing as precision grows.

To exploit this behavior without requiring manual tuning, the \texttt{Auto}
selection mode chooses the perturbation algorithm based on the current zoom
factor as a proxy for required precision.  At moderate zoom levels, it favors
single-threaded periodicity detection to avoid unnecessary parallel overhead.
At higher zoom levels, it transitions to the \texttt{MTPeriodicity3} path, where
parallel MPIR arithmetic becomes advantageous.  For extreme zoom, a hybrid
multi-threaded perturbation strategy (e.g.,
\texttt{MTPeriodicity3PerturbMTHighMTMed3}) is selected to maintain throughput at
very high precision, acknowledging that some configurations remain
experimental.

Overall, single-threaded and multi-threaded reference-orbit evaluation are
treated as complementary strategies rather than a strict hierarchy: fewer limbs
favor single-thread execution, while increasing precision increasingly favors
multi-threaded decomposition, with \texttt{Auto} providing an adaptive runtime
selection mechanism.


\section{GPU reference orbit backend}
\label{subsec:ref-orbit-gpu}

In many numerical algorithms, correctness and stability depend on arithmetic
precision far exceeding that of standard IEEE~754 floating-point formats. The
Mandelbrot reference orbits is a canonical example: small rounding errors
introduced early in the iteration can grow exponentially, eventually corrupting
orbit classification, perturbation terms, or bailout logic. While
arbitrary-precision libraries like MPIR can provide the required accuracy, their
performance is often insufficient when reference orbits must be computed
repeatedly or at very high precision. This creates a fundamental tension between
numerical fidelity and throughput.

To resolve this tension, high-precision arithmetic operations---in particular
multiplication, addition, and subtraction of large significands---must be
implemented with both mathematical rigor and architectural efficiency. Addition
and subtraction are dominated by carry propagation across hundreds or thousands
of limbs, while multiplication scales quadratically unless asymptotically faster
algorithms are employed. For precisions relevant to deep zoom reference orbits,
naive limb-by-limb multiplication quickly becomes the dominant cost,
overwhelming all other parts of the computation.

\begin{figure}[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{gpu-reference-orbit.png}
  \caption{View \#30 rendered at zoom factor \(10^{114,514}\) using the GPU
  reference orbit backend.  Requires ~73s to render on RTX 5090.}
  \label{fig:gpu-reference-orbit}
\end{figure}

\subsection{Why Fast Multiplication Matters for Reference Orbits}

Generating many fractal reference orbits typically involves iterating a
recurrence relation (e.g.\ $z_{n+1} = f(z_n)$) at very high precision, often for
thousands of iterations per reference point. Each iteration requires multiple
high-precision multiplications and additions, and the total cost grows linearly
with iteration count and superlinearly with precision. Even modest speedups in
the core arithmetic therefore compound dramatically over the full computation.

By using an NTT-based multiplication strategy, large significand products can be
computed in $O(n \log n)$ time instead of $O(n^2)$, shifting the performance
profile of high-precision arithmetic into a regime where GPUs can be effectively
utilized. When combined with carefully engineered carry propagation and
normalization steps, this enables high-precision multiply/add/subtract
operations that are fast enough to make reference orbit computation practical at
scales that would otherwise be prohibitive.

\subsection{System Architecture and Execution Context}
\label{subsec:ref-orbit-architecture}

The GPU reference orbit backend is a stateful, precision-specialized execution
engine that computes authoritative Mandelbrot reference orbits entirely on the
GPU. The backend advances the recurrence
\[
z_{n+1} \leftarrow z_n^2 + c
\]
at fixed high precision by executing multiple iterations per kernel invocation,
while the CPU orchestrates invocation, result collection, and termination.

\paragraph{Division of responsibility.}
The design follows a strict separation of concerns. The CPU is responsible for
selecting a supported precision, initializing GPU resources, invoking the kernel
in bounded iteration batches, and accumulating emitted iteration records into
\texttt{PerturbationResults}. The GPU owns all numerical state of the orbit,
executes the high-precision arithmetic pipeline, detects escape and periodicity,
and reports authoritative outcomes to the host.

\paragraph{Persistent backend state.}
Reference orbit computation is centered around a persistent \emph{combo} object
(\texttt{HpSharkReferenceResults<SharkFloatParams>}) that encapsulates all state
required to advance the orbit at a fixed precision. The combo contains:
\begin{itemize}
  \item high-precision orbit-related values (e.g.\ radius threshold and derivative terms),
  \item persistent arithmetic pipeline state for multiplication
        (\texttt{Multiply}, implementing NTT-based convolution) and addition
        (\texttt{Add}, implementing parallel-prefix carry propagation),
  \item a fixed-size output mailbox (\texttt{OutputIters} and
        \texttt{OutputIterCount}) with \texttt{MaxOutputIters}=1024,
  \item termination status (\texttt{PeriodicityStatus}),
  \item and host-only launch and resource plumbing, including a device pointer to
        the GPU-resident state, temporary device buffers, an associated CUDA
        stream, and cached kernel argument arrays.
\end{itemize}
The combo is initialized once, reused across multiple kernel invocations, and
destroyed only after the reference orbit computation completes.

\paragraph{Batch-oriented kernel invocation.}
Each call to \texttt{InvokeHpSharkReferenceKernel} launches a GPU kernel that
advances the Mandelbrot recurrence by up to a specified number of iterations
(\texttt{itersToRun}). Within a single invocation, the kernel performs multiple
full iterations internally, combining high-precision squaring and addition on
the GPU. As iterations progress, the kernel emits up to
\texttt{MaxOutputIters} iteration records into the combo’s output mailbox and
updates \texttt{OutputIterCount} accordingly.

This batching strategy bounds kernel runtime, enables incremental streaming of
orbit data back to the host, and allows long orbits to be computed without
reinitializing GPU state.

\paragraph{Termination and authority.}
After each invocation, the host inspects the combo’s \texttt{PeriodicityStatus}
and stops early if the GPU reports escape, detected periodicity, or an error
condition, or if the total requested iteration budget has been reached. In this
model, the GPU backend is authoritative: it owns the orbit state, performs all
high-precision arithmetic exactly with respect to the fixed significand
precision, and determines orbit outcomes without CPU-side recomputation or
correction.

\paragraph{Precision specialization.}
The entire lifecycle of a reference orbit computation is specialized by
\texttt{HpSharkFloatParams}. Precision is selected once at startup via
\texttt{DispatchByPrecision} and remains fixed for the lifetime of the combo
object. This ensures that kernel code, shared-memory usage, and arithmetic
pipelines are fully specialized for the chosen precision, at the cost of
supporting a finite, enumerated set of precisions.

\medskip

This architectural structure defines the execution context for the remainder of
this section. Subsequent subsections describe the internal arithmetic pipelines
used within each kernel invocation, including NTT-based multiplication,
high-precision addition, carry propagation, and normalization, and explain how
these components are organized to maintain correctness and performance within
the bounded-chunk execution model.

\subsection{High-Precision Floating-Point Addition on the GPU}
\label{sec:hp-add}

While high-precision multiplication dominates the asymptotic cost of many
arbitrary-precision algorithms, addition and subtraction remain critical in
practice. In iterative computations such as reference orbit generation, each
iteration performs multiple additions and subtractions on large significands,
often regardless of whether multiplication is required. At precisions of
hundreds or thousands of limbs, even these ostensibly simple operations become
nontrivial due to long-range carry and borrow propagation.

A naive limbwise implementation resolves carries sequentially: a carry-out from
limb $i$ must be fully determined before limb $i+1$ can be finalized. This
introduces strict serial dependence and $O(N)$ latency, making naive carry
handling fundamentally incompatible with massively parallel GPU architectures.
Without reformulation, carry propagation quickly becomes a synchronization
bottleneck that erodes the benefits of fast high-precision multiplication.

\subsubsection{Numerical Representation}

We represent a high-precision floating-point number as
\begin{equation}
x = s_x \cdot 2^{e_x} \cdot \sum_{i=0}^{N-1} d_i \, 2^{-w i},
\end{equation}
where $s_x \in \{-1,+1\}$ is the sign, $e_x \in \mathbb{Z}$ is a shared exponent,
$d_i \in \{0,\dots,2^w-1\}$ are fixed-width limbs, $w$ is the limb bit width
(typically $32$ or $64$), and $N$ is the limb count. Arithmetic is performed on
the limb array, while the exponent is tracked separately. The mantissa is thus
treated as a fixed-point integer scaled by an implicit radix $2^{-w}$.

\subsubsection{Problem Statement and Exponent Alignment}

Given two high-precision floating-point numbers
\[
x = s_x \cdot 2^{e_x} \cdot M_x, \qquad
y = s_y \cdot 2^{e_y} \cdot M_y,
\]
the objective is to compute
\[
z = x + y
\]
exactly, or under a well-defined rounding mode, using GPU-parallel execution.

Assuming without loss of generality that $e_x \ge e_y$, we define
\[
\Delta = e_x - e_y
\]
and align the mantissas by shifting
\begin{equation}
M_y' = M_y \cdot 2^{-\Delta}.
\end{equation}
If $\Delta \ge Nw$, the contribution of $y$ is below the representable precision
and the operation degenerates to $z \approx x$. Otherwise, both mantissas are
expressed at the shared exponent $e = e_x$.

\subsubsection{Signed Limbwise Accumulation}

After alignment, addition and subtraction are unified by introducing signed limb
arrays
\[
a_i = s_x \cdot d_i^{(x)}, \qquad
b_i = s_y \cdot d_i^{(y')}.
\]
The raw limbwise sum is then
\begin{equation}
t_i = a_i + b_i,
\end{equation}
which may lie outside the canonical range $[0,2^w)$. This computation is
embarrassingly parallel and maps directly to GPU threads, with each thread
computing one or more limb sums independently.

\subsubsection{Carry Structure and Prefix Reformulation}

Each intermediate sum $t_i$ can be decomposed as
\begin{equation}
t_i = r_i + c_i \cdot 2^w,
\end{equation}
where
\[
r_i = t_i \bmod 2^w, \qquad
c_i = \left\lfloor \frac{t_i}{2^w} \right\rfloor.
\]
The carry $c_i$ contributes to limb $i+1$, inducing a dependency chain that is
sequential in the worst case.

To parallelize this process, carry propagation is reformulated as a prefix
problem. Each limb defines a carry transfer operator
\begin{equation}
T_i(x) = x + c_i,
\end{equation}
where $x$ is the incoming carry from lower-order limbs. The total carry entering
limb $i$ is given by the prefix composition
\begin{equation}
C_i = (T_{i-1} \circ T_{i-2} \circ \cdots \circ T_0)(0).
\end{equation}
Because the operator composition
\[
(T_a \circ T_b)(x) = x + c_b + c_a
\]
is associative, carry propagation reduces to a parallel prefix sum over the
carry values $\{c_i\}$, generalizing classical carry-lookahead techniques to
arbitrary-precision arithmetic.

\subsubsection{Single-Pass Parallel Prefix on the GPU}

On GPUs, conventional multi-pass prefix scans incur excessive global
synchronization or kernel-launch overhead. Instead, we employ a single-pass
parallel prefix formulation with decoupled look-back
\cite{merrill2016single}. Each thread block computes its local carry prefix
independently, while a lightweight look-back mechanism resolves inter-block
dependencies only when required.

This approach allows blocks to make forward progress without global barriers,
stalling only when an upstream carry must be observed. Long carry chains, while
possible, are rare in practice and are handled efficiently when they occur. The
resulting carry propagation exhibits near-linear throughput with logarithmic
critical path length, making it well-suited to large limb counts.

\subsubsection{Finalization and Normalization}

The corrected result limbs are obtained as
\begin{equation}
d_i^{(z)} = (r_i + C_i) \bmod 2^w.
\end{equation}
A carry beyond the most significant limb increments the exponent,
\[
d_N^{(z)} \neq 0 \;\Rightarrow\; e \leftarrow e + 1,
\]
while leading-zero cancellation triggers renormalization and a corresponding
decrement of the exponent.

\subsubsection{Correctness and Complexity}

The algorithm is mathematically equivalent to exact integer addition of aligned
mantissas,
\[
z = 2^e \cdot (M_x + M_y'),
\]
and the prefix formulation preserves the strict associativity of integer
addition. All operations are integer-valued and deterministic, yielding
bitwise-identical results across executions.

For $N$ limbs, the algorithm performs $O(N)$ work with $O(\log N)$ span and
$O(N)$ global memory traffic, making it asymptotically optimal for high-precision
addition on massively parallel architectures.

\paragraph{Comparison with MPIR/GMP Serial Carry Propagation}

Conventional high-precision libraries such as MPIR and GMP implement addition
using strictly serial carry propagation. After exponent alignment, limbs are
processed sequentially from least to most significant, with each carry-out
immediately applied to the next limb. This approach is optimal for scalar CPUs:
it minimizes memory traffic, exploits tight data locality, and benefits from
branch prediction and instruction-level parallelism. For moderate precisions,
the constant factors are small and the simplicity of the algorithm dominates.

However, the serial carry dependency enforces an $O(N)$ critical path that
fundamentally limits parallelism. Even when vector instructions are used to
accelerate limbwise addition, the carry chain itself remains sequential and
cannot be overlapped across independent execution units. As a result, MPIR/GMP
addition scales primarily in throughput with clock frequency rather than in
latency with available parallel resources.

In contrast, the GPU-oriented formulation presented here trades a modest
increase in algorithmic complexity for a dramatic reduction in critical path
length. By expressing carry propagation as an associative prefix operation and
resolving dependencies using a single-pass, decoupled look-back scheme, the
effective span is reduced from $O(N)$ to $O(\log N)$. This shift aligns the
algorithm with the strengths of massively parallel architectures, allowing
thousands of threads to participate in carry resolution while preserving exact
arithmetic semantics.

Importantly, the GPU approach does not outperform MPIR/GMP for small limb
counts, where launch overhead and synchronization dominate. Its advantage
emerges at large precisions, where serial carry latency becomes comparable to
or exceeds the cost of multiplication. In this regime, parallel carry
propagation is essential for sustaining end-to-end performance in
high-precision iterative workloads.

\paragraph{Role of Merrill--Garland in Parallel Carry Propagation}

The approach adopted here is closely informed by the work of
\cite{merrill2016single} on single-pass parallel prefix scans with decoupled
look-back. Their contribution is not a numerical
algorithm per se, but a general execution strategy for evaluating long,
associative prefix dependencies efficiently on GPUs.

In the context of high-precision addition, carry propagation is naturally
expressed as a prefix composition over per-limb transfer functions. However,
naively mapping this formulation to a GPU using classical parallel scans would
require multiple global synchronization phases or kernel launches, introducing
significant overhead and limiting scalability. The Merrill--Garland framework
addresses this by separating the computation into two logically distinct
components: (1) local prefix evaluation within a thread block, and (2)
resolution of inter-block dependencies through a lightweight, dynamically
resolved look-back mechanism.

Crucially, the decoupled look-back design allows blocks to proceed independently
until a true dependency on an upstream block is encountered. This property
aligns well with the statistical structure of carry propagation in large
arbitrary-precision additions, where long carry chains are relatively rare and
most limbs can be finalized without waiting on distant predecessors. The
algorithm therefore achieves high average throughput while retaining correctness
in worst-case carry scenarios.

It is important to emphasize that Merrill--Garland does not prescribe how carry
information should be represented, nor does it assume a particular numerical
domain. In this work, their scan framework is specialized to the algebra of
carry transfer functions arising from signed limbwise addition. The numerical
semantics---exact integer arithmetic with deterministic behavior---remain
entirely independent of the scan mechanism. In this sense, Merrill--Garland
provides the parallel control structure, while the arithmetic formulation
determines the correctness and stability of the result.

By combining a mathematically exact carry formulation with a single-pass prefix
execution model, the resulting algorithm bridges a gap between classical
high-precision arithmetic and modern GPU execution. This synthesis enables
carry-dominated operations, traditionally viewed as inherently serial, to scale
efficiently on massively parallel hardware.

\paragraph{Contrast with Blelloch-Style Parallel Prefix Scans}

Classical parallel prefix algorithms, most notably the Blelloch scan, provide a
well-established framework for evaluating associative operators with
$O(\log N)$ depth and $O(N)$ work. Blelloch-style scans proceed in two distinct
phases: an up-sweep (reduce) phase that computes partial aggregates over a
balanced tree, followed by a down-sweep phase that distributes prefix values to
all elements. This structure is well suited to SIMD and shared-memory parallel
models and forms the basis of many CPU and GPU prefix implementations.

However, on GPUs, Blelloch scans impose rigid global synchronization points
between phases. When applied to large limb arrays, this typically requires
either multiple kernel launches or explicit grid-wide synchronization barriers.
As a result, the execution is dominated by synchronization latency rather than
arithmetic throughput, particularly when the scan is embedded inside a larger
iterative algorithm such as high-precision reference orbit computation.

In contrast, the single-pass parallel prefix algorithm
of\cite{merrill2016single} eliminates the global phase separation inherent in
Blelloch scans. Instead of enforcing a strict up-sweep/down-sweep structure,
each thread block computes its local prefix independently and resolves
inter-block dependencies dynamically using a decoupled look-back mechanism. This
allows blocks to make forward progress asynchronously and avoids global barriers
in the common case.

For high-precision carry propagation, this distinction is critical. Carry chains
often terminate locally, and only a small fraction of limbs depend on distant
predecessors. Blelloch-style scans nevertheless force all blocks to participate
in every global synchronization phase, regardless of whether a true dependency
exists. Merrill--Garland, by contrast, pays synchronization cost only when a
carry must cross block boundaries, yielding substantially better utilization of
GPU resources.

From a numerical perspective, both approaches compute the same associative
prefix over carry transfer operators and are therefore equally correct. The
difference lies entirely in execution strategy. Blelloch scans provide a
deterministic, phase-structured evaluation with predictable synchronization,
while Merrill--Garland offers a latency-tolerant, demand-driven execution model
that is better aligned with the irregular dependency structure of carry
propagation in large arbitrary-precision additions.

In this work, the Merrill--Garland formulation is preferred not because it
changes the arithmetic, but because it minimizes synchronization on massively
parallel hardware, allowing carry-dominated operations to scale to precisions
where serial or multi-pass approaches become prohibitive.


\subsubsection{Discussion}

By expressing carry propagation as a parallel prefix problem and employing a
single-pass, decoupled look-back strategy, high-precision floating-point
addition becomes compatible with GPU execution. The formulation cleanly
separates numerical correctness from execution strategy and composes naturally
with NTT-based multiplication and fused arithmetic pipelines. As a result,
fully high-precision arithmetic becomes practical for demanding workloads such
as GPU-accelerated reference orbit computation.


\subsection{Number Theoretic Transform and Multiply}

High-precision floating-point multiplication ultimately reduces to multiplying
two large integers (the significands), followed by normalization and exponent
adjustment. A standard way to accelerate large-integer multiplication is to
compute the convolution of digit-limbs using a fast transform. Over the reals,
one would use an FFT; over modular arithmetic, the analogous tool is the
\emph{Number Theoretic Transform} (NTT).

\subsection{From Large-Integer Multiplication to Convolution}

Let the two nonnegative integers to be multiplied be represented in base $B$ as

\[
A = \sum_{i=0}^{n-1} a_i B^i,\qquad
C = \sum_{i=0}^{n-1} c_i B^i,
\]

with $0 \le a_i, c_i < B$. Their product is

\[
A\cdot C = \sum_{k=0}^{2n-2} \left(\sum_{i=0}^{k} a_i c_{k-i}\right) B^k.
\]

The coefficients

\[
s_k = \sum_{i=0}^{k} a_i c_{k-i}
\]

form the \emph{discrete convolution} of the sequences $(a_i)$ and $(c_i)$. If we
can compute $(s_k)$ quickly, then we can recover the product (with subsequent
carry propagation in base $B$).

\subsection{NTT as an FFT Over a Finite Field}

The NTT computes a discrete Fourier transform, but with all operations performed
modulo a prime $p$ rather than over $\mathbb{C}$. Choose a prime modulus $p$ and
an $N$-th primitive root of unity $\omega \in \mathbb{Z}_p$ such that

\[
\omega^N \equiv 1 \pmod p,\qquad
\omega^k \not\equiv 1 \pmod p \text{ for } 0<k<N.
\]

Then the forward NTT of a length-$N$ sequence $x = (x_0,\dots,x_{N-1})$ is

\[
X_k = \sum_{j=0}^{N-1} x_j \,\omega^{jk} \pmod p,\qquad k=0,\dots,N-1,
\]

and the inverse NTT is

\[
x_j = N^{-1} \sum_{k=0}^{N-1} X_k \,\omega^{-jk} \pmod p,\qquad j=0,\dots,N-1,
\]

where $N^{-1}$ is the multiplicative inverse of $N$ modulo $p$.

Just as with the complex FFT, the key property is that the transform diagonalizes convolution:

\[
\mathrm{NTT}(x \star y) \;=\; \mathrm{NTT}(x)\odot \mathrm{NTT}(y),
\]

where $\odot$ denotes pointwise multiplication and $\star$ denotes cyclic
convolution modulo $x^N-1$. To compute the \emph{linear} convolution needed for
multiplication, we pad both inputs with zeros to length $N \ge 2n$ so that the
cyclic convolution coincides with the linear convolution.

\subsection{Why a Special Prime Helps: \texttt{MagicPrime}}

A practical NTT requires:

\begin{enumerate}

\item $N$ to be highly composite (typically a power of two) so that
Cooley--Tukey style butterflies apply efficiently;

\item a modulus $p$ such that $N \mid (p-1)$, guaranteeing the existence of an
$N$-th primitive root of unity in $\mathbb{Z}_p$;

\item fast modular multiplication on the target hardware (here, GPUs with
efficient 64-bit integer operations).

\end{enumerate}

The prime used here, denoted \texttt{MagicPrime}, is chosen specifically to
satisfy these constraints. The critical mathematical implication of using a
prime modulus is that $\mathbb{Z}_p$ is a field, so every nonzero element has a
multiplicative inverse and the NTT is well-defined and invertible when $\omega$
exists.

The most important structural requirement is:

\[
N \mid (p-1).
\]

When $N$ is a power of two, say $N = 2^m$, this becomes

\[
2^m \mid (p-1).
\]

Thus, the larger the power of two dividing $(p-1)$, the larger the supported
transform sizes.

\subsection{The ``Goldilocks'' Form and Power-of-Two Roots}

A particularly convenient choice on 64-bit hardware is a prime of the form

\[
p = 2^{64} - 2^{32} + 1,
\]

often called a ``Goldilocks'' prime. (In the codebase this role is played by
\texttt{MagicPrime}.) Two consequences make this form attractive:

\paragraph{(1) Large power-of-two factor in $p-1$.}

We have

\[
p - 1 = 2^{64} - 2^{32} = 2^{32}\left(2^{32}-1\right),
\]

so $2^{32}$ divides $(p-1)$. Therefore, for any $N = 2^m$ with $m \le 32$, there
exists an $N$-th root of unity in $\mathbb{Z}_p$. This enables power-of-two NTTs
up to length $2^{32}$ in principle (practically limited by memory and
implementation constraints), which is ample for large-limb convolutions.

\paragraph{(2) Efficient modular arithmetic with 64-bit operations.}

While modular multiplication in $\mathbb{Z}_p$ conceptually involves products up
to $p^2$, this modulus is engineered so reductions can be implemented
efficiently using 64-bit and 128-bit intermediates and/or Montgomery reduction.
The prime is close to $2^{64}$, which aligns well with native unsigned integer
ranges, and its special structure admits reduction strategies that avoid slow
division.

\subsection{NTT-Based Multiplication at a High Level}

Given limb sequences $(a_i)$ and $(c_i)$:

\begin{enumerate}

  \item Choose $N$ as a power of two with $N \ge 2n$, and choose a primitive
  $N$-th root $\omega \in \mathbb{Z}_p$.

  \item Zero-pad both sequences to length $N$ (interpreting limbs as elements of
  $\mathbb{Z}_p$).

  \item Compute $A_k = \mathrm{NTT}(a)_k$ and $C_k = \mathrm{NTT}(c)_k$.

  \item Compute pointwise products $P_k = A_k \cdot C_k \pmod p$.

  \item Compute $p_j = \mathrm{NTT}^{-1}(P)_j$ to obtain the convolution
  coefficients modulo $p$.

\end{enumerate}

Finally, because the convolution coefficients are computed modulo $p$, we must
ensure they represent the \emph{true integer} convolution values, not values
wrapped modulo $p$. This is achieved by choosing the limb base $B$ and transform
length $N$ so that each exact coefficient $s_k$ satisfies

\[
0 \le s_k < p,
\]

(or more generally, can be reconstructed from one or more moduli). With a single
sufficiently large prime (as is typical with a 64-bit Goldilocks prime and
appropriately sized limbs), the coefficients can be recovered directly and then
normalized via carry propagation in base $B$.

\medskip

This section establishes the mathematical foundation: \texttt{MagicPrime} is
selected so that large power-of-two NTTs exist (because $2^m \mid p-1$) and so
that modular arithmetic is efficient on 64-bit GPU hardware. Subsequent sections
will describe how this is specialized for high-precision floating-point
significands, including limb packing, Montgomery-domain multiplication, twiddle
scheduling, and normalization/carry handling.

\subsection{NTT in \FractalShark{}}

Fundamentally, the NTT implementation in \FractalShark{} is somewhat naive.  It
doesn't make much effort at memory layout or optimized coalescing, which is
likely costing it in performance.  This section describes how the NTT-based
multiplication is realized in practice, following the structure of
\FractalShark{}'s CUDA implementation. The overall goal is to compute the exact
convolution of two large significand arrays using modular arithmetic modulo
\texttt{MagicPrime}, and then to recover a canonical high-precision result via
normalization and carry propagation.

\subsubsection{Plan Construction and Parameter Selection}

Before launching any GPU kernels, an NTT ``plan'' is constructed. The plan
determines:

\begin{itemize}

  \item the coefficient bit-width $b$ used to pack the original 32-bit limbs
  into NTT coefficients,

  \item the packed coefficient length $L$,

  \item the transform size $N$, chosen as a power of two with $N \ge 2L$.

\end{itemize}

Correctness requires that no convolution coefficient overflow the prime modulus.
If the coefficients are bounded by $2^b$ and the transform length is $N$, then a
conservative bound on the largest convolution term is

\[
\max_k s_k \;\le\; N \cdot 2^{2b}.
\]

The plan builder enforces

\[
2b + \log_2 N + \delta \;\le\; 64,
\]

for a small safety margin $\delta$, ensuring that all exact convolution values
lie strictly below \texttt{MagicPrime}. This guarantees that the modular
convolution coincides with the true integer convolution.

\subsubsection{Roots of Unity and Montgomery Domain}

The CUDA setup phase precomputes all roots of unity required for the radix--2
NTT. For each stage $s$, a primitive $2^s$-th root $\omega_s$ and its inverse
are generated, along with:

\begin{itemize}

  \item powers of a twist root $\psi^i$ and $\psi^{-i}$,

  \item the modular inverse $N^{-1}$.

\end{itemize}

All constants are stored in Montgomery representation. As a result, every
modular multiplication inside the NTT butterflies is implemented as a Montgomery
multiply, avoiding explicit modular reduction by division.

\subsubsection{Packing, Twisting, and Forward NTT}

The input significands are initially stored as arrays of 32-bit limbs. These are
packed into base-$2^b$ coefficients

\[
A(x) = \sum_{i=0}^{L-1} a_i x^i,\qquad
C(x) = \sum_{i=0}^{L-1} c_i x^i,
\]

with $0 \le a_i, c_i < 2^b$. During packing, each coefficient is:

\begin{enumerate}

  \item mapped into $\mathbb{Z}_p$,

  \item multiplied by the twist factor $\psi^i$,

  \item converted into Montgomery form.

\end{enumerate}

After packing, an in-place radix--2 forward NTT is performed. A bit-reversal
permutation places coefficients into the correct order, followed by iterative
butterfly stages:

\[
(u, v) \;\mapsto\; (u + \omega v,\; u - \omega v),
\]

with all operations performed modulo \texttt{MagicPrime}. A grid-stride loop is
used so that threads collectively cover all $N$ coefficients, independent of the
exact grid dimensions.

\subsubsection{Multiway Pointwise Multiplication}

In the transform domain, convolution reduces to pointwise multiplication. To
amortize the cost of the NTT, the implementation computes three related
convolutions simultaneously using a standard three-multiply decomposition.
Conceptually, if

\[
X = X_r + iX_i,\qquad Y = Y_r + iY_i,
\]

then the products

\[
X_rY_r,\quad X_iY_i,\quad (X_r+X_i)(Y_r+Y_i)
\]

are sufficient to reconstruct both real and imaginary components. Accordingly,
three frequency-domain products are computed per transform index:

\[
\widehat{XX}_k,\quad \widehat{YY}_k,\quad \widehat{XY}_k,
\]

each via a Montgomery modular multiplication modulo \texttt{MagicPrime}.

\subsubsection{Inverse NTT and Untwisting}

Following pointwise multiplication, inverse radix--2 NTTs are applied using the
inverse roots of unity. The inverse transform yields coefficients still in
Montgomery form and still containing the twist factor. These are corrected by:

\begin{enumerate}
\item multiplying by $\psi^{-i}$,
\item multiplying by $N^{-1}$,
\item converting out of Montgomery representation.
\end{enumerate}

After this step, the data represents the exact integer convolution coefficients in base $2^b$.

\subsubsection{Unpacking and Normalization}

The final stage converts the convolution coefficients back into the original
limb representation. Base-$2^b$ digits are unpacked into wide accumulators
(typically 128-bit), after which carry propagation and normalization are
performed to produce a canonical high-precision result. This normalization step
is handled separately and is optimized using parallel-prefix techniques as
described in earlier sections.

\medskip

Together, these steps implement \FractalShark{}'s NTT-based multiplication
pipeline: from high-precision significand limbs, through modular convolution in
$\mathbb{Z}_{\texttt{MagicPrime}}$, and back to an exact, normalized
high-precision product suitable for subsequent arithmetic in reference orbit
computation.

\subsection{Amortizing Synchronization via Simultaneous Products}

In the presented GPU implementation, a dominant cost of large-scale NTT-based
multiplication is not the modular arithmetic itself, but the required grid-wide
synchronization. Each major phase of the pipeline— packing and twisting of
inputs, forward NTT, pointwise multiplication, inverse NTT, and untwisting with
normalization—requires that all threads observe a consistent global state. This
is enforced using explicit grid-level barriers (e.g.,
\texttt{cooperative\_groups::grid\_group::sync()}), whose latency grows with
grid size and is largely independent of the amount of arithmetic performed
between barriers.

To reduce the amortized cost of these synchronization points, the implementation
computes three related convolution products, $X\cdot Y$, $X^2$, and $Y^2$,
within a single NTT pipeline. All three products share the same forward
transforms, inverse transforms, twiddle scheduling, temporary buffers, and
synchronization boundaries. As a result, the fixed cost of grid-wide
coordination and memory visibility is incurred once, while producing three
mathematically independent convolution results. The additional arithmetic
required for the extra pointwise products consists only of a small number of
Montgomery multiplications per frequency index and is negligible compared to the
cost of global synchronization and memory traffic.

At the code level, this strategy is realized by a fused front-end that packs the
input limb arrays, applies twist factors, and converts values into Montgomery
form while producing multiple frequency-domain streams in a single pass. A
single grid-stride pointwise multiplication phase then computes the three
products concurrently. These are followed by a single multiway inverse radix--2
NTT, after which a unified untwisting, scaling by $N^{-1}$, and conversion out
of Montgomery representation are performed. Explicit grid-wide barriers appear
only at true phase boundaries, serving as producer--consumer separators between
global-memory stages rather than as fine-grained synchronization.

This design is well matched to the target workload. In high-precision reference
orbit computation, both cross terms and squared terms arise naturally and
repeatedly. By folding these operations into a single multiway NTT, the
implementation reduces the total number of kernel phases and synchronization
events, increases arithmetic intensity per barrier, and improves overall GPU
utilization. Consequently, the effective performance of the multiplication
pipeline is governed primarily by arithmetic throughput and memory bandwidth,
rather than by synchronization overhead.

\subsection{Usage in \FractalShark{}}
The key design is a persistent \emph{combo} object returned by initialization:

\begin{enumerate}
  \item \texttt{InitHpSharkReferenceKernel}: allocates device/host state for the reference orbit and sets
        \((c_x,c_y)\), max radius, and launch configuration.
  \item \texttt{InvokeHpSharkReferenceKernel}: advances the orbit in bounded batches of at most
        \(\texttt{MaxOutputIters}\), storing results in \texttt{OutputIters}.
  \item \texttt{ShutdownHpSharkReferenceKernel}: frees persistent resources.
\end{enumerate}

Because the precision must be fixed at compile time for the \texttt{
HpSharkFloatParams} specialization, \texttt{DispatchByPrecision} rounds the
requested precision to a power of two and chooses from a fixed set (\(\{256,512,
\dots,524288\}\) bits). Each invocation appends the emitted \texttt{OutputIters}
records into the CPU-side \texttt{PerturbationResults}. Periodicity and escape
are reported through \texttt{PeriodicityStatus} and handled similarly to the CPU
paths.

\subsection{High-Precision Floating-Point Representation in \FractalShark{}}
\label{subsec:hpfloat-model}

The GPU reference orbit backend operates on a custom high-precision
floating-point format implemented by the \texttt{HpSharkFloat} family of types.
This format is intentionally \emph{not} IEEE~754--compatible. Instead, it is
designed to support exact arithmetic on large significands, deterministic
behavior across CPU and GPU backends, and efficient execution on massively
parallel architectures.

Conceptually, an \texttt{HpSharkFloat} value represents a real number of the
form
\[
x = s \cdot M \cdot 2^E,
\]
where $s \in \{-1,+1\}$ is the sign, $M$ is a nonnegative integer significand,
and $E$ is a signed integer exponent. The sign, significand, and exponent are
tracked independently, and no implicit normalization or rounding is performed
during arithmetic operations.

\paragraph{Significand representation.}
The significand $M$ is stored as a fixed-size array of 32-bit limbs. The number
of limbs is determined at compile time by the
\texttt{HpSharkFloatParams} specialization and remains constant throughout the
lifetime of the computation. As a result, precision is explicit and static:
all arithmetic kernels operate on limb arrays of identical size, and no dynamic
reallocation or precision growth occurs during iteration.

The significand represents a nonnegative integer magnitude. Negative values are
handled by explicit sign logic rather than two’s-complement arithmetic, which
simplifies carry propagation and normalization.

\paragraph{Exponent handling and deferred normalization.}
The exponent $E$ is maintained separately from the significand and updated
explicitly by arithmetic kernels. In contrast to IEEE~754 arithmetic,
normalization is not performed eagerly after every operation. Intermediate
results may remain temporarily denormalized, with exponent adjustments and
carry propagation deferred until well-defined normalization phases.

This separation is essential for GPU execution. Immediate normalization would
introduce serial dependencies and global synchronization at every arithmetic
step, whereas deferred normalization allows large batches of arithmetic to be
performed in parallel before a single, optimized carry-propagation pass.

\paragraph{Normalization and canonical form.}
A value is considered to be in canonical form when:
\begin{itemize}
  \item the significand has been fully carry-propagated,
  \item the most significant limb is nonzero (except for the zero value),
  \item the exponent reflects the exact binary scaling of the significand.
\end{itemize}
Normalization is implemented as a separate phase and is optimized using
parallel-prefix carry propagation techniques, as described in earlier
sections.

\paragraph{Rounding, exceptional values, and determinism.}
No rounding is performed during addition, subtraction, or multiplication. All
operations are exact with respect to the fixed significand size. If an
intermediate result were to exceed the representable precision, this would
constitute a logic error rather than silently rounded behavior.

The format does not represent NaNs, infinities, or subnormal values. These
concepts are unnecessary for reference orbit computation, where arithmetic
ranges are structurally controlled and exceptional conditions (such as escape)
are handled at the algorithmic level.

Because the precision is fixed, arithmetic is exact, and no data-dependent
rounding occurs, the \texttt{HpSharkFloat} model provides deterministic,
bitwise-reproducible results across runs and across CPU and GPU backends,
subject only to correct synchronization.

\paragraph{Implications for reference orbit computation.}
This floating-point model is best viewed as high-precision integer arithmetic
with an explicit exponent rather than as an extension of IEEE floating-point.
The design directly enables NTT-based multiplication, parallel carry
propagation, and predictable numerical behavior at extreme precisions, making
it well suited for authoritative reference orbit generation on the GPU.


\section{Checksum-Guided Debugging via Host--Device Cross-Validation}
\label{sec:checksums}

Correctness bugs in GPU high-precision arithmetic are notoriously difficult to
localize: a single off-by-one carry, lane-misaligned shuffle, or missing
synchronization can corrupt results far downstream, often without an obvious
local symptom. To shorten the feedback loop, we instrument the arithmetic
pipeline with lightweight \emph{stage checksums} that can be computed on the GPU,
computed independently on the host reference implementation, and compared to
pinpoint the first divergence. This converts a diffuse ``wrong final answer''
into a small set of candidate stages, dramatically reducing the search space
during development.  These checksums are disabled in production builds to avoid
any performance impact but can be enabled selectively during debugging.

\subsection{Stage Checksums as Homomorphic Summaries}

Let $A \in (\mathbb{Z}/2^w\mathbb{Z})^N$ denote a limb array representing an
intermediate mantissa or scratch buffer (e.g., aligned addends, raw limbwise
sums, carry descriptors, normalized limbs). We associate to each such array a
64-bit checksum
\begin{equation}
  H(A) \in \{0,1\}^{64},
\end{equation}
computed deterministically over the ordered limb sequence. In our implementation
we employ 64-bit rolling checksums (e.g., CRC-64 or Fletcher-64) because they are
(1) inexpensive relative to high-precision arithmetic, (2) order-sensitive, and
(3) easily reducible in parallel.

Each \emph{debug checkpoint} records a tuple
\begin{equation}
  S_k = \bigl(\texttt{Purpose}_k,\, \texttt{Depth}_k,\, \texttt{CallIndex}_k,\,
  \texttt{Conv}_k,\, H(A_k)\bigr),
\end{equation}
where $\texttt{Purpose}$ identifies which logical stage or buffer is being
summarized (e.g., \texttt{ADigits}, \texttt{BDigits}, \texttt{FinalAdd1},
\texttt{Result\_offsetXX}, etc.), and the remaining metadata disambiguates
dynamic execution contexts such as recursion depth or multiple invocations of
the same primitive within a larger operation. This metadata ensures that host
and device checkpoints can be matched without ambiguity even when the same
kernel path is exercised repeatedly in a pipeline.

\subsection{Parallel Computation with Order Preservation}

The checksum must match between host and device, so the GPU computation must be
\emph{equivalent} to the sequential definition. This requires two properties:
\begin{enumerate}
  \item \textbf{Deterministic segmentation}: the array is partitioned into
  contiguous chunks assigned to threads in a deterministic manner.
  \item \textbf{Order-preserving reduction}: partial checksums are combined in
  the same left-to-right order as the sequential traversal.
\end{enumerate}

To make this concrete, consider a checksum family whose internal state can be
represented as a small tuple (e.g., for Fletcher-64, $(s_1,s_2)$ with implicit
length). Each thread processes a contiguous chunk $A[\ell:r)$ and produces a
local state $\sigma_j$. To recover the global checksum, we require an
associative \emph{combine} operator $\oplus$ such that
\begin{equation}
  \sigma(A[0:n)) \;=\; \sigma(A[0:k)) \;\oplus\; \sigma(A[k:n)).
\end{equation}
Associativity enables hierarchical reduction, while order preservation requires
that the reduction tree respects the left-to-right concatenation order. On the
GPU, we implement this reduction hierarchically:
\begin{enumerate}
  \item Each thread computes $\sigma_j$ for its chunk.
  \item Within each warp, we compute an order-preserving prefix/reduction using
  warp shuffles and $\oplus$.
  \item Warp results are combined in increasing warp order to form a per-block
  checksum.
  \item Block results are reduced across the grid using repeated pairwise
  left-to-right combinations, yielding a single 64-bit checksum.
\end{enumerate}
This structure ensures that the device result is \emph{definitionally identical}
to the host's sequential traversal, modulo the checksum's arithmetic rules.

\subsection{Host Reference and Cross-Comparison}

On the host, we compute the same checksum $H(A_k)$ over the corresponding
reference buffers produced by the MPIR/GMP-based implementation (or a trusted
CPU implementation). Because both sides share the identical stage labeling
(\texttt{Purpose}, \texttt{Depth}, \texttt{CallIndex}, \texttt{Conv}), we can
compare checkpoint sequences as keyed records:
\begin{equation}
  \Delta_k \;=\;
  \bigl(H_{\mathrm{gpu}}(A_k) \stackrel{?}{=} H_{\mathrm{host}}(A_k)\bigr).
\end{equation}

A mismatch at checkpoint $k$ implies that the first divergence occurred at or
before the computation that produces $A_k$. By inserting checkpoints at
strategically chosen boundaries (e.g., after exponent alignment, after signed
limbwise accumulation, after carry resolution, after normalization), we obtain a
coarse-to-fine localization mechanism. In practice this behaves like a binary
search over the pipeline: we start with a small number of high-level checkpoints
and then refine around the earliest failing stage by adding more granular
summaries (e.g., per-buffer or per-substep).

\subsection{Practical Debugging Workflow}

The checksum mechanism supports a fast iterative workflow:
\begin{enumerate}
  \item \textbf{Instrument}: enable checksum capture for selected stages and
  record the resulting $(\texttt{key}, H)$ tuples from GPU execution.
  \item \textbf{Replay on host}: run the corresponding host reference path over
  the same inputs, generating the same keyed tuples.
  \item \textbf{Diff}: locate the earliest key for which
  $H_{\mathrm{gpu}} \ne H_{\mathrm{host}}$.
  \item \textbf{Refine}: insert additional checkpoints in the immediate
  neighborhood of the first mismatch (e.g., split ``carry propagation'' into
  descriptor publish, look-back resolution, digit transfer, and final writeback).
\end{enumerate}

This approach is especially effective for bugs that only manifest at scale:
race conditions, mis-specified synchronization, and rare carry-chain corner
cases. Even when the final numerical error is small or intermittent, checksum
mismatches provide a crisp and reproducible signature of divergence.

\subsection{Why Checksums Help for High-Precision Arithmetic}

High-precision arithmetic pipelines are composed of large, structured arrays
whose semantics evolve across stages (aligned limbs $\rightarrow$ raw sums
$\rightarrow$ carry-resolved digits $\rightarrow$ normalized mantissa). A
checksum provides a compact witness of correctness for each stage without
requiring full array dumps or expensive per-element comparisons. While a checksum
does not identify the exact index of the first wrong limb, it reliably answers a
more valuable question during early debugging: \emph{which transformation first
produced an incorrect state?} Once localized, targeted assertions, partial dumps,
or reduced test cases can isolate the exact defect with far less effort.

Taken together, stage checksums act as a low-overhead correctness scaffold that
bridges device execution and a trusted host reference, enabling rapid fault
localization in complex GPU high-precision arithmetic kernels.


\section{Per-Iteration Periodicity Checking under High-Precision Arithmetic on the GPU}
\label{sec:hp-periodicity}

The \FractalShark{} GPU reference-orbit kernel evaluates the Mandelbrot recurrence
\(
z_{n+1} = z_n^2 + c
\)
using high-precision arithmetic for the state \(z_n\), while interleaving a
low-overhead periodicity/termination test between the (expensive) high-precision
multiply and add phases. This interleaving is essential: high-precision
multiplication dominates runtime at large limb counts, so an early stop
condition that can be evaluated with negligible additional cost yields
substantial savings when a cycle is detected or the orbit escapes.

\paragraph{State extraction and logging.}

At iteration \(n\), the high-precision state \((x_n, y_n)\) is available in the
multiplication output registers/structures. The periodicity checker converts
these values into a reduced high-dynamic-range scalar format,
\(\mathrm{HDR}(\cdot)\), via a lossy but monotone extraction
(\texttt{ToHDRFloat}) and stores the resulting pair in a per-iteration trace
buffer:

\begin{equation}
  \widehat{x}_n \leftarrow \mathrm{HDR}(x_n), \quad
  \widehat{y}_n \leftarrow \mathrm{HDR}(y_n), \quad
  \texttt{OutputIters}[n] \leftarrow (\widehat{x}_n, \widehat{y}_n).
\end{equation}

This trace is used both for downstream diagnostics and to provide a compact,
device-side record of the authoritative orbit prefix.

\paragraph{Derivative-based periodicity criterion.}

In addition to the orbit state, we propagate the complex derivative
\(
\frac{dz}{dc}
\)
along the orbit. For the Mandelbrot map \(f(z)=z^2+c\), the chain rule yields

\begin{equation}
  \frac{d z_{n+1}}{d c} = f'(z_n)\frac{d z_n}{d c} + 1
  = 2 z_n \frac{d z_n}{d c} + 1,
  \label{eq:dzdc-recurrence}
\end{equation}

with initialization \(\frac{dz_0}{dc}=0\) when \(z_0=0\) (equivalently, one may
store the pair \((d x_n/dc, d y_n/dc)\) and update in-place).
Writing \(z_n = x_n + i y_n\) and \(d_n = d x_n/dc + i\, d y_n/dc\), expanding
\eqref{eq:dzdc-recurrence} into real and imaginary parts gives:

\begin{align}
  d x_{n+1}/dc &= 2\,(x_n\, d x_n/dc - y_n\, d y_n/dc) + 1, \label{eq:dzdc-real}\\
  d y_{n+1}/dc &= 2\,(x_n\, d y_n/dc + y_n\, d x_n/dc). \label{eq:dzdc-imag}
\end{align}

In our implementation, the derivative accumulators \texttt{dzdcX} and
\texttt{dzdcY} are stored in the same high-precision float type used by the
orbit state, but the periodicity decision itself is performed on reduced HDR
magnitudes to minimize overhead:

\begin{equation}
  \|z_n\|_\infty \approx \max(|\widehat{x}_n|, |\widehat{y}_n|), \qquad
  \left\|\frac{dz_n}{dc}\right\|_\infty \approx \max(|\widehat{d x}_n|, |\widehat{d y}_n|),
\end{equation}

where hats denote reduced HDR representations after a normalization/reduction
step (\texttt{HdrReduce}) to ensure stable comparisons.

The checker uses a sufficient condition of the form

\begin{equation}
  \|z_n\|_\infty < 2\,R\,\left\|\frac{dz_n}{dc}\right\|_\infty,
  \label{eq:periodicity-ineq}
\end{equation}

where \(R\) is a problem-dependent bound (stored as \texttt{RadiusY} in the
reference structure). Intuitively, \eqref{eq:periodicity-ineq} compares the
current orbit magnitude against a scaled sensitivity radius implied by the
derivative growth: when the orbit state becomes small relative to the local
linearization bound, further iteration is treated as converging toward a
previous state and the algorithm flags a periodic cycle.
Concretely, the device computes

\begin{equation}
  n_2 \leftarrow \max(|\widehat{x}_n|, |\widehat{y}_n|), \quad
  r_0 \leftarrow \max(|\widehat{d x}_n|, |\widehat{d y}_n|), \quad
  n_3 \leftarrow 2\,R\,r_0,
\end{equation}

and terminates when \(n_2 < n_3\), setting \texttt{PeriodicityStatus =
PeriodFound} and recording \texttt{OutputIterCount = n+1}.

\paragraph{Escape test.}
We additionally apply a conventional escape-radius test in the same reduced
domain. After forming the next-state candidates
\(
\widehat{x}_n + \widehat{c}_x
\)
and
\(
\widehat{y}_n + \widehat{c}_y
\),
the checker evaluates
\begin{equation}
  \|\widehat{z}_n + \widehat{c}\|_2^2
  = (\widehat{x}_n + \widehat{c}_x)^2 + (\widehat{y}_n + \widehat{c}_y)^2,
\end{equation}
and declares \texttt{Escaped} when this value exceeds a fixed threshold
(\texttt{256.0} in the current implementation). As with the periodicity
criterion, all comparisons are performed on reduced positive HDR values to avoid
high-precision control-flow costs on the device.

\paragraph{Control-flow placement and synchronization.}
Periodicity checking is invoked once per iteration at the top of the reference
step, before launching the high-precision multiplication and addition kernels
that compute \(z_{n+1}\). In the current design, a single distinguished thread
(\texttt{block 0, thread 0}) executes the checker and updates the shared
termination flag stored in the global \texttt{reference} structure. The kernel
then executes a cooperative-grid barrier:
\begin{equation}
  \texttt{grid.sync()},
\end{equation}
ensuring that (i) all blocks observe a consistent \texttt{PeriodicityStatus} and
(ii) any updates to \texttt{dzdcX}, \texttt{dzdcY}, and the trace buffer are
visible before subsequent work proceeds. If termination is requested, the
iteration loop exits immediately; otherwise, the kernel proceeds into the
multiply-add pipeline that produces the next high-precision state.

This organization isolates the control-intensive termination logic from the
throughput-oriented NTT multiplication and carry-propagating addition stages,
while maintaining deterministic, grid-wide early-exit behavior. The only global
synchronization introduced by periodicity checking is the single \texttt{grid.sync()}
already required by the cooperative multi-block arithmetic pipeline, making the
incremental overhead of periodicity checking negligible relative to the cost of
high-precision multiplication at large precisions.

\paragraph{Alternative host-side periodicity checking.}
An alternative design would be to retain the authoritative high-precision orbit
state entirely on the device, periodically copy the full-precision values
\((x_n, y_n, dz_n/dc)\) back to the host, and perform periodicity and escape
testing on the CPU using existing MPIR-based logic. While conceptually simpler,
this approach is impractical at large precisions: each iteration would require
transferring hundreds to thousands of limbs per component over the PCIe bus,
and the cumulative bandwidth and synchronization costs would dominate the total
runtime. Moreover, host-side checking would introduce additional latency and
force a tighter coupling between device execution and CPU control flow. By
performing periodicity detection directly on the GPU using reduced HDR
representations, the implementation avoids repeated large data transfers while
preserving an early-termination mechanism that is both inexpensive and tightly
integrated with the cooperative-grid execution model.



\section{Perturbation-only Mandelbrot rendering (without linear approximation)}
\label{sec:perturb-only}

This kernel can render the Mandelbrot set using \emph{perturbation alone}, i.e.,
without taking any LA v2 linear-approximation steps. The same CUDA entry point
\code{mandel\_1xHDR\_float\_perturb\_lav2<IterType,T,SubType,Mode,PExtras>} is
used; the behavior is selected at compile time via \code{LAv2Mode}. In
particular, when \code{Mode} includes only the perturbation path (e.g.\
\code{LAv2Mode::PO}), the kernel skips the LA stage traversal and runs only the
perturbation loop against a stored reference orbit.

\subsection{Rendering objective}
\label{sec:perturb-only-goal}

The goal remains standard escape-time rendering for
\[
z_{n+1} = z_n^2 + c,\qquad z_0=0,
\]
with bailout \(|z|^2 \ge 4\). For each pixel, the kernel computes the parameter
\(c\), iterates until escape or \code{n\_iterations}, and stores the resulting
iteration count in \code{OutputIterMatrix[idx]}.

In perturbation rendering, the expensive high-precision orbit evaluation for
each pixel is avoided by reusing a \emph{reference orbit} computed at a
reference parameter \(c_\star\), then evolving only the \emph{delta orbit} for
nearby pixels.

\subsection{Reference orbit and delta formulation}
\label{sec:perturb-only-deltas}

Let the reference parameter be \(c_\star\), with stored reference orbit
\(\{z_n^\star\}\) satisfying
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a pixel parameter \(c\) near \(c_\star\), define the parameter delta
\[
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star,
\]
and the orbit delta
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star.
\]
Substituting \(z_n = z_n^\star + \Delta z_n\) into the Mandelbrot recurrence
yields the \emph{exact} delta recurrence:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right) \\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
Perturbation uses this recurrence directly: it is not a linearization. The work
per iteration is reduced because \(z_n^\star\) is fetched from storage rather
than recomputed in high precision.

\subsection{Pixel parameter delta \texorpdfstring{$\Delta c$}{Delta c}}
\label{sec:perturb-only-deltac}

For each pixel \((X,Y)\), the kernel constructs a delta parameter relative to a
chosen reference center (sign conventions incorporate the image mapping):
\begin{align}
\Delta c_x &= dx \cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy \cdot Y - \texttt{centerY},
\end{align}
and stores this as \code{DeltaSub0} (with scalar components \code{DeltaSub0X},
\code{DeltaSub0Y}). The perturbation state starts at
\[
\Delta z_0 = 0,
\]
so \code{DeltaSubN} is initialized to zero.

\subsection{Perturbation recurrence used in the kernel}
\label{sec:perturb-only-update}

Writing the reference sample as \(z^\star = x^\star + i y^\star\) and the delta
as \(\Delta z = \Delta x + i \Delta y\), the delta recurrence can be expressed
in a factored form convenient for implementation:
\begin{equation}
\Delta z \leftarrow \Delta z \cdot (2 z^\star + \Delta z) + \Delta c.
\label{eq:perturb-factor}
\end{equation}
This identity is equivalent to
\(\Delta z_{n+1} = 2 z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c\), because
\(\Delta z\cdot(2z^\star+\Delta z) = 2z^\star\Delta z + (\Delta z)^2\).

The kernel implements \cref{eq:perturb-factor} in real arithmetic by forming
the sums
\[
(2x^\star + \Delta x),\qquad (2y^\star + \Delta y),
\]
then updating:
\begin{align}
\Delta x &\leftarrow \Delta x\,(2x^\star + \Delta x) - \Delta y\,(2y^\star + \Delta y) + \Delta c_x, \\
\Delta y &\leftarrow \Delta x\,(2y^\star + \Delta y) + \Delta y\,(2x^\star + \Delta x) + \Delta c_y.
\end{align}
In the code, the intermediate quantities correspond to:
\begin{verbatim}
tempSum1 = 2*zy + DeltaSubNYOrig;  // (2 y^\star + \Delta y)
tempSum2 = 2*zx + DeltaSubNXOrig;  // (2 x^\star + \Delta x)
\end{verbatim}
followed by the real/imag updates. For extended or HDR types, the kernel routes
the same math through type-specialized implementations
(\code{T::custom\_perturb2} / \code{T::custom\_perturb3}) to keep the inner loop
tight and to enforce the type's reduction/normalization rules.

\subsection{Reconstructing the absolute orbit for escape testing}
\label{sec:perturb-only-reconstruct}

Perturbation evolves \(\Delta z_n\), but escape-time rendering requires a
bailout test on the absolute orbit \(z_n\). Each iteration reconstructs:
\[
z_n \approx z_n^\star + \Delta z_n,
\]
using the stored reference sample \(z_n^\star\) and the current delta. In the
kernel, this appears as:
\[
x = x^\star + \Delta x,\qquad y = y^\star + \Delta y.
\]
The bailout test is then performed on
\[
|z|^2 = x^2 + y^2,
\]
with the canonical threshold \(4\). For HDR and related types, the norm and the
comparison are performed on reduced values using specialized reduced
comparators to keep the escape decision stable at deep zoom.

\subsection{Reference index management and rebasing}
\label{sec:perturb-only-rebase}

The perturbation loop advances a reference-orbit index \code{RefIteration} in
lockstep with \code{iter}, fetching \(z^\star\) samples from \code{Perturb}. The
kernel includes a \emph{rebasing} mechanism that resets the delta
representation when it becomes ill-conditioned. Conceptually, if the delta
becomes comparable to or larger than the reconstructed orbit, the decomposition
\(z = z^\star + \Delta z\) stops being numerically advantageous. In that case,
the kernel folds the delta into the base by setting:
\[
\Delta z \leftarrow z,\qquad \text{and restart the reference index.}
\]
Operationally, the code compares the reconstructed orbit magnitude proxy
against the delta magnitude proxy, and also rebasing if the reference index
reaches the end of the stored orbit samples (to avoid out-of-range sampling).
After rebasing, perturbation continues from the new base representation.

This mechanism keeps the perturbation method usable across a wide range of
pixels and iteration depths while preserving the core rendering objective:
compute escape-time using a stable bailout on the reconstructed orbit.

\subsection{Using \code{LAv2Mode} to select perturbation-only execution}
\label{sec:perturb-only-mode}

The kernel is structured as two compile-time phases:
\begin{itemize}
\item an LA v2 phase guarded by \code{Mode == Full || Mode == LAO},
\item a perturbation phase guarded by \code{Mode == Full || Mode == PO}.
\end{itemize}
Therefore, perturbation-only rendering is achieved by instantiating the kernel
with a mode that includes only the perturbation path (e.g.\ \code{LAv2Mode::PO}).
In this configuration:
\begin{itemize}
\item \code{DeltaSub0} is computed from the pixel location,
\item \code{DeltaSubN} remains initialized to \(\Delta z_0 = 0\),
\item the kernel runs the perturbation loop, reconstructing \(z\) each step for
      bailout tests,
\item the final escape-time count is written to \code{OutputIterMatrix}.
\end{itemize}
This provides a complete Mandelbrot renderer based purely on reference-orbit
perturbation, without relying on any linear-approximation hierarchy.


\section{TODO mess approximation-based acceleration for Mandelbrot rendering}
\label{sec:approx-accel}

At deep zoom, the cost of iterating high-precision types for every pixel can be
dominant. The remaining components described below are still in service of the
same rendering goal: compute escape-time for \(z_{n+1}=z_n^2+c\), but by reusing
a \emph{reference orbit} and evolving \emph{deltas} for nearby pixels.

\subsection{Bilinear approximation (BLA) for orbit deltas}
\label{sec:bilinear-approx}

\subsubsection{Reference orbit and delta formulation}
Let the reference parameter be \(c_\star\) with orbit:
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a nearby pixel parameter \(c = c_\star + \Delta c\), define:
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star,\qquad
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star.
\]
Expanding the recurrence gives:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right)\\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
When \(\Delta z_n\) remains small, the quadratic term can be neglected,
yielding a linearized update:
\begin{equation}
\Delta z_{n+1} \approx A_n\,\Delta z_n + B_n\,\Delta c,\qquad
A_n = 2z_n^\star,\quad B_n = 1.
\end{equation}
BLA generalizes this into precomputed multi-step maps that \emph{jump} multiple
iterations while maintaining an explicit validity bound.

\subsubsection{What \code{BLA<T>} stores}
A \code{BLA<T>} instance stores two complex coefficients:
\[
A = A_x + iA_y,\qquad B = B_x + iB_y,
\]
plus:
\begin{itemize}
  \item \code{r2}: a squared-radius validity bound used during lookup,
  \item \code{l}: the number of Mandelbrot iterations summarized by this step.
\end{itemize}
These objects exist to accelerate \emph{escape-time evaluation} by evolving
\(\Delta z\) cheaply for many pixels, then reconstructing \(z \approx z^\star +
\Delta z\) to perform bailout checks consistent with Mandelbrot rendering.

\subsubsection{Applying a step: complex multiply-add}
The method \code{getValue(RealDeltaSubN, ImagDeltaSubN, RealDeltaSub0, ImagDeltaSub0)}
applies:
\[
\Delta z \leftarrow A\,\Delta z + B\,\Delta c,
\]
expanded into real arithmetic:
\begin{align}
\Re(\Delta z') &= A_x \Re(\Delta z) - A_y \Im(\Delta z) + B_x \Re(\Delta c) - B_y \Im(\Delta c), \\
\Im(\Delta z') &= A_x \Im(\Delta z) + A_y \Re(\Delta z) + B_x \Im(\Delta c) + B_y \Re(\Delta c).
\end{align}

\subsubsection{Composing steps to build longer jumps}
If one step maps \(\Delta z \mapsto A_x\Delta z + B_x\Delta c\) and a second maps
\(\Delta z' \mapsto A_y\Delta z' + B_y\Delta c\), the composition is:
\[
A_{\text{new}} = A_yA_x,\qquad B_{\text{new}} = A_yB_x + B_y.
\]
This supports a hierarchy of step sizes (often powers of two) for quickly
advancing delta orbits while rendering the escape-time field.

\subsubsection{GPU lookup: selecting a valid aligned step}
A GPU-side helper such as \code{GPU\_BLAS} stores the hierarchy and selects a
step that is both \emph{aligned} with the current iteration index and
\emph{valid} under the current bound check (typically comparing a computed
squared-magnitude proxy \code{z2} against \code{r2}). When a step is valid, the
renderer can advance the orbit by \code{l} iterations at a cost far below
performing \code{l} full high-precision Mandelbrot updates.

\subsection{LA v2 linear approximation with perturbation (HDR kernel)}
\label{sec:la-v2-perturb}

This kernel family combines staged linear-approximation steps with a
perturbation finisher loop against a stored reference orbit. The rendering
objective remains escape-time Mandelbrot evaluation; the kernel accelerates
that evaluation by evolving deltas and periodically reconstructing \(z\) to
perform bailout checks.

\subsubsection{Parameter delta per pixel}
Each pixel constructs a parameter offset \(\Delta c\) relative to a selected
reference center (sign conventions may incorporate the image \(Y\)-flip):
\begin{align}
\Delta c_x &= dx\cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy\cdot Y - \texttt{centerY},
\end{align}
and packs \(\Delta c=\Delta c_x+i\Delta c_y\) into \code{DeltaSub0}.

\subsubsection{Delta state and reconstruction}
The kernel maintains \(\Delta z_n\) in \code{DeltaSubN} and reconstructs an
absolute orbit estimate using the stored reference orbit sample
\(z_j^\star\):
\[
z \approx z_j^\star + \Delta z.
\]
Escape-time rendering then proceeds by applying approximation steps when valid,
or performing perturbation updates otherwise, while periodically testing the
bailout condition on the reconstructed \(z\).

\subsubsection{Perturbation update}
Given a reference sample \(z^\star=x^\star+iy^\star\) and \(\Delta z=\Delta
x+i\Delta y\), the perturbation form is:
\[
\Delta z \leftarrow \Delta z\cdot(2z^\star+\Delta z) + \Delta c.
\]
In real arithmetic, with temporaries corresponding to \((2x^\star+\Delta x)\)
and \((2y^\star+\Delta y)\), this yields:
\begin{align}
\Delta x &\leftarrow \Delta x\,(2x^\star+\Delta x) - \Delta y\,(2y^\star+\Delta y) + \Delta c_x, \\
\Delta y &\leftarrow \Delta x\,(2y^\star+\Delta y) + \Delta y\,(2x^\star+\Delta x) + \Delta c_y.
\end{align}
In code comments, write these as \(x^\star\) and \(\Delta x\) (and similarly for
\(y\)) rather than using Unicode symbols.

\subsubsection{Escape test}
After updating \(\Delta z\), reconstruct \(z\approx z^\star+\Delta z\) and test:
\[
|z|^2 = x^2+y^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\]
For HDR/expanded types, norms and comparisons are performed using reduced values
and specialized reduced comparators, preserving meaningful bailout decisions at
deep zoom.



\section{Disk-Backed Growable Vectors with Stable Virtual Addresses}
\label{sec:disk_backed_growable_vectors}

High-resolution Mandelbrot rendering and reference-orbit techniques impose
extreme demands on memory management. At deep zoom levels, a single authoritative
reference orbit may contain tens of billions of elements; for example, an orbit
with $2.7\times10^{10}$ iterations already implies a logical memory footprint far
beyond what can be held resident in RAM, even with compact per-iteration storage.
At this scale, both the cost of allocation and the cost of relocation become
dominant design constraints.

Several subsystems in \FractalShark{}—including orbit logging, metadata streams,
debug outputs, and the custom heap allocator—require \emph{growable} storage
while simultaneously maintaining \emph{stable virtual addresses} for previously
returned pointers. This requirement arises naturally when the program hands out
interior pointers into a buffer (e.g., into an orbit record array or a heap
arena). Any reallocation-and-copy strategy would invalidate those pointers and
would require copying enormous amounts of data, which is computationally
infeasible at multi-billion-element scales.

In addition to scale and pointer stability, persistence is a first-class
concern. Reference orbits are expensive to compute but comparatively cheap to
reuse. If orbit data can be left resident on disk and later reloaded simply by
mapping the same backing file back into the process address space, recomputation
can be avoided entirely. This enables efficient multi-pass rendering workflows,
interactive debugging, and reuse of authoritative reference orbits across program
invocations.

To address these requirements, we implement a
\texttt{GrowableVector<EltT>} abstraction that combines (i) a fixed,
virtually-contiguous address range reserved up front and (ii) incremental growth
within that range by committing additional pages on demand. Growth is performed
in place, so the base address never changes and no copying is required as
capacity increases. The abstraction supports two complementary backing modes:

\begin{enumerate}
  \item \textbf{Anonymous (pagefile-backed) growth.} A large virtual region is
  reserved once via \texttt{VirtualAlloc(..., MEM\_RESERVE, ...)} and pages are
  subsequently committed within that region using
  \texttt{VirtualAlloc(base, bytes, MEM\_COMMIT, ...)}. Because each commit
  targets the original base address, successful growth preserves
  \texttt{m\_Data} and does not require relocation.

  \item \textbf{File-backed (disk-backed) growth.} The reserved address range is
  backed by a disk file and mapped into the process using a section object. In
  this mode, pages are demand-paged from the backing file and become resident
  only as accessed, substantially reducing peak commit requirements when the
  logical capacity greatly exceeds physical memory. When persistence is enabled,
  the same backing file can later be remapped to recover the stored data without
  recomputation.
\end{enumerate}

\subsection{Stable-address growth via reserve-and-commit}
\label{sec:reserve_commit}

At initialization time, the vector establishes an address anchor:
\begin{itemize}
  \item In anonymous mode, the anchor is created by reserving a large region
  (typically sized to a fraction of physical memory, or an explicit override)
  and recording the returned base pointer \texttt{m\_Data}.
  \item In file-backed mode, the anchor is created by mapping a section at a
  base address and thereafter requiring all remaps/extends to preserve that base.
\end{itemize}

Growth proceeds by increasing the logical capacity \texttt{m\_CapacityInElts}
without moving data. In anonymous mode, this corresponds to committing
additional pages within the reserved region. In file-backed mode, it corresponds
to extending the underlying section and allowing the view to cover the expanded
range. Critically, in both cases we \emph{never} allocate a second buffer and
copy: the address stability is enforced by explicitly requesting the existing
base address and treating any base-address change as a hard error.

\subsection{File-backed mapping for low-commit large buffers}
\label{sec:file_backed_mapping}

When a filename is provided (or a temporary filename is generated), the vector
opens or creates a backing file and constructs a section object over it. We use
native NT section APIs (e.g., \texttt{NtCreateSection},
\texttt{NtMapViewOfSection}, and \texttt{NtExtendSection}) to obtain a
reserve-style mapping suitable for later extension.

Conceptually, the pipeline is:
\begin{enumerate}
  \item \textbf{Open/Create backing file.} The file can be persistent (saved
  results) or temporary (e.g., created with delete-on-close semantics when the
  data is not meant to be retained).
  \item \textbf{Create section.} A section is created with read/write
  protection. For growable vectors, the section is configured so that the view
  can be reserved and later extended.
  \item \textbf{Map a view to establish the base address.} The first mapping
  produces the stable anchor pointer \texttt{m\_Data}. Subsequent growth must
  preserve this base.
  \item \textbf{Extend on demand.} When capacity increases, the section is
  extended to the new byte size. Because the base mapping remains fixed, all
  previously returned pointers remain valid.
\end{enumerate}

This approach has two key practical benefits:
\begin{itemize}
  \item \textbf{Stable pointers with large logical capacity.} The program may
  reserve and grow to very large capacities (hundreds of GiB or more) without
  the pointer invalidation inherent in \texttt{realloc}-style strategies.
  \item \textbf{Reduced peak commit pressure.} The OS can keep most of the
  logical buffer non-resident until touched; pages are faulted in as accessed
  and can be evicted back to disk under memory pressure. This is particularly
  effective when the workload has a large addressable dataset but a much smaller
  active working set.
\end{itemize}

\subsection{Trimming and persistence policies}
\label{sec:trim_persist}

The vector supports multiple policies that control whether data is persisted:
\begin{itemize}
  \item \textbf{Persistent save (\texttt{EnableWithSave}).} Upon \texttt{Trim()},
  the backing file is truncated to match the used size, ensuring the on-disk
  representation reflects the logical content exactly.
  \item \textbf{Temporary file-backed (\texttt{EnableWithoutSave}).} Upon
  \texttt{Trim()}, the view can be remapped to match the used size while
  \emph{reusing the same base address}. This reduces the mapped span without
  sacrificing pointer stability, and the file is automatically deleted when the
  handle closes.
  \item \textbf{Anonymous (\texttt{DontSave}).} Trimming is a logical operation
  (used size changes) while the reserved region remains as the stable anchor.
\end{itemize}

\subsection{Using the same mechanism for the heap arena}
\label{sec:heap_uses_growable_vector}

The custom heap allocator builds its arena on top of the same
\texttt{GrowableVector<uint8\_t>} abstraction. The heap is initialized by
creating a file-backed growable vector for the heap arena and committing an
initial chunk of memory. When the allocator needs more space, it expands the
arena by growing the underlying vector in-place and then updating the heap's
wilderness block (coalescing if possible). Because the arena grows within a
reserved and stable virtual range, all heap pointers returned to the program
remain valid across heap expansions, and expansion does not require copying or
relocating the heap.

\subsection{Portability and backing-file considerations}
\label{sec:disk_backed_portability_notes}

The implementation described above relies on Windows-specific virtual memory and
section-mapping semantics, in particular the ability to reserve a virtually
contiguous address range and to grow into that range while preserving a fixed
base address. On Windows, this is achieved using native section objects backed by
ordinary (non-sparse) files whose size is explicitly extended as the logical
capacity grows.

An important consequence of this design choice is that the backing file itself
physically grows on disk as the vector expands, even if large portions of the
mapped address space are never touched. Windows mitigates the runtime memory
impact by demand-paging pages into RAM and evicting them back to disk under
memory pressure, but the on-disk footprint still reflects the full logical extent
of the section. This behavior is acceptable—and often desirable—on Windows,
where file-backed sections are tightly integrated with the operating system's
paging and commit model.

More specifically, Windows treats a section backed by a regular (non-sparse)
file with a well-defined size as a complete and deterministic backing store for
any page in the mapped range. Page faults are resolved by reading from the file,
and evicted pages can be written back to the same file without requiring
additional allocation or metadata decisions at fault time. By explicitly
growing the backing file as capacity increases, the implementation ensures that
the backing storage for every potential page already exists. This aligns with
Windows-native paging expectations and simplifies correctness guarantees around
section growth, page-in, and page-out behavior.

On other platforms, or in alternative designs, a \emph{large sparse file} may be
a more appropriate backing store for growable, disk-backed vectors. Sparse files
allow the logical file size to greatly exceed the actual disk space consumed,
allocating physical blocks only for regions that are written. When combined with
memory-mapped I/O, this can preserve the core advantages of the approach—stable
virtual addresses and low resident memory usage—while also minimizing disk space
consumption for unused regions.

The present implementation intentionally does not rely on sparse-file behavior,
favoring predictability and robustness of paging semantics over minimizing
nominal disk usage. This choice, while well aligned with Windows-native behavior,
has implications for portability: environments that do not support extending a
mapped view in place, or that impose stricter constraints on file-backed mappings
(such as Wine or certain Unix-like kernels), may require alternative strategies.
Possible approaches include explicit sparse-file usage, preallocation of large
sparse sections, or fallback allocators that relax the stable-address requirement
in exchange for broader compatibility.



\section{Development Notes}

This section simply has some development notes that were posted over time.

\subsection{2026-1-18 News --- Version 0.51}

\begin{itemize}
\item \textbf{Basic Wine compatibility.}  Tested without a GPU and working under
Wine 9.0 on Linux.   There are still some quirks, but basic rendering works in
CPU-only mode. Whether FractalShark can work with Wine + CUDA remains unknown as
an appropriate test platform is not currently available.
\item \textbf{Proper fallback when no GPU is present.}  The program now detects lack of a CUDA
device and falls back to CPU-only rendering, with a warning message.
\item \textbf{Fixed some windowing bugs.}
\item \textbf{Improved these notes.}
\item \textbf{Linked at FractalForums.}  See \url{https://fractalforums.org/index.php?topic=5564.0}
\end{itemize}

\subsection{2025-12-29 News --- Version 0.5}
\begin{itemize}
  \item \textbf{Major feature: GPU-accelerated Mandelbrot reference orbit.} Adds
  an experimental CUDA reference-orbit calculator intended to complement the
  existing CPU/MPIR multithreaded approach.

  \item \textbf{High-precision arithmetic pipeline on GPU.} Includes a full
  multiply/add/subtract pipeline, a Number Theoretic Transform (NTT)
  implementation, parallel-prefix carry propagation, and HDR-style exponent
  tracking.

  \item \textbf{Interoperability and correctness.} Supports conversion between
  MPIR and the GPU high-precision float format and adds manually-run test
  infrastructure.

  \item \textbf{Performance notes.} Release notes report large speedups at high
  limb counts on RTX 4090/5090-class GPUs; intended to shine at very deep
  built-in views.

  \item \textbf{Project plumbing.} Hooks up GitHub Actions to produce “official”
  builds, plus some refactoring; notes a startup windowing quirk.
\end{itemize}

\subsection{2025-11-30 News}

The neat thing about this GPU approach is that it still has low-hanging fruit 
related to optimization, unlike MPIR.  I've been wasting a bunch of time on the 
(Thanksgiving in USA) holiday weekend working on it.  I'll keep fussing with it 
and will probably not post again until I put out a version of \FractalShark{} with 
it hooked up, which I expect to happen later in December when I have time off.  
All code is on github, just no new version yet since it remains rather 
experimental and hacked up.

Example times in ms of updated implementation, comparing against serial host-
based approach (1 thread MPIR AVX2 for experiments).  This is the first 20000 
iterations of View 30 in \FractalShark{}, which is a depth $\sim 1\mathrm{e}{100000
}$ point.  This uses 16384 limbs on the GPU.  The CPU/MPIR uses the minimum 
bits required for that point, which is less, because the GPU implementation 
requires a power of 2.

\begin{verbatim}
Host (ms)    GPU(ms)    Ratio
57990        2055    28.2189781
58227        2022    28.79673591
57478        2041    28.16168545
57538        2014    28.56901688
55997        2015    27.79007444

Averages:
57446        2029.4    28.30729816
\end{verbatim}

Here's a summary of what I want to get done before \FractalShark{} 0.5 happens:

\begin{itemize}

\item Perform code cleanup, better comments etc.  It's a mess right now, this 
is just a weekend hobby after all and because it was unclear this would even 
work it's a real hack job.

\item Improve integration with \FractalShark{}, it's really just hacked in there 
currently since I've mostly used a standalone test program

\item Improve reference orbit memory usage - I would like to be able to 
allocate a fixed amount of memory to store the orbit and expand as needed.  
Right now it pessimistically allocates a lot, and it may be the wrong amount 
because it does periodicity detection on the GPU.  This is just engineering 
work and nothing fundamental, but will take time and without it the current 
implementation is rather impractical.

\item Automatically choose kernel launch parameters, right now it's manual.  
More engineering.  This is required to support other cards than mine.  In any 
case this implementation requires a feature call cooperative groups, which I 
believe implies Nvidia RTX 2XXX series or newer.

\end{itemize}

Those are the main things offhand, and I'm hoping with time off in December I can clean these up.

\subsection{2025-9-1 News}

The full reference orbit works with CUDA, though without periodicity detection 
or high precision to ``float exp'' conversion.  That's future work but I'm not 
worried about it.  To be clear, this is not hooked up end-to-end with 
\FractalShark{} itself, it's only working in a standalone test environment.  But 
the results are promising and prove it works.

This initial implementation relies on Karatsuba for the multiplies/squaring and 
then follows those with the high-precision adds/subtracts.  Initial results 
suggest a $\sim 12\times$ perf improvement relative to single-threaded CPU 
only, when comparing an overclocked 5950X vs an RTX 4090 with CUDA.  I'm happy 
with that, but not completely.

The main performance problem is this Karatsuba implementation.  Getting decent 
performance out of Karatsuba obviously requires recursion, and that gets costly 
on the GPU.  This implementation recurses several levels, which avoids costly 
local memory spill, but bites us because of register pressure.  The high 
register pressure limits the parallelism we can achieve.  The nice thing about 
Karatsuba for me is that it's not that hard to understand conceptually, so it 
was a great initial target for someone who doesn't know what they're doing.

Now that it's working, and I have a better sense of what's going on, I'm going 
to try a full NTT-based high-precision multiply approach.  The idea here is to 
rely on the number theoretic transform, similar to FFT, and parallelize the 
high precision multiply that way.

With this commit, we have a working host-based (CPU-only) approach to NTT high-
precision multiply that supports power-of-2 mantissa sizes and should scale 
effectively to CUDA but that's TBD.  It will be at least several months more 
work at my current rate (a few hours on the weekends) to achieve a first-cut 
CUDA implementation.

\subsubsection{NTT-based high-precision multiply (magic prime $2^{64} - 2^{32} + 1$)}

AI-generated slop follows in this subsection.  It looks accurate.

I'm experimenting with an NTT implementation over the 64-bit ``magic'' prime $p 
= 2^{64} - 2^{32} + 1$. This prime is NTT-friendly: it admits $2^{32}$-th roots 
of unity, so power-of-two transform sizes are straightforward, and it enables 
fast modular reduction on 128-bit products using the identity $2^{64} \equiv 2^{
32} - 1 \pmod p$.

\paragraph{High-level plan}

\begin{itemize}

\item Represent big mantissas as base-2 limbs (currently 32-bit limbs are 
convenient on GPU/CPU). Choose $N =$ next power of two $\ge 2\cdot L$ ($L =$ 
limb count) for the convolution length.

\item Forward NTT(A), NTT(B) mod $p$, pointwise multiply, inverse NTT, multiply 
by $N^{-1} \bmod p$, then perform carry propagation back to the chosen limb 
base.

\item Use iterative radix-2 Cooley--Tukey with an explicit bit-reversal 
permutation (DIT). Twiddles (powers of a primitive root) are precomputed and 
cached.

\item Butterflies and pointwise products operate in Montgomery form; 128-bit 
products are reduced via Montgomery multiplication ($R = 2^{64}$). A direct 
pseudo-Mersenne fold $(\text{lo} + (\text{hi} \ll 32) - \text{hi})$ exists but 
isn’t used on the hot path.

\end{itemize}

\paragraph{Notes and guardrails}

\begin{itemize}

\item Single-prime NTT is attractive here because $p$ fits in 64 bits and gives 
ample dynamic range; if/when larger bases or tighter bounds are desired, a multi
-prime CRT variant is the next step.

\item Power-of-two sizes only: that matches the current host prototype and 
simplifies CUDA mapping.

\item Carry fix-up remains outside the NTT and is done in base-$2^k$ with linear
-time passes; lazy (deferred) carries may help throughput.

\end{itemize}

\paragraph{Why this might beat Karatsuba on GPU}

\begin{itemize}

\item Avoids deep recursion and its register pressure; most work is regular 
butterflies, which parallelize and schedule well.

\item Pointwise multiplies dominate cost but are simple $64\times 64 \rightarrow
 128$ with fast reduction; memory access is structured and coalesced.

\end{itemize}

If the CUDA path pans out, the NTT route should scale better across precisions 
while keeping occupancy higher than the recursive Karatsuba path.

\paragraph{References (NTT / GPU big-int)}
\begin{itemize}
\item CGBN: CUDA Big-Num with Cooperative Groups \cite{CGBN_NVlabs}
\item Number-theoretic transform \cite{NTT_Wikipedia}
\end{itemize}

\subsection{2025-6-15 News}

This page actually gets traffic occasionally, so I just wanted to post a short 
update.  Since last August, I've been working on a CUDA-based, high-precision 
reference orbit implementation.  The objective is to beat \FractalShark{}'s 
existing multithreaded reference-orbit performance at higher digit counts, at 
least if you have a decent card.  Scroll down to ``[2025-6-15](\#2025-6-15)'' 
for the latest information on this subject.

Still fussing with it, with some delays because of vacation etc.  Having some 
issues with the optimized ``add'' implementation that does the 5-way add/
subtract.  It's a fun project, but has ended up more complex than I'd 
expected.  The reference implementation is almost working the way I want.

Worst case I could dump it and fallback to a series of regular A+B adds/
subtracts but I'm pretty determined to make the optimized approach work.  TBD 
if the performance actually pays off.  (Yes, I can hear you saying the 
Mandelbrot multiplies/squares dominate the cost, but it's bugging me and fun to 
play with).

\subsection{2025-4-26}

This high-precision arithmetic project is a lot of fun even if it's pretty 
ameteur-hour -- I know I'm leaving a lot of perf on the table yet.

Here's a brief update.  I'm happy enough with Karatsuba multiply now.  I've got 
$3\times$ parallel multiplies working.  For Mandelbrot, that corresponds to $x^2
$, $y^2$ and $x\cdot y$.  Rather than doing an optimized squaring 
implementation, I'm just jamming everything into the same Karatsuba 
implementation, so that all the synchronization is re-used.

A few weeks ago I had CUDA floating point add working.  That's much easier of 
course, though carry propagation is interesting.  I tried a parallel prefix sum 
but the performance was a bit underwhelming in the average case, which is what 
I care about.  I instead implemented a more naive strategy that has better 
average case perf and linear worst-case performance, which I think I'm fine 
with for Mandelbrot.  I'm not using warp-level primitives and haven't hooked up 
shared memory on that, so it's horrid performance compared to simply doing it 
on the CPU but as a percentage of the total it's minor, because large 
multiplies are so costly.  I'm not that worried about Add at this point.

I'm currently hooking up a 5-way combined add/subtract that does $A - B + C$ 
and $D + E$ in parallel to produce two outputs.  These inputs corresponds to $X^
2 - Y^2 + C_x$ and $2\cdot X\cdot Y + C_y$.

Strategy-wise, the idea is to complete this multi-way add operator, and then we 
should be able to do a reference orbit using alternating $3$-way multiply and $5
$-way add calls in CUDA.  It also needs periodicity detection and high-
precision to float+exp conversion, which shouldn't be bad.  Maybe in a few 
months I'll have something working end-to-end.

I was also speculating about trying Schönhage--Strassen CUDA multiply, but 
that's a ways out.

\subsection{2025-3}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepency arises in the CUDA 
calculation.  The discrepency points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \texttt{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \texttt{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \texttt{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

Current best result (5950X MPIR vs RTX 4090), 50000 sequential multiplies of 
7776-limb numbers, 128 blocks, 96 threads/block, uses shared memory and 162 
registers (max 255):

Host iter time: 26051 ms

GPU iter time: 2757 ms

Edited from earlier, fussing with perf-related parameters.  I'm very happy with how it's looking now.

\subsection{2025-2 -- What's going on with this native CUDA reference orbit calculation?}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepency arises in the CUDA 
calculation.  The discrepency points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \texttt{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \texttt{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \texttt{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

\subsection{2025-1 -- What's going on with this native CUDA reference orbit calculation?}

The repository (``TestCuda'') has a new Karatsuba, high-precision, floating 
point multiply implementation working on my GPU and results are showing the CPU 
take $3$--$4\times$ longer (MPIR/AVX2) than the GPU on sequential multiplies of 
random numbers.  That's a key point -- sequential multiplies, so the result is 
applicable to e.g.\ a reference orbit calculation.  I'm really happy with this 
result, because there's leftover hardware on the GPU that could be used to run 
a couple of these in parallel (e.g.\ two squares and a multiplication or three 
squares for Mandelbrot).

Getting high-throughput high-precision on-GPU is already more-or-less solved:  
Nvidia already provides a library for it (see related work below).  But getting 
sequential to work decently at sizes that are still interesting (e.g.\ ones we 
might actually try on the Mandelbrot) is not as widely investigated, which is 
why I've been dwelling on this for a while and still have only mediocre results 
:p

A variety of caveats currently:

\begin{itemize}

\item I'm comparing a 5950X vs RTX 4090, which clearly affects the relative 
numbers.

\item The approach requires CUDA cooperative groups, which I think is RTX 2xxx 
and later, so fairly recent cards.

\item The size I'm getting the best result at currently is relatively large: 
8192 32-bit limbs, which is pretty big.  It still beats the CPU down to 2048 
limbs though (CPU takes $1.6\times$ longer here) and at that size there is a 
bunch of unused hardware on the GPU so it should be possible to do the three 
multiplies for Mandelbrot in parallel.

\end{itemize}

Anyhow, I wanted to post it because this is a pretty complex investigation and 
I expect to spend some more time on it because it's been fun to look at.

Here are some bullet points on the approach:

\begin{itemize}

\item Uses CUDA cooperative groups, so we should be able to do a reference 
orbit with a single kernel invocation.

\item 32-bit limbs, 128-bit intermediate results (2x64 integers) because of 
intermediate carries.  Really it's just 64 bits + plus a few more.  This 
approach is likely not super-efficient but it's where we're at.

\item Stores the input mantissa in the chip ``shared memory''.  For 8192 limbs, 
that's $8192 \cdot 4$ bytes $\cdot 2$ numbers to multiply $= 64$KB, and then 
stores $2 \cdot 16$KB extra for an intermediate Karatsuba result, for 96KB 
total.  This piece is negotiable and I could bring down/eliminate the shared 
memory requirement depending on how things progress.

\item One GPU thread per input limb, or two output limbs per thread.  It does a 
full 16384 limb output in this example and then truncates/shifts it.

\item The GPU floating point format has a separate integer exponent, which is 
overkill for Mandelbrot but I figured I'd keep it for now because it's not a 
performance problem.  It also keeps a sign separately.

\item The application I'm using to test this has a bunch of cases to verify 
that it's producing correct results.  It generates pseudo-random numbers with 
many \texttt{0xFFF...} limbs, zero-limbs, and related, to force carries/
borrows.  The test program compares all results against my own Karatsuba CPU-
based implementation I can use as a reference, and more importantly, the MPIR 
implementation (\texttt{mpf\_mul}) for correctness.

\item I've got MPIR to GPU and GPU to MPIR conversion capability, so it's easy 
to translate formats as needed.

\item The GPU implementation gets its best performance when it doesn't recurse, 
and instead switches straight to convolutions on the sub problems.  Recursing 
is still getting me slightly worse performance and I think I know why but 
haven't worked out how to fix it.

\end{itemize}

Here is some related work:

\begin{itemize}

\item A Study of High Performance Multiple Precision Arithmetic on Graphics 
Processing Units: Niall Emmart \cite{Emmart_GPU_MultiPrecision}

\item Missing a Trick: Karatsuba Variations: Michael Scott \cite{Scott_Karatsuba_MissingTrick}

\item MFFT: A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT 
Framework: \cite{MFFT_GPU_FFT}

\item Karatsuba Multiplication in GMP: \cite{GMP_Karatsuba}

\item Karatsuba Algorithm: \cite{Karatsuba_Wikipedia}

\item Toom--Cook Multiplication: \cite{ToomCook_Wikipedia}

\item Sch{\"o}nhage--Strassen Algorithm: \cite{SchonhageStrassen_Wikipedia}

\item CGBN: CUDA Accelerated Multiple Precision Arithmetic Using Cooperative 
Groups: \cite{CGBN_NVlabs}

\end{itemize}

All the code is here / GPL etc but it's just an experiment: \\
\url{https://github.com/mattsaccount364/FractalShark}

It'll all end up in \FractalShark{} eventually assuming I can get something end-to-
end working, and at this point I believe I can, but this is very slow-going 
yet.  Anyway, it's a fun area and I'll probably continuing dinking with it, so 
there probably won't be much new on \FractalShark{} proper until I can get this 
behemoth under better control.  And we'll see if it works out -- lots of 
remaining details to resolve and it may not work decently when combined into a 
full reference orbit.  Overall though I'm really happy with where it's at.  
Happy to answer questions etc.


\subsection{2025-07-28 News --- Version 0.46}

Version \texttt{0.46} was published as a pre-release, focusing on refactoring,
reference compression experimentation, save-file compatibility, and test infrastructure.

\begin{itemize}

  \item Continued internal \textbf{refactoring} of core rendering and control logic.
  
  \item Experimental support for \textbf{Imagina-style “max” reference compression},
        as an alternative to the existing “simple” scheme.
  
  \item Added preliminary support for \textbf{Imagina-compatible save files}.

  \item Enhanced internal \textbf{test infrastructure} with broader coverage and tooling.
    
  \item Work in progress toward additional infrastructure improvements.

\end{itemize}

This release was published on GitHub on July 28, 2025 (UTC) and is tagged as a
pre-release build.  


\subsection{2024-04-20 --- Version 0.45}

\begin{itemize}
  
  \item Adds a first cut at reference compression for an intermediate “perturbed
  perturbation” reference-orbit strategy.

  \item Includes three internal variants (v1: uncompressed/3 threads; v2:
  uncompressed/4 threads; v3: compressed/4 threads).

  \item v3 becomes the default at very deep “auto” perturbation settings;
  guidance given for when to stick to the older MT2 + periodicity workflow.

\end{itemize}


\subsection{2024-03-31 News --- Version 0.44}

Version \texttt{0.44} represents a substantial internal refactor emphasizing memory management, allocator control, and reference-orbit performance.

\begin{itemize}

  \item Removed the Boost dependency and introduced a custom
  \textbf{HighPrecision} wrapper.

  \item Improved reference-orbit performance, especially for low-precision,
  high-period locations.

  \item Improved performance of the \textbf{perturbed perturbation} algorithm.

  \item Introduced a custom file-backed \textbf{bump allocator} with stable
  interior pointers.

  \item Extended \texttt{GrowableVector} to support allocator-style usage
  without increasing committed memory.
  
  \item Updated reference orbit and linear approximation save-file formats.

  \item Added an \textbf{automatic perturbation mode} switching between single-threaded and multithreaded execution.

  \item Instrumented for memory-leak detection and fixed all known leaks.

  \item Fixed a long-standing correctness bug in BLAv1 dating back to version
  0.21.

  \item \textbf{Clarified ``perturbed perturbation'' design.} The implementation
  is framed as storing an intermediate-resolution reference orbit (e.g.\
  $\sim$500 bits), then using low-precision perturbation off that intermediate
  orbit to generate subsequent nearby reference orbits more efficiently.

\end{itemize}


\subsection{2024-03-03 News --- Version 0.43}
\begin{itemize}
  
  \item Significant performance wins for lower-precision, high-period
  reference-orbit computation.
  
  \item Adds a custom per-thread bump allocator for MPIR high-precision numbers.
  
  \item Rebalances work across the existing 3-thread multithreaded
  reference-orbit approach.

  \item \textbf{Allocator usage guidance.} The bump allocator is primarily
  beneficial for low-precision, high-period reference orbits; at very high
  precision the arithmetic dominates and allocation overhead is negligible. A
  suggested heuristic is to use the fixed-block bump allocator below roughly
  1000 digits and fall back to the global allocator above that.

  \item \textbf{Empirical sizing recommendation.} Tested successfully on a $\sim
  1\mathrm{e}{6000}$ location with about 4\,MB of bump-allocator space;
  recommendation is to use the smallest practical buffer (cache behavior matters).

  \item \textbf{Implementation refinement note.} In the current multithreaded
  orbit implementation, most allocations may occur on the main thread;
  thread-local allocation may be unnecessary. A few small allocator issues were
  identified for cleanup in the following release.

\end{itemize}


\subsection{2024-02-24 News --- Version 0.42}

Version \texttt{0.42} introduced major improvements to linear approximation performance, benchmarking, and configurability.

\begin{itemize}
  \item Added \textbf{multithreaded linear approximation table generation}.
  \item Dramatically improved performance for high-period locations, especially with reference compression enabled.
  \item Added fine-grained benchmarking and performance breakdowns.
  \item Enabled regeneration of linear approximation tables independently of per-pixel data.
  \item Introduced adjustable linear approximation presets trading memory, accuracy, and performance.
\end{itemize}

This release corrected a prior precision regression and restored high-accuracy defaults.


\subsection{2024-01-15 News --- Version 0.41}

\begin{itemize}
  \item Fixes a minor bug in reference compression / GPU.
  \item Memory-use overhaul for storing LA tables and reference orbit to minimize RAM needed for hard views (e.g., View \#27).
  \item Default mode now uses temporary files + memory-mapped IO; avoids needing 2$\times$ memory during resize/copy and reduces committed-memory requirements with no measurable overhead per the release note.
\end{itemize}


\subsection{2024-01-01 News --- Version 0.4}

\begin{itemize}
  \item First working implementation of \textbf{runtime} reference compression
  (decompress-on-demand during rendering).

  \item Motivated as reducing end-to-end RAM requirements dramatically on very
  high-period locations; not enabled by default for “Auto” rendering and
  described as work-in-progress.

  \item \textbf{Motivation: memory pressure.} Reference compression is
  explicitly framed as a response to high RAM requirements at difficult views
  (notably after enabling 64-bit iteration counts).
\end{itemize}


\subsection{2023-12 News --- Version 0.32}

Version \texttt{0.32} focused on usability, correctness, and incremental architectural cleanup.

\begin{itemize}
  \item Added \textbf{progressive rendering}, allowing partial image updates
  during long GPU renders.
  \item Added a \textbf{2x32 non-HDR linear approximation} path for numerically
  difficult shallow zooms.
  \item Fixed a subtle but severe correctness bug in the 2x32 implementation
  present in version 0.31.
  \item Added a basic regression test suite.
  \item Refactored code and restored buildability across inactive code paths.
\end{itemize}


\subsection{2023-12-09 News --- Version 0.31}

Version \texttt{0.31} expanded the set of available rendering algorithms and
substantially improved default performance at shallow and mid-depth zooms.

\begin{itemize}
  \item Added \textbf{1x32 linear approximation / perturbation} for
  high-performance shallow zooms.
  
  \item Added \textbf{1x64 linear approximation / perturbation}, primarily for
  correctness testing.
  
  \item Implemented \textbf{automatic render-algorithm selection} based on zoom
  depth.
  
  \item Eliminated unnecessary reference-orbit copying, significantly improving
  frame-to-frame performance.
  
  \item Fixed several bugs, including 2x32 correctness issues and unaligned
  HDRx64 memory access.

  \item \textbf{Auto-selection policy detail.} Auto mode is described as staging
  from direct 32-bit rendering at very shallow zooms, to 1x32 perturb-only, to
  1x32 LA, and finally 1x32 float-exp LA at deeper zooms, with the goal of
  improving default performance below roughly $10^{38}$.

  \item \textbf{Reference-orbit reuse optimization.} Removing unnecessary
  reference-orbit copying is called out as a major win for deep-zoom
  frame-to-frame responsiveness when reusing a reference orbit.

\end{itemize}


\subsection{2023-11-26 News --- Version 0.3}

\begin{itemize}
  
  \item Adds HDRx2x32 linear approximation: faster than LA/HDRx64 on consumer
  cards while retaining nearly native-64-bit precision.
  
  \item Notes that a “Debug View 20” anomaly is resolved under HDRx2x32,
  supporting the hypothesis that HDRx32 precision limits caused the issue.
  
  \item Misc performance improvements and bug fixes elsewhere.

  \item \textbf{Debug View 20 consistency improvement.} The ``Debug View 20''
  anomaly is reported as resolved under HDRx2x32, supporting the conclusion that
  the earlier HDRx32 behavior was a precision-limit issue.

  \item \textbf{HDRx2x32 precision characterization.} Described as ``Linear
  Approximation + Float EXP'' using a pair of 32-bit floats plus an exponent,
  with an estimated combined precision of roughly 46 bits.

\end{itemize}


\subsection{2023-11-12 News --- Version 0.24}

\begin{itemize}
\item \textbf{UI/UX improvements.} Adds a hotkey help view and general UI
cleanup.
\item \textbf{Random palette generation.} Adds a random palette generator; notes
the palette system remains intentionally constrained and may be expanded.
\item \textbf{Rendering diagnostics.} Adds a ``Show Rendering Details'' view for
inspecting current algorithm/settings.
\item \textbf{Early HDRx2x32 work.} Begins a GPU HDR 2x32 float implementation
(explicitly noted as not working yet).
\item \textbf{Crash diagnostics.} Automatically generates a crash dump on
unhandled exceptions.
\end{itemize}


\subsection{2023-11-05 News --- Version 0.23}
\begin{itemize}
  \item Adds switchable 64-bit iteration count support (enabling tens of
  billions of iterations per pixel, leveraging LA).
  \item Moves antialiasing/coloring into its own CUDA kernel for better
  performance.
  \item Multiple memory-reduction optimizations (systems-level) and related perf
  work.
  \item Adds reference-orbit save/load (no compression yet), useful for
  debugging deep spots.
  \item Notes a known bug manifesting in built-in View \#20 under a particular
  configuration (64-bit iterations + default HDRx32 LAv2), with long render
  times due to slow reference-orbit calculation.
\end{itemize}


\subsection{2023-10-14 News --- Version 0.22}

\begin{itemize}

  \item Significant LAv2 performance improvements (bug fixes); notes BLAv1 still
  winning in some cases.

  \item Fixes an LAv2 correctness bug; release note claims no known correctness
  bugs remaining.

  \item \textbf{Performance data point.} Reports a minibrot example improving
  from roughly 470\,ms to 370\,ms (render-only; reference orbit excluded) on the
  author’s machine.

\end{itemize}


\subsection{2023-10-08 News --- Version 0.21}
\begin{itemize}
  \item Performance improvements to BLA and LAv2.
  \item Memory-usage improvements (commit size / virtual address space issues
  called out).
  \item Generates more CUDA kernels for older GPUs (system requirements
  unchanged).
  \item Notes LAv2 is strong for many deep-zoom cases but still behind older BLA
  in some shallower cases.
\end{itemize}


\subsection{2023-09-16 News --- Version 0.2}

Version \texttt{0.2} introduced a major algorithmic milestone: the integration
of Imagina's linear approximation implementation as a CUDA kernel.

\begin{itemize}
  \item Added Imagina-based linear approximation (\texttt{LAv2}), now the
  default rendering algorithm.
  \item Resolved known correctness bugs in the new approximation path.
  \item Fixed a major performance regression caused by an earlier implementation oversight.
  \item Achieved large performance gains at deeper zoom levels compared to the
  existing bilinear approximation.
\end{itemize}

While the legacy bilinear approximation remains faster at very shallow zooms,
the new approach dominates once meaningful magnification is reached.


\subsection{2023-07-22 News --- Version 0.11}

Version \texttt{0.11} was released with a focus on performance improvements and
improved diagnostics.

\begin{itemize}
  \item Approximately \textbf{+10\% performance improvement} in the HDRx32 CUDA
  rendering path.
  \item Fixed several minor bugs affecting correctness and stability.
  \item Improved CUDA error reporting by displaying descriptive error strings
  instead of numeric codes.
\end{itemize}

A long-duration technology demonstration video accompanied this release,
showcasing a zoom to approximately $10^{4000}$ using the default HDRx32/BLA CUDA
kernel.  Rendering and post-processing each required roughly one and a half
days.

\bibliographystyle{plain} % or ieeetr, unsrt, alpha, etc.
\bibliography{FractalShark}      % <-- filename WITHOUT .bib

\end{document}
