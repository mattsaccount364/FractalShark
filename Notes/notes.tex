\documentclass[12pt]{article}

% --- Encoding / fonts (fixes OT1 issues + improves monospace metrics) ---
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Math ---
\usepackage{amsmath}
\usepackage{amssymb}

% --- Layout ---
\usepackage{geometry}
\geometry{a4paper, margin=1in}

% --- Better line breaking / fewer overfull boxes ---
\usepackage{microtype}
\emergencystretch=2em % last-resort "be less picky" knob for line breaks

% --- Links and cross-refs ---
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}

% --- Convenience for inline code that may need breaks ---
\newcommand{\code}[1]{\texttt{#1}}

\title{FractalShark: High-Performance Mandelbrot Rendering}
\author{Matthew Renzelmann}
\date{}

\begin{document}
\maketitle

\tableofcontents
\clearpage

\section{Goal and overview}
\label{sec:goal}

The goal of these kernels is to \emph{render the Mandelbrot set} by computing an
\emph{escape-time} iteration count per pixel over a 2D image grid. Each CUDA
thread evaluates a single complex parameter \(c\) corresponding to one pixel,
iterates the Mandelbrot recurrence, and stores the iteration count into an
output buffer. Separate (or fused) stages can map iteration counts to colors,
apply palettes, and perform anti-aliasing.

Across kernels, the primary variation is the numeric representation used for
the orbit arithmetic: from IEEE-754 \code{float}/\code{double} up through
expansion types (float-float, double-double, quad-float, quad-double) and
HDR-normalized formats. The shared objective remains the same: compute the
escape-time for the Mandelbrot iteration as accurately and quickly as needed
for a desired zoom depth.

\section{Code Overview}

\begin{itemize}

\item All the CUDA kernels are in \texttt{render\_gpu.cu}. The two most 
interesting are probably \texttt{mandel\_1xHDR\_float\_perturb\_bla} and \texttt
{mandel\_1xHDR\_float\_perturb\_lav2}, which are the ones I've spent the most 
time on lately. For better performance at low zoom levels, you could look at 
\texttt{mandel\_1x\_float\_perturb}, which leaves out linear approximation and 
just does straight perturbation up to $\sim 10^{30}$, which corresponds with 
the 32-bit float exponent range.

\item One fun thing you can try is running with LAv2 + ``LA only''. This 
approach actually works pretty well once Linear Approximation kicks in at 
deeper zooms --- it gives you an idea of what the actual image should look like 
but is very fast, since it does no perturbation. The images it produces are not 
precise, and often leave out the fine detail; however, it's fun to play with 
when zooming in on a specific point.

\item The \texttt{mandel\_1x\_float} is the classic 32-bit float Mandelbrot and 
is screaming fast on a GPU. This one is optimized with fused multiply-add for 
fun even though it's kind of useless because you can barely zoom in before you 
get pixellation.

\item The most interesting reference orbit calculation is at \texttt{
AddPerturbationReferencePointMT3}. It includes a ``bad'' calculation which is 
used for the ``scaled'' CUDA kernels. The multithreaded approach handily beats 
the single-threaded implementation on my CPU in all scenarios.

\item There are CPU renderers, but they were mostly to learn/debug more easily, 
and aren't optimized heavily. They're much easier to understand and reason 
about though.

\end{itemize}

\section{The Mandelbrot set and escape-time rendering}
\label{sec:mandelbrot-intro}

The Mandelbrot set \(M\subset\mathbb{C}\) is defined as the set of complex
parameters \(c\) for which the orbit of
\begin{equation}
z_{n+1} = z_n^2 + c,\qquad z_0 = 0
\end{equation}
remains bounded. A common rendering method is \emph{escape-time}: iterate until
the orbit magnitude exceeds a bailout threshold, or until a maximum iteration
count is reached.

A standard bailout uses \(|z_n|^2 \ge 4\), since \(|z|>2\) implies divergence:
\begin{equation}
|z_n|^2 = \Re(z_n)^2 + \Im(z_n)^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}
For each pixel, the stored value is the smallest \(n\) (or a bounded proxy) at
which escape occurs, or the maximum iteration limit if escape never occurs.


\section{Pixel-to-parameter mapping}
\label{sec:mapping}

Each CUDA thread computes a pixel coordinate \((X,Y)\) and maps it to a complex
parameter \(c = x_0 + i y_0\). A typical kernel starts with:
\begin{verbatim}
int X = blockIdx.x * blockDim.x + threadIdx.x;
int Y = blockIdx.y * blockDim.y + threadIdx.y;
if (X >= width || Y >= height) return;
size_t idx = ConvertLocToIndex(X, height - Y - 1, width);
\end{verbatim}

\subsection{Thread-to-pixel mapping}
A 2D CUDA grid of 2D blocks covers the image. Each thread is responsible for one
pixel. The bounds check prevents out-of-range threads from writing.

\subsection{Y-axis convention}
The expression \code{height - Y - 1} flips \(Y\). This is common when the image
buffer uses a top-left origin but the complex-plane mapping assumes a
bottom-left origin (or vice versa).

\subsection{Affine map into the complex plane}
The parameter \(c\) is computed by an affine transform:
\begin{align}
x_0 &= cx + dx \cdot X, \\
y_0 &= cy + dy \cdot Y,
\end{align}
where \code{cx,cy} anchor the plane (e.g., the coordinate at pixel \((0,0)\)),
and \code{dx,dy} are per-pixel increments. Different kernels compute these
expressions in different numeric types; the intent is always the same: map each
pixel to its associated complex parameter \(c\).


\section{Mandelbrot recurrence in real arithmetic}
\label{sec:real-form}

Writing \(z = x + i y\) and \(c = x_0 + i y_0\), the iteration becomes:
\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + x_0, \\
y_{n+1} &= 2 x_n y_n + y_0.
\end{align}
The escape test is:
\begin{equation}
x_n^2 + y_n^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}

Many kernels cache squares:
\[
zrsqr = x^2,\quad zisqr = y^2,
\]
so that \(x^2-y^2\) and \(x^2+y^2\) can be formed cheaply as \code{zrsqr - zisqr}
and \code{zrsqr + zisqr}. This is especially valuable when \(x\) and \(y\) are
represented by multi-component expansion types.


\section{Iteration chunking via \code{iteration\_precision}}
\label{sec:chunking}

Some kernels are templated on an integer \code{iteration\_precision}
(\(1,2,4,8,16\)) and unroll multiple Mandelbrot steps inside the loop:
\begin{itemize}
  \item Each loop iteration performs \code{iteration\_precision} updates.
  \item The counter \code{iter} increases by that amount.
  \item The maximum iteration \code{n\_iterations} is adjusted so \code{iter}
        does not exceed the requested limit.
\end{itemize}

This reduces loop overhead. The trade-off is that the escape predicate is
typically checked only once per chunk; therefore the reported escape iteration
can be larger than the true first-escape iteration by up to
\code{iteration\_precision - 1}. For strict escape iteration counts (e.g., for
continuous/smooth coloring based on the first bailout), use a chunk size of 1
or insert bailout checks within the unrolled body.


\section{Kernel family: numeric precision ladder for Mandelbrot rendering}
\label{sec:precision-ladder}

Each kernel variant renders the same Mandelbrot escape-time field; the only
difference is the numeric type used for mapping and orbit iteration. The
following sections describe how each type realizes the same recurrence and
escape test.

Throughout, \code{IterType} is the integer type used to store the escape-time
iteration count (e.g., \code{uint16\_t} or \code{uint32\_t}). An output color
buffer may appear in signatures (e.g., \code{AntialiasedColors OutputColorMatrix})
even when not written in the shown kernel body; the intent is to support a
coloring stage using the computed iteration counts.

\subsection{Kernel: \code{mandel\_1x\_float}}
\label{sec:mandel-1x-float}

\subsubsection{Numeric type}
This variant uses IEEE-754 single precision \code{float}. It provides the
highest throughput but limits usable zoom depth due to rounding error and loss
of significance in \(c\) and the orbit.

\subsubsection{FMA-based orbit update}
A typical implementation uses fused multiply-add intrinsics:
\begin{verbatim}
ytemp = __fmaf_rd(-y, y, x0);     // x0 - y^2
xtemp = __fmaf_rd(x, x, ytemp);   // x^2 - y^2 + x0
xtemp2 = 2.0f * x;
y = __fmaf_rd(xtemp2, y, y0);     // 2xy + y0
x = xtemp;
\end{verbatim}
This corresponds exactly to:
\begin{align}
x &\leftarrow x^2 - y^2 + x_0,\\
y &\leftarrow 2xy + y_0.
\end{align}
Using FMA reduces intermediate rounding and can improve performance. The
\code{\_\_fmaf\_rd} variant rounds downward; if IEEE round-to-nearest is desired,
use \code{\_\_fmaf\_rn} (or plain \code{fmaf}).

\subsubsection{Escape test}
The kernel tests \(x^2+y^2 < 4\) (often recomputing squares each loop in this
simplest variant).

\subsection{Kernel: \code{mandel\_1x\_double}}
\label{sec:mandel-1x-double}

\subsubsection{Numeric type}
This variant uses IEEE-754 \code{double} and therefore carries substantially
more mantissa precision than float. The Mandelbrot recurrence is identical, but
performance depends strongly on GPU FP64 throughput.

\subsubsection{Orbit update and escape test}
The update uses double-precision FMA intrinsics (e.g., \code{\_\_fma\_rd}) or
equivalent arithmetic to compute:
\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]
with the same bailout condition \(|z|^2 \ge 4\).

\subsection{Kernel: \code{mandel\_2x\_float}}
\label{sec:mandel-2x-float}

\subsubsection{Numeric type: float-float expansion}
This variant uses a float-float expansion type \code{dblflt} to represent a
value as:
\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]
where \code{head} (hi) carries the leading magnitude and \code{tail} (lo) is a
correction term. Arithmetic uses compensated routines such as
\code{add\_dblflt}, \code{sub\_dblflt}, \code{mul\_dblflt}, \code{sqr\_dblflt},
and often a specialized \code{mul\_dblflt2x(x,y)} to compute \(2xy\) with good
accuracy.

\subsubsection{Mapping and orbit iteration}
The affine mapping for \(c\) is performed in \code{dblflt}:
\[
x_0 = cx + dx \cdot X,\qquad y_0 = cy + dy \cdot Y,
\]
and the orbit update follows the same real-form recurrence using expansion
operations. Cached squares are typically maintained as \code{dblflt}:
\[
zrsqr = x^2,\quad zisqr = y^2.
\]

\subsubsection{Escape test}
Some implementations compare only the leading component (e.g., \code{head}) for
speed:
\[
zrsqr.\mathrm{head} + zisqr.\mathrm{head} < 4.
\]
This is fast but can misclassify points extremely near the boundary. A fully
robust bailout can incorporate both components (or a conservative bound).

\subsection{Kernel: \code{mandel\_2x\_double}}
\label{sec:mandel-2x-double}

\subsubsection{Numeric type: double-double expansion}
This variant uses a double-double type \code{dbldbl} representing:
\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]
with both components in double precision (often yielding \(\sim\)106 bits of
precision in favorable cases). This enables deeper zoom while retaining a
structure similar to the float-float kernel.

\subsubsection{Mapping, orbit update, and escape test}
The kernel computes the same affine mapping and iterates the same recurrence,
but with double-double arithmetic. The escape predicate may use the leading
component for speed, depending on the library layout and conventions in use.
If the implementation compares against a single component field (e.g., \code{.x}
or \code{.y}), verify that this field corresponds to the high part for that
library.

\subsection{Kernel: \code{mandel\_4x\_float}}
\label{sec:mandel-4x-float}

\subsubsection{Numeric type: quad-float (4-term expansion)}
This variant uses a four-float expansion type \code{GQF::gqf\_real}:
\[
a \approx a_0 + a_1 + a_2 + a_3,
\]
with decreasing-magnitude components. Pixel coordinates and constants are lifted
into this type (e.g., \code{make\_qf(X,0,0,0)}), then the affine mapping and orbit
update are performed in quad-float arithmetic.

\subsubsection{Orbit update and escape test}
The update implements:
\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]
using quad-float operations (including specialized square and power-of-two
multiply helpers). The escape test can be performed in the full quad-float type:
\[
zrsqr + zisqr \le 4.
\]
This yields a numerically faithful bailout for extreme zoom rendering.

\subsection{Kernel: \code{mandel\_4x\_double}}
\label{sec:mandel-4x-double}

\subsubsection{Numeric type: quad-double (4-term expansion)}
This variant uses a four-double expansion type \code{GQD::gqd\_real}:
\[
a \approx a_0 + a_1 + a_2 + a_3,\qquad a_k\in\mathbb{R}_{double}.
\]
It supports very deep zoom rendering with high numerical stability.

\subsubsection{Mapping, orbit update, and escape test}
The affine mapping and orbit update are evaluated in quad-double arithmetic. A
literal bailout constant (e.g., \code{4.0}) is promoted via overloads, so the
escape compare remains a full-precision comparison in the quad-double domain.

\subsection{Kernel: \code{mandel\_hdr\_float}}
\label{sec:mandel-hdr-float}

\subsubsection{Numeric type: HDR-normalized expansion}
This variant uses an HDR wrapper around an expansion type, e.g.:
\[
\code{HDRFloat<CudaDblflt<dblflt>>}.
\]
The intent is to combine (i) a wider mantissa via expansion arithmetic with
(ii) explicit normalization/reduction to maintain numeric conditioning and
dynamic range across many iterations.

\subsubsection{Reduction and stable comparisons}
The kernel frequently calls \code{Reduce()} / \code{HdrReduce()} on intermediate
values. These reductions are part of the numeric contract: norms and
comparisons are assumed to be applied to reduced/normalized values, enabling
specialized comparators without repeatedly materializing primitive scalars.

Instead of testing \(x^2+y^2 < 4\) in primitive form, the kernel maintains:
\[
zsq\_sum = zrsqr + zisqr
\]
and checks escape via a reduced comparator:
\begin{verbatim}
while (zsq_sum.compareToBothPositiveReduced(Four) < 0)
\end{verbatim}
This directly supports escape-time Mandelbrot rendering in HDR arithmetic while
keeping comparisons meaningful and stable.


\section{Perturbation Rendering of the Mandelbrot Set}
\label{sec:perturbation-concept}

This section explains \emph{perturbation} as a mathematical and algorithmic
technique for rendering the Mandelbrot set efficiently at deep zoom. The goal
is to compute accurate escape-time values for many nearby parameters \(c\)
while avoiding repeated high-precision evaluation of the full Mandelbrot
recurrence.

\subsection{Motivation: why perturbation is needed}

At large zoom depths, direct evaluation of
\[
z_{n+1} = z_n^2 + c,\qquad z_0=0,
\]
in floating-point arithmetic becomes inaccurate due to loss of significance in
\(c\) and accumulated rounding error. Using extended or arbitrary precision
solves the accuracy problem but is expensive when performed independently for
every pixel.

However, in a typical Mandelbrot image, nearby pixels correspond to parameters
\(c\) that differ only slightly. Their orbits therefore remain close for many
iterations. Perturbation exploits this coherence by computing one
high-precision \emph{reference orbit} and expressing nearby orbits as small
deviations from it.

\subsection{Reference orbit}

Choose a reference parameter \(c_\star\), usually the center of the current
view, and compute its orbit in sufficiently high precision:
\begin{equation}
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star = 0.
\end{equation}
The sequence \(\{z_n^\star\}\) is stored and reused for many pixels. This is the
only orbit that requires full high-precision evaluation.

\subsection{Delta formulation for nearby pixels}
\label{sec:perturb-delta-real}

For a pixel parameter \(c\) close to the reference parameter \(c_\star\), define
the parameter delta:
\begin{equation}
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star,
\end{equation}
and express the orbit at iteration \(n\) as a perturbation of the reference
orbit:
\begin{equation}
z_n = z_n^\star + \Delta z_n,
\end{equation}
where \(\Delta z_n\) is the \emph{orbit delta}. Substituting into the Mandelbrot
recurrence,
\[
z_{n+1} = z_n^2 + c,
\]
gives:
\begin{align}
z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) \\
&= (z_n^\star)^2 + 2z_n^\star \Delta z_n + (\Delta z_n)^2 + c_\star + \Delta c.
\end{align}
Subtracting the reference recurrence
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star
\]
yields the \emph{exact delta update}:
\begin{equation}
\Delta z_{n+1} = 2z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturb-exact}
\end{equation}
No approximation has been made: perturbation preserves the exact Mandelbrot
dynamics as long as the reference orbit is computed accurately.

\subsubsection{Expansion into real and imaginary components}

To map \cref{eq:perturb-exact} directly to an implementation, write all
quantities in real and imaginary parts:
\begin{align*}
z_n^\star &= x_n^\star + i y_n^\star, \\
\Delta z_n &= \Delta x_n + i \Delta y_n, \\
\Delta c &= \Delta c_x + i \Delta c_y.
\end{align*}
First expand the quadratic term:
\begin{align}
(\Delta z_n)^2
&= (\Delta x_n + i \Delta y_n)^2 \\
&= (\Delta x_n^2 - \Delta y_n^2)
   + i(2 \Delta x_n \Delta y_n).
\end{align}
Next expand the linear term:
\begin{align}
2 z_n^\star \Delta z_n
&= 2(x_n^\star + i y_n^\star)(\Delta x_n + i \Delta y_n) \\
&= 2(x_n^\star \Delta x_n - y_n^\star \Delta y_n)
 + i\,2(x_n^\star \Delta y_n + y_n^\star \Delta x_n).
\end{align}
Combining terms gives the real and imaginary delta updates:
\begin{align}
\Delta x_{n+1}
&= 2(x_n^\star \Delta x_n - y_n^\star \Delta y_n)
   + (\Delta x_n^2 - \Delta y_n^2)
   + \Delta c_x, \label{eq:perturb-real} \\
\Delta y_{n+1}
&= 2(x_n^\star \Delta y_n + y_n^\star \Delta x_n)
   + 2\Delta x_n \Delta y_n
   + \Delta c_y. \label{eq:perturb-imag}
\end{align}

\subsubsection{Factored form used in implementations}

For numerical efficiency, \cref{eq:perturb-real,eq:perturb-imag} are typically
evaluated in a factored form. Grouping terms yields:
\begin{align}
\Delta x_{n+1}
&= \Delta x_n\,(2x_n^\star + \Delta x_n)
   - \Delta y_n\,(2y_n^\star + \Delta y_n)
   + \Delta c_x, \label{eq:perturb-real-factored} \\
\Delta y_{n+1}
&= \Delta x_n\,(2y_n^\star + \Delta y_n)
   + \Delta y_n\,(2x_n^\star + \Delta x_n)
   + \Delta c_y. \label{eq:perturb-imag-factored}
\end{align}
This corresponds exactly to the compact complex form:
\[
\Delta z_{n+1}
= \Delta z_n \bigl(2 z_n^\star + \Delta z_n\bigr) + \Delta c,
\]
and maps directly onto typical GPU code using temporaries such as:
\[
(2x_n^\star + \Delta x_n), \quad (2y_n^\star + \Delta y_n).
\]

\subsubsection{Reconstruction for escape testing}

Although perturbation evolves only the delta, escape-time rendering requires the
absolute orbit value:
\begin{equation}
z_n = z_n^\star + \Delta z_n
     = (x_n^\star + \Delta x_n) + i(y_n^\star + \Delta y_n).
\end{equation}
The standard Mandelbrot bailout condition is then applied:
\begin{equation}
|z_n|^2
= (x_n^\star + \Delta x_n)^2 + (y_n^\star + \Delta y_n)^2
\ge 4.
\end{equation}
This separation cleanly explains how perturbation math maps onto real-valued
implementation variables while preserving the exact Mandelbrot dynamics.


\subsection{Efficient perturbation update}

Equation~\eqref{eq:perturb-exact} can be rearranged into a form well suited for
implementation:
\begin{equation}
\Delta z_{n+1} = \Delta z_n \bigl(2z_n^\star + \Delta z_n\bigr) + \Delta c.
\label{eq:perturb-factored}
\end{equation}
This form requires only:
\begin{itemize}
\item one complex multiply,
\item one complex add,
\item access to the stored reference sample \(z_n^\star\).
\end{itemize}
Crucially, it avoids squaring large high-precision numbers for every pixel.
Instead, the expensive squaring is performed once for the reference orbit and
reused implicitly through \(z_n^\star\).

\subsection{Reconstructing the absolute orbit and escape test}

Perturbation evolves \(\Delta z_n\), but Mandelbrot rendering requires testing
escape of the absolute orbit. At each iteration, the current iterate is
reconstructed as:
\begin{equation}
z_n \approx z_n^\star + \Delta z_n.
\end{equation}
Escape-time rendering then applies the standard bailout condition:
\begin{equation}
|z_n|^2 = \Re(z_n)^2 + \Im(z_n)^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}
The iteration index \(n\) at which this condition is first satisfied (or the
maximum iteration count if it never is) determines the pixelâ€™s color.

\subsection{Numerical stability and recentering}

Perturbation is efficient only while \(\Delta z_n\) remains small relative to
\(z_n^\star\). If \(|\Delta z_n|\) grows comparable to or larger than
\(|z_n^\star|\), numerical cancellation and loss of significance can degrade
accuracy.

To maintain stability, practical implementations use \emph{recentering}. When
a stability criterion is violated, the current approximation is folded into a
new base:
\begin{equation}
z_n^\star \leftarrow z_n^\star + \Delta z_n,\qquad \Delta z_n \leftarrow 0,
\end{equation}
and perturbation continues relative to this updated reference state. This
preserves correctness while keeping deltas small.

\subsection{Why perturbation accelerates Mandelbrot rendering}

The efficiency gains come from two sources:
\begin{itemize}
\item \textbf{Reduced arithmetic cost.} Only one orbit is computed in full
high precision. All other pixels use cheaper delta updates.
\item \textbf{Spatial coherence.} Nearby pixels share the same reference orbit
for many iterations before diverging.
\end{itemize}
As a result, perturbation enables deep-zoom Mandelbrot rendering that would be
prohibitively slow if each pixel were evaluated independently in arbitrary
precision.

\subsection{Relation to other acceleration techniques}

Perturbation alone preserves the exact dynamics of the Mandelbrot map and
requires no linearization. It can be used by itself or combined with further
approximations (such as linear or bilinear approximation steps) to skip multiple
iterations at once. In all cases, perturbation provides the mathematical
foundation that makes efficient deep-zoom Mandelbrot rendering feasible.


\section{Reference Orbit Calculation}
\label{sec:ref-orbit-calc}

This section describes how \texttt{RefOrbitCalc} constructs (and optionally 
reuses) a high-precision \emph{reference orbit} for perturbation rendering of 
the quadratic map

\begin{equation}
  z_{n+1} = z_n^2 + c,\qquad z,c\in\mathbb{C},
\end{equation}

with an implementation that supports single-threaded CPU, multi-threaded CPU, 
and GPU backends, plus several storage/compression modes that trade memory footprint against recomputation.

\subsection{High-level pipeline and dispatch}
\label{subsec:ref-orbit-pipeline}

A reference orbit is stored in a \texttt{PerturbationResults<IterType,T,PExtras>}
instance, where:

\begin{itemize}
  \item \texttt{IterType} is the iteration index type (\texttt{uint32\_t} or \texttt{uint64\_t}).
  \item \texttt{T} is the low-precision numeric type used for downstream perturbation math (e.g.\ \texttt{float},
        \texttt{double}, \texttt{HDRFloat<...>}).
  \item \texttt{PExtras} selects the storage format for per-iteration orbit data:
        uncompressed (\texttt{Disable}), a lightweight compressor (\texttt{SimpleCompression}),
        or additional diagnostic bookkeeping (e.g.\ \texttt{Bad}).
\end{itemize}

Orbit construction is initiated through \texttt{AddPerturbationReferencePoint()}, which:
\begin{enumerate}
  \item Picks an initial guess \((c_x,c_y)\) (center of the current view if unset).
  \item Chooses an algorithm (\texttt{ST}, \texttt{MT}, reuse-based hybrids, or \texttt{GPU}) based on
        \texttt{m\_PerturbationAlg} and zoom factor heuristics.
  \item Allocates a new \texttt{PerturbationResults} slot, initializes metadata and bounds, and runs the chosen
        orbit kernel until escape, periodicity detection, or the maximum iteration count is reached.
\end{enumerate}

To control memory pressure, \texttt{OptimizeMemory()} monitors process commit 
usage and opportunistically drops cached orbits that are not of the currently 
demanded variant type when the working set exceeds a configurable threshold.

\subsection{Single-threaded authoritative orbit}
\label{subsec:ref-orbit-st}

The single-threaded path (\texttt{AddPerturbationReferencePointST}) computes 
the authoritative orbit directly using MPIR/GMP \texttt{mpf\_t} for the 
recurrence, while simultaneously emitting a low-precision shadow copy \((\hat{x}
_n,\hat{y}_n)\in T^2\) for downstream work (compression, bailout checks, 
periodicity tests).

\paragraph{State and initialization.}
Given a selected reference parameter \(c = c_x + i c_y\), the implementation:
\begin{itemize}
  \item Initializes \texttt{mpf\_t} temporaries for \(x,y,x^2\), and scratch products.
  \item Sets the initial iterate \((x_0,y_0) = (c_x,c_y)\) (this code uses the common convention \(z_0=c\)).
  \item Computes low-precision cast values \(\hat{c}_x,\hat{c}_y \in T\) either via \texttt{mpf\_get\_d} for
        native float/double, or via a mantissa/exponent extraction for extended formats.
\end{itemize}

\paragraph{Recurrence.}
Writing \(z_n = x_n + i y_n\), one iteration evaluates
\begin{align}
  x_{n+1} &= x_n^2 - y_n^2 + c_x, \\
  y_{n+1} &= 2 x_n y_n + c_y.
\end{align}
The implementation uses:
\begin{itemize}
  \item \texttt{mpf\_mul} and \texttt{mpf\_sub}/\texttt{mpf\_add} for the high-precision update.
  \item A low-precision snapshot \((\hat{x}_n,\hat{y}_n)\) acquired once per iteration for storage/compression,
        periodicity heuristics, and bailout checks.
\end{itemize}

\paragraph{Bailout.}
The bailout threshold is evaluated in low precision using
\begin{equation}
  \|\hat{z}_n + \hat{c}\|^2 = (\hat{x}_n + \hat{c}_x)^2 + (\hat{y}_n + \hat{c}_y)^2 > 256,
\end{equation}
which matches the code's use of \texttt{TwoFiftySix} and avoids a high-precision norm each step.

\subsection{Periodicity tracking via \texorpdfstring{$\partial z/\partial c$}{dz/dc}}
\label{subsec:ref-orbit-periodicity}

Several modes enable periodicity detection. The implementation tracks the complex derivative
\(\frac{\partial z_n}{\partial c}\) in low precision:
\begin{equation}
  d_{n+1} = 2 z_n d_n + 1,\qquad d_0 = 1,
\end{equation}
with \(d_n = d_{x,n} + i d_{y,n}\). Expanding into real and imaginary parts yields:
\begin{align}
  d_{x,n+1} &= 2(x_n d_{x,n} - y_n d_{y,n}) + 1, \\
  d_{y,n+1} &= 2(x_n d_{y,n} + y_n d_{x,n}).
\end{align}

The code applies a radius-based heuristic: let
\begin{equation}
  n_2 = \max(|\hat{x}_n|,|\hat{y}_n|),\qquad
  r_0 = \max(|d_{x,n}|,|d_{y,n}|),
\end{equation}
and define a detection threshold
\begin{equation}
  n_3 = 2\,R_{\max}\,r_0,
\end{equation}

where \(R_{\max}\) is the maximum perturbation radius stored in \texttt{results}
. If \(n_2 < n_3\), the orbit is marked as \emph{maybe periodic} and the 
reference loop terminates early (unless benchmarking mode disables the break). 
Otherwise, \((d_{x,n},d_{y,n})\) is advanced using the update above.

\subsection{Compression and reuse datasets}
\label{subsec:ref-orbit-compression-reuse}

Two orthogonal storage decisions are made while iterating:
\begin{enumerate}
  \item \textbf{Orbit storage} for perturbation use (\texttt{PExtras}):
  \begin{itemize}
    \item \texttt{Disable}: store every \((\hat{x}_n,\hat{y}_n)\) uncompressed.
    \item \texttt{SimpleCompression}: store a compressed subset of iterations using an error exponent
          determined by \texttt{Fractal::CompressionError}.
    \item \texttt{Bad}: store orbit values plus underflow/diagnostic flags.
  \end{itemize}
  \item \textbf{Reuse storage} for intermediate-precision regeneration (\texttt{ReuseMode}):
  \begin{itemize}
    \item \texttt{SaveForReuse1/2}: store uncompressed \texttt{mpf\_t} reuse entries.
    \item \texttt{SaveForReuse3}: store an intermediate-compressed reuse stream.
    \item \texttt{SaveForReuse4}: store a maximally-compressed intermediate reuse stream.
  \end{itemize}
\end{enumerate}

The authoritative orbit and the reuse stream are logically decoupled: the first 
feeds perturbation evaluation, while the second provides an anchor sequence to 
reconstruct nearby orbits without recomputing the full authoritative \texttt{mpf
\_t} recurrence.

\subsection{Reuse-based orbit regeneration}
\label{subsec:ref-orbit-reuse}

At extreme zoom, a previously computed authoritative orbit may be reused if it remains valid for the
current view. Reuse is gated by two checks:
\begin{enumerate}
  \item \textbf{Precision headroom:} the estimated precision increase demanded by the new view must be below a
        configured extra-precision margin (otherwise, authoritative recomputation is required).
  \item \textbf{Spatial locality:} the parameter delta \(\Delta c = c - c^\star\) must lie within the
        authoritative orbit's validity radius, i.e.\ \(|\Delta x|\le R_{\max}\) and \(|\Delta y|\le R_{\max}\).
\end{enumerate}

\paragraph{Delta-subiteration recurrence.}
Let \(z_n^\star\) be the stored authoritative orbit for \(c^\star\), and let \(z_n\) be the desired orbit for
\(c = c^\star + \Delta c\). Define \(\Delta z_n = z_n - z_n^\star\). For the quadratic map,
\begin{equation}
  z_{n+1}^\star = (z_n^\star)^2 + c^\star,\qquad
  z_{n+1} = (z_n^\star + \Delta z_n)^2 + (c^\star + \Delta c).
\end{equation}
Subtracting gives the exact delta recurrence
\begin{equation}
  \Delta z_{n+1} = 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{equation}

The implementation stores \(\Delta c\) as \((\Delta_0^x,\Delta_0^y)\) and advances
\((\Delta_n^x,\Delta_n^y)\) in \texttt{mpf\_t}, using reuse entries for \(z_n^\star\). Writing
\(z_n^\star = x_n^\star + i y_n^\star\) and \(\Delta z_n = \Delta_n^x + i \Delta_n^y\), the real/imaginary
updates are:
\begin{align}
  \Delta_{n+1}^x &=
    \Delta_n^x\,(2x_n^\star + \Delta_n^x) - \Delta_n^y\,(2y_n^\star + \Delta_n^y) + \Delta_0^x, \\
  \Delta_{n+1}^y &=
    \Delta_n^x\,(2y_n^\star + \Delta_n^y) + \Delta_n^y\,(2x_n^\star + \Delta_n^x) + \Delta_0^y.
\end{align}
Finally, the reconstructed iterate is obtained by
\begin{equation}
  x_n = x_n^\star + \Delta_n^x,\qquad y_n = y_n^\star + \Delta_n^y.
\end{equation}

\paragraph{Reference iteration reset.}
To reduce accumulated error when walking a compressed reuse stream, the code periodically resets the reuse
index back to zero when the delta magnitude becomes comparable to or larger than the current orbit magnitude.
Operationally, it compares low-precision norms
\begin{equation}
  \|\hat{z}_n\|^2 \quad \text{vs.} \quad \|\widehat{\Delta z_n}\|^2,
\end{equation}
and if \(\|\hat{z}_n\|^2 < \|\widehat{\Delta z_n}\|^2\) (or the reuse index reaches the end), it sets
\(\Delta z_n \leftarrow z_n\) and restarts from the beginning of the reuse stream.

\subsection{Multi-threaded CPU acceleration}
\label{subsec:ref-orbit-mt}

The \texttt{MT3} path parallelizes high-cost MPIR operations by splitting the per-iteration work across
threads. Two recurring patterns appear:

\paragraph{Asynchronous squaring.}
For authoritative orbit computation, two worker threads compute \(x_n^2\) and \(y_n^2\) concurrently while the
main thread evaluates the cross term for \(y_{n+1} = 2 x_n y_n + c_y\), performs periodicity checks in low
precision, and orchestrates reuse/serialization. The main update then becomes:
\begin{equation}
  x_{n+1} = x_n^2 - y_n^2 + c_x,
\end{equation}
where \(x_n^2\) and \(y_n^2\) are returned from the worker threads.

\paragraph{Lock-free handoff with prefetch.}
Threads communicate through a minimal \texttt{ThreadPtrs<T>} mailbox containing atomic \texttt{In} and \texttt{Out}
pointers. The protocol is:
\begin{enumerate}
  \item Producer stores a work pointer in \texttt{In}.
  \item Worker spins until it swaps \texttt{In} to \texttt{nullptr}, prefetches the pointed-to operands, executes MPIR
        arithmetic, then publishes the same pointer in \texttt{Out}.
  \item Producer spins until it swaps \texttt{Out} back to \texttt{nullptr}, then consumes the computed fields.
\end{enumerate}
To mitigate cache miss latency on large MPIR limb arrays, the worker explicitly prefetches both MPIR headers
and limb ranges (64-byte stride), which is particularly helpful when iterating at very high precision.

\subsection{GPU reference orbit backend}
\label{subsec:ref-orbit-gpu}

The GPU backend delegates authoritative reference orbit generation to \texttt{HpShark} kernels specialized by
precision. The key design is a persistent \emph{combo} object returned by initialization:
\begin{enumerate}
  \item \texttt{InitHpSharkReferenceKernel}: allocates device/host state for the reference orbit and sets
        \((c_x,c_y)\), max radius, and launch configuration.
  \item \texttt{InvokeHpSharkReferenceKernel}: advances the orbit in bounded batches of at most
        \(\texttt{MaxOutputIters}\), storing results in \texttt{OutputIters}.
  \item \texttt{ShutdownHpSharkReferenceKernel}: frees persistent resources.
\end{enumerate}

Because the precision must be fixed at compile time for the \texttt{
HpSharkFloatParams} specialization, \texttt{DispatchByPrecision} rounds the 
requested precision to a power of two and chooses from a fixed set (\(\{256,512,
\dots,524288\}\) bits). Each invocation appends the emitted \texttt{OutputIters}
 records into the CPU-side \texttt{PerturbationResults}. Periodicity and escape 
are reported through \texttt{PeriodicityStatus} and handled similarly to the 
CPU paths.


\section{Perturbation-only Mandelbrot rendering (without linear approximation)}
\label{sec:perturb-only}

This kernel can render the Mandelbrot set using \emph{perturbation alone}, i.e.,
without taking any LA v2 linear-approximation steps. The same CUDA entry point
\code{mandel\_1xHDR\_float\_perturb\_lav2<IterType,T,SubType,Mode,PExtras>} is
used; the behavior is selected at compile time via \code{LAv2Mode}. In
particular, when \code{Mode} includes only the perturbation path (e.g.\
\code{LAv2Mode::PO}), the kernel skips the LA stage traversal and runs only the
perturbation loop against a stored reference orbit.

\subsection{Rendering objective}
\label{sec:perturb-only-goal}

The goal remains standard escape-time rendering for
\[
z_{n+1} = z_n^2 + c,\qquad z_0=0,
\]
with bailout \(|z|^2 \ge 4\). For each pixel, the kernel computes the parameter
\(c\), iterates until escape or \code{n\_iterations}, and stores the resulting
iteration count in \code{OutputIterMatrix[idx]}.

In perturbation rendering, the expensive high-precision orbit evaluation for
each pixel is avoided by reusing a \emph{reference orbit} computed at a
reference parameter \(c_\star\), then evolving only the \emph{delta orbit} for
nearby pixels.

\subsection{Reference orbit and delta formulation}
\label{sec:perturb-only-deltas}

Let the reference parameter be \(c_\star\), with stored reference orbit
\(\{z_n^\star\}\) satisfying
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a pixel parameter \(c\) near \(c_\star\), define the parameter delta
\[
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star,
\]
and the orbit delta
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star.
\]
Substituting \(z_n = z_n^\star + \Delta z_n\) into the Mandelbrot recurrence
yields the \emph{exact} delta recurrence:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right) \\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
Perturbation uses this recurrence directly: it is not a linearization. The work
per iteration is reduced because \(z_n^\star\) is fetched from storage rather
than recomputed in high precision.

\subsection{Pixel parameter delta \texorpdfstring{$\Delta c$}{Delta c}}
\label{sec:perturb-only-deltac}

For each pixel \((X,Y)\), the kernel constructs a delta parameter relative to a
chosen reference center (sign conventions incorporate the image mapping):
\begin{align}
\Delta c_x &= dx \cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy \cdot Y - \texttt{centerY},
\end{align}
and stores this as \code{DeltaSub0} (with scalar components \code{DeltaSub0X},
\code{DeltaSub0Y}). The perturbation state starts at
\[
\Delta z_0 = 0,
\]
so \code{DeltaSubN} is initialized to zero.

\subsection{Perturbation recurrence used in the kernel}
\label{sec:perturb-only-update}

Writing the reference sample as \(z^\star = x^\star + i y^\star\) and the delta
as \(\Delta z = \Delta x + i \Delta y\), the delta recurrence can be expressed
in a factored form convenient for implementation:
\begin{equation}
\Delta z \leftarrow \Delta z \cdot (2 z^\star + \Delta z) + \Delta c.
\label{eq:perturb-factor}
\end{equation}
This identity is equivalent to
\(\Delta z_{n+1} = 2 z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c\), because
\(\Delta z\cdot(2z^\star+\Delta z) = 2z^\star\Delta z + (\Delta z)^2\).

The kernel implements \cref{eq:perturb-factor} in real arithmetic by forming
the sums
\[
(2x^\star + \Delta x),\qquad (2y^\star + \Delta y),
\]
then updating:
\begin{align}
\Delta x &\leftarrow \Delta x\,(2x^\star + \Delta x) - \Delta y\,(2y^\star + \Delta y) + \Delta c_x, \\
\Delta y &\leftarrow \Delta x\,(2y^\star + \Delta y) + \Delta y\,(2x^\star + \Delta x) + \Delta c_y.
\end{align}
In the code, the intermediate quantities correspond to:
\begin{verbatim}
tempSum1 = 2*zy + DeltaSubNYOrig;  // (2 y^\star + \Delta y)
tempSum2 = 2*zx + DeltaSubNXOrig;  // (2 x^\star + \Delta x)
\end{verbatim}
followed by the real/imag updates. For extended or HDR types, the kernel routes
the same math through type-specialized implementations
(\code{T::custom\_perturb2} / \code{T::custom\_perturb3}) to keep the inner loop
tight and to enforce the type's reduction/normalization rules.

\subsection{Reconstructing the absolute orbit for escape testing}
\label{sec:perturb-only-reconstruct}

Perturbation evolves \(\Delta z_n\), but escape-time rendering requires a
bailout test on the absolute orbit \(z_n\). Each iteration reconstructs:
\[
z_n \approx z_n^\star + \Delta z_n,
\]
using the stored reference sample \(z_n^\star\) and the current delta. In the
kernel, this appears as:
\[
x = x^\star + \Delta x,\qquad y = y^\star + \Delta y.
\]
The bailout test is then performed on
\[
|z|^2 = x^2 + y^2,
\]
with the canonical threshold \(4\). For HDR and related types, the norm and the
comparison are performed on reduced values using specialized reduced
comparators to keep the escape decision stable at deep zoom.

\subsection{Reference index management and recentering}
\label{sec:perturb-only-recenter}

The perturbation loop advances a reference-orbit index \code{RefIteration} in
lockstep with \code{iter}, fetching \(z^\star\) samples from \code{Perturb}. The
kernel includes a \emph{recentering} mechanism that resets the delta
representation when it becomes ill-conditioned. Conceptually, if the delta
becomes comparable to or larger than the reconstructed orbit, the decomposition
\(z = z^\star + \Delta z\) stops being numerically advantageous. In that case,
the kernel folds the delta into the base by setting:
\[
\Delta z \leftarrow z,\qquad \text{and restart the reference index.}
\]
Operationally, the code compares the reconstructed orbit magnitude proxy
against the delta magnitude proxy, and also recenters if the reference index
reaches the end of the stored orbit samples (to avoid out-of-range sampling).
After recentering, perturbation continues from the new base representation.

This mechanism keeps the perturbation method usable across a wide range of
pixels and iteration depths while preserving the core rendering objective:
compute escape-time using a stable bailout on the reconstructed orbit.

\subsection{Using \code{LAv2Mode} to select perturbation-only execution}
\label{sec:perturb-only-mode}

The kernel is structured as two compile-time phases:
\begin{itemize}
\item an LA v2 phase guarded by \code{Mode == Full || Mode == LAO},
\item a perturbation phase guarded by \code{Mode == Full || Mode == PO}.
\end{itemize}
Therefore, perturbation-only rendering is achieved by instantiating the kernel
with a mode that includes only the perturbation path (e.g.\ \code{LAv2Mode::PO}).
In this configuration:
\begin{itemize}
\item \code{DeltaSub0} is computed from the pixel location,
\item \code{DeltaSubN} remains initialized to \(\Delta z_0 = 0\),
\item the kernel runs the perturbation loop, reconstructing \(z\) each step for
      bailout tests,
\item the final escape-time count is written to \code{OutputIterMatrix}.
\end{itemize}
This provides a complete Mandelbrot renderer based purely on reference-orbit
perturbation, without relying on any linear-approximation hierarchy.

\section{Approximation-based acceleration for Mandelbrot rendering}
\label{sec:approx-accel}

At deep zoom, the cost of iterating high-precision types for every pixel can be
dominant. The remaining components described below are still in service of the
same rendering goal: compute escape-time for \(z_{n+1}=z_n^2+c\), but by reusing
a \emph{reference orbit} and evolving \emph{deltas} for nearby pixels.

\subsection{Bilinear approximation (BLA) for orbit deltas}
\label{sec:bilinear-approx}

\subsubsection{Reference orbit and delta formulation}
Let the reference parameter be \(c_\star\) with orbit:
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a nearby pixel parameter \(c = c_\star + \Delta c\), define:
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star,\qquad
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star.
\]
Expanding the recurrence gives:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right)\\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
When \(\Delta z_n\) remains small, the quadratic term can be neglected,
yielding a linearized update:
\begin{equation}
\Delta z_{n+1} \approx A_n\,\Delta z_n + B_n\,\Delta c,\qquad
A_n = 2z_n^\star,\quad B_n = 1.
\end{equation}
BLA generalizes this into precomputed multi-step maps that \emph{jump} multiple
iterations while maintaining an explicit validity bound.

\subsubsection{What \code{BLA<T>} stores}
A \code{BLA<T>} instance stores two complex coefficients:
\[
A = A_x + iA_y,\qquad B = B_x + iB_y,
\]
plus:
\begin{itemize}
  \item \code{r2}: a squared-radius validity bound used during lookup,
  \item \code{l}: the number of Mandelbrot iterations summarized by this step.
\end{itemize}
These objects exist to accelerate \emph{escape-time evaluation} by evolving
\(\Delta z\) cheaply for many pixels, then reconstructing \(z \approx z^\star +
\Delta z\) to perform bailout checks consistent with Mandelbrot rendering.

\subsubsection{Applying a step: complex multiply-add}
The method \code{getValue(RealDeltaSubN, ImagDeltaSubN, RealDeltaSub0, ImagDeltaSub0)}
applies:
\[
\Delta z \leftarrow A\,\Delta z + B\,\Delta c,
\]
expanded into real arithmetic:
\begin{align}
\Re(\Delta z') &= A_x \Re(\Delta z) - A_y \Im(\Delta z) + B_x \Re(\Delta c) - B_y \Im(\Delta c), \\
\Im(\Delta z') &= A_x \Im(\Delta z) + A_y \Re(\Delta z) + B_x \Im(\Delta c) + B_y \Re(\Delta c).
\end{align}

\subsubsection{Composing steps to build longer jumps}
If one step maps \(\Delta z \mapsto A_x\Delta z + B_x\Delta c\) and a second maps
\(\Delta z' \mapsto A_y\Delta z' + B_y\Delta c\), the composition is:
\[
A_{\text{new}} = A_yA_x,\qquad B_{\text{new}} = A_yB_x + B_y.
\]
This supports a hierarchy of step sizes (often powers of two) for quickly
advancing delta orbits while rendering the escape-time field.

\subsubsection{GPU lookup: selecting a valid aligned step}
A GPU-side helper such as \code{GPU\_BLAS} stores the hierarchy and selects a
step that is both \emph{aligned} with the current iteration index and
\emph{valid} under the current bound check (typically comparing a computed
squared-magnitude proxy \code{z2} against \code{r2}). When a step is valid, the
renderer can advance the orbit by \code{l} iterations at a cost far below
performing \code{l} full high-precision Mandelbrot updates.

\subsection{LA v2 linear approximation with perturbation (HDR kernel)}
\label{sec:la-v2-perturb}

This kernel family combines staged linear-approximation steps with a
perturbation finisher loop against a stored reference orbit. The rendering
objective remains escape-time Mandelbrot evaluation; the kernel accelerates
that evaluation by evolving deltas and periodically reconstructing \(z\) to
perform bailout checks.

\subsubsection{Parameter delta per pixel}
Each pixel constructs a parameter offset \(\Delta c\) relative to a selected
reference center (sign conventions may incorporate the image \(Y\)-flip):
\begin{align}
\Delta c_x &= dx\cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy\cdot Y - \texttt{centerY},
\end{align}
and packs \(\Delta c=\Delta c_x+i\Delta c_y\) into \code{DeltaSub0}.

\subsubsection{Delta state and reconstruction}
The kernel maintains \(\Delta z_n\) in \code{DeltaSubN} and reconstructs an
absolute orbit estimate using the stored reference orbit sample
\(z_j^\star\):
\[
z \approx z_j^\star + \Delta z.
\]
Escape-time rendering then proceeds by applying approximation steps when valid,
or performing perturbation updates otherwise, while periodically testing the
bailout condition on the reconstructed \(z\).

\subsubsection{Perturbation update}
Given a reference sample \(z^\star=x^\star+iy^\star\) and \(\Delta z=\Delta
x+i\Delta y\), the perturbation form is:
\[
\Delta z \leftarrow \Delta z\cdot(2z^\star+\Delta z) + \Delta c.
\]
In real arithmetic, with temporaries corresponding to \((2x^\star+\Delta x)\)
and \((2y^\star+\Delta y)\), this yields:
\begin{align}
\Delta x &\leftarrow \Delta x\,(2x^\star+\Delta x) - \Delta y\,(2y^\star+\Delta y) + \Delta c_x, \\
\Delta y &\leftarrow \Delta x\,(2y^\star+\Delta y) + \Delta y\,(2x^\star+\Delta x) + \Delta c_y.
\end{align}
In code comments, write these as \(x^\star\) and \(\Delta x\) (and similarly for
\(y\)) rather than using Unicode symbols.

\subsubsection{Escape test}
After updating \(\Delta z\), reconstruct \(z\approx z^\star+\Delta z\) and test:
\[
|z|^2 = x^2+y^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\]
For HDR/expanded types, norms and comparisons are performed using reduced values
and specialized reduced comparators, preserving meaningful bailout decisions at
deep zoom.

\section{Summary: rendering-oriented view}
\label{sec:summary}

All kernels and acceleration schemes described here serve the same rendering
task: compute per-pixel escape-time for the Mandelbrot recurrence
\(z_{n+1}=z_n^2+c\). The implementation provides a precision ladder:
\begin{itemize}
  \item \code{1x} float/double: direct iteration for speed and moderate zoom,
  \item \code{2x} and \code{4x} expansions: deeper zoom with controlled error,
  \item HDR-normalized expansions: stable deep zoom with explicit reduction,
  \item BLA / LA v2 + perturbation: reuse a reference orbit to accelerate
        high-precision escape-time rendering for nearby pixels.
\end{itemize}
The common structure (pixel mapping, orbit iteration, bailout test, iteration
count store) ensures each section remains directly tied to Mandelbrot rendering
as the end goal.

\section{Development Notes}

This section simply has some development notes that were posted over time.

\subsection{2025-11-30 News}

The neat thing about this GPU approach is that it still has low-hanging fruit 
related to optimization, unlike MPIR.  I've been wasting a bunch of time on the 
(Thanksgiving in USA) holiday weekend working on it.  I'll keep fussing with it 
and will probably not post again until I put out a version of FractalShark with 
it hooked up, which I expect to happen later in December when I have time off.  
All code is on github, just no new version yet since it remains rather 
experimental and hacked up.

Example times in ms of updated implementation, comparing against serial host-
based approach (1 thread MPIR AVX2 for experiments).  This is the first 20000 
iterations of View 30 in FractalShark, which is a depth $\sim 1\mathrm{e}{100000
}$ point.  This uses 16384 limbs on the GPU.  The CPU/MPIR uses the minimum 
bits required for that point, which is less, because the GPU implementation 
requires a power of 2.

\begin{verbatim}
Host (ms)    GPU(ms)    Ratio
57990        2055    28.2189781
58227        2022    28.79673591
57478        2041    28.16168545
57538        2014    28.56901688
55997        2015    27.79007444

Averages:
57446        2029.4    28.30729816
\end{verbatim}

Here's a summary of what I want to get done before FractalShark 0.5 happens:

\begin{itemize}

\item Perform code cleanup, better comments etc.  It's a mess right now, this 
is just a weekend hobby after all and because it was unclear this would even 
work it's a real hack job.

\item Improve integration with FractalShark, it's really just hacked in there 
currently since I've mostly used a standalone test program

\item Improve reference orbit memory usage - I would like to be able to 
allocate a fixed amount of memory to store the orbit and expand as needed.  
Right now it pessimistically allocates a lot, and it may be the wrong amount 
because it does periodicity detection on the GPU.  This is just engineering 
work and nothing fundamental, but will take time and without it the current 
implementation is rather impractical.

\item Automatically choose kernel launch parameters, right now it's manual.  
More engineering.  This is required to support other cards than mine.  In any 
case this implementation requires a feature call cooperative groups, which I 
believe implies Nvidia RTX 2XXX series or newer.

\end{itemize}

Those are the main things offhand, and I'm hoping with time off in December I can clean these up.

\subsection{2025-9-1 News}

The full reference orbit works with CUDA, though without periodicity detection 
or high precision to ``float exp'' conversion.  That's future work but I'm not 
worried about it.  To be clear, this is not hooked up end-to-end with 
FractalShark itself, it's only working in a standalone test environment.  But 
the results are promising and prove it works.

This initial implementation relies on Karatsuba for the multiplies/squaring and 
then follows those with the high-precision adds/subtracts.  Initial results 
suggest a $\sim 12\times$ perf improvement relative to single-threaded CPU 
only, when comparing an overclocked 5950X vs an RTX 4090 with CUDA.  I'm happy 
with that, but not completely.

The main performance problem is this Karatsuba implementation.  Getting decent 
performance out of Karatsuba obviously requires recursion, and that gets costly 
on the GPU.  This implementation recurses several levels, which avoids costly 
local memory spill, but bites us because of register pressure.  The high 
register pressure limits the parallelism we can achieve.  The nice thing about 
Karatsuba for me is that it's not that hard to understand conceptually, so it 
was a great initial target for someone who doesn't know what they're doing.

Now that it's working, and I have a better sense of what's going on, I'm going 
to try a full NTT-based high-precision multiply approach.  The idea here is to 
rely on the number theoretic transform, similar to FFT, and parallelize the 
high precision multiply that way.

With this commit, we have a working host-based (CPU-only) approach to NTT high-
precision multiply that supports power-of-2 mantissa sizes and should scale 
effectively to CUDA but that's TBD.  It will be at least several months more 
work at my current rate (a few hours on the weekends) to achieve a first-cut 
CUDA implementation.

\subsubsection{NTT-based high-precision multiply (magic prime $2^{64} - 2^{32} + 1$)}

AI-generated slop follows in this subsection.  It looks accurate.

I'm experimenting with an NTT implementation over the 64-bit ``magic'' prime $p 
= 2^{64} - 2^{32} + 1$. This prime is NTT-friendly: it admits $2^{32}$-th roots 
of unity, so power-of-two transform sizes are straightforward, and it enables 
fast modular reduction on 128-bit products using the identity $2^{64} \equiv 2^{
32} - 1 \pmod p$.

\paragraph{High-level plan}

\begin{itemize}

\item Represent big mantissas as base-2 limbs (currently 32-bit limbs are 
convenient on GPU/CPU). Choose $N =$ next power of two $\ge 2\cdot L$ ($L =$ 
limb count) for the convolution length.

\item Forward NTT(A), NTT(B) mod $p$, pointwise multiply, inverse NTT, multiply 
by $N^{-1} \bmod p$, then perform carry propagation back to the chosen limb 
base.

\item Use iterative radix-2 Cooley--Tukey with an explicit bit-reversal 
permutation (DIT). Twiddles (powers of a primitive root) are precomputed and 
cached.

\item Butterflies and pointwise products operate in Montgomery form; 128-bit 
products are reduced via Montgomery multiplication ($R = 2^{64}$). A direct 
pseudo-Mersenne fold $(\text{lo} + (\text{hi} \ll 32) - \text{hi})$ exists but 
isnâ€™t used on the hot path.

\end{itemize}

\paragraph{Notes and guardrails}

\begin{itemize}

\item Single-prime NTT is attractive here because $p$ fits in 64 bits and gives 
ample dynamic range; if/when larger bases or tighter bounds are desired, a multi
-prime CRT variant is the next step.

\item Power-of-two sizes only: that matches the current host prototype and 
simplifies CUDA mapping.

\item Carry fix-up remains outside the NTT and is done in base-$2^k$ with linear
-time passes; lazy (deferred) carries may help throughput.

\end{itemize}

\paragraph{Why this might beat Karatsuba on GPU}

\begin{itemize}

\item Avoids deep recursion and its register pressure; most work is regular 
butterflies, which parallelize and schedule well.

\item Pointwise multiplies dominate cost but are simple $64\times 64 \rightarrow
 128$ with fast reduction; memory access is structured and coalesced.

\end{itemize}

If the CUDA path pans out, the NTT route should scale better across precisions 
while keeping occupancy higher than the recursive Karatsuba path.

\paragraph{References (NTT / GPU big-int)}
\begin{itemize}
\item CGBN: CUDA Big-Num with Cooperative Groups \cite{CGBN_NVlabs}
\item Number-theoretic transform \cite{NTT_Wikipedia}
\end{itemize}

\subsection{2025-6-15 News}

This page actually gets traffic occasionally, so I just wanted to post a short 
update.  Since last August, I've been working on a CUDA-based, high-precision 
reference orbit implementation.  The objective is to beat FractalShark's 
existing multithreaded reference-orbit performance at higher digit counts, at 
least if you have a decent card.  Scroll down to ``[2025-6-15](\#2025-6-15)'' 
for the latest information on this subject.

Still fussing with it, with some delays because of vacation etc.  Having some 
issues with the optimized ``add'' implementation that does the 5-way add/
subtract.  It's a fun project, but has ended up more complex than I'd 
expected.  The reference implementation is almost working the way I want.

Worst case I could dump it and fallback to a series of regular A+B adds/
subtracts but I'm pretty determined to make the optimized approach work.  TBD 
if the performance actually pays off.  (Yes, I can hear you saying the 
Mandelbrot multiplies/squares dominate the cost, but it's bugging me and fun to 
play with).

\subsection{2025-4-26}

This high-precision arithmetic project is a lot of fun even if it's pretty 
ameteur-hour -- I know I'm leaving a lot of perf on the table yet.

Here's a brief update.  I'm happy enough with Karatsuba multiply now.  I've got 
$3\times$ parallel multiplies working.  For Mandelbrot, that corresponds to $x^2
$, $y^2$ and $x\cdot y$.  Rather than doing an optimized squaring 
implementation, I'm just jamming everything into the same Karatsuba 
implementation, so that all the synchronization is re-used.

A few weeks ago I had CUDA floating point add working.  That's much easier of 
course, though carry propagation is interesting.  I tried a parallel prefix sum 
but the performance was a bit underwhelming in the average case, which is what 
I care about.  I instead implemented a more naive strategy that has better 
average case perf and linear worst-case performance, which I think I'm fine 
with for Mandelbrot.  I'm not using warp-level primitives and haven't hooked up 
shared memory on that, so it's horrid performance compared to simply doing it 
on the CPU but as a percentage of the total it's minor, because large 
multiplies are so costly.  I'm not that worried about Add at this point.

I'm currently hooking up a 5-way combined add/subtract that does $A - B + C$ 
and $D + E$ in parallel to produce two outputs.  These inputs corresponds to $X^
2 - Y^2 + C_x$ and $2\cdot X\cdot Y + C_y$.

Strategy-wise, the idea is to complete this multi-way add operator, and then we 
should be able to do a reference orbit using alternating $3$-way multiply and $5
$-way add calls in CUDA.  It also needs periodicity detection and high-
precision to float+exp conversion, which shouldn't be bad.  Maybe in a few 
months I'll have something working end-to-end.

I was also speculating about trying SchÃ¶nhage--Strassen CUDA multiply, but 
that's a ways out.

\subsection{2025-3}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepency arises in the CUDA 
calculation.  The discrepency points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \texttt{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \texttt{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \texttt{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

Current best result (5950X MPIR vs RTX 4090), 50000 sequential multiplies of 
7776-limb numbers, 128 blocks, 96 threads/block, uses shared memory and 162 
registers (max 255):

Host iter time: 26051 ms

GPU iter time: 2757 ms

Edited from earlier, fussing with perf-related parameters.  I'm very happy with how it's looking now.

\subsection{2025-2 -- What's going on with this native CUDA reference orbit calculation?}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepency arises in the CUDA 
calculation.  The discrepency points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \texttt{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \texttt{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \texttt{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

\subsection{2025-1 -- What's going on with this native CUDA reference orbit calculation?}

The repository (``TestCuda'') has a new Karatsuba, high-precision, floating 
point multiply implementation working on my GPU and results are showing the CPU 
take $3$--$4\times$ longer (MPIR/AVX2) than the GPU on sequential multiplies of 
random numbers.  That's a key point -- sequential multiplies, so the result is 
applicable to e.g.\ a reference orbit calculation.  I'm really happy with this 
result, because there's leftover hardware on the GPU that could be used to run 
a couple of these in parallel (e.g.\ two squares and a multiplication or three 
squares for Mandelbrot).

Getting high-throughput high-precision on-GPU is already more-or-less solved:  
Nvidia already provides a library for it (see related work below).  But getting 
sequential to work decently at sizes that are still interesting (e.g.\ ones we 
might actually try on the Mandelbrot) is not as widely investigated, which is 
why I've been dwelling on this for a while and still have only mediocre results 
:p

A variety of caveats currently:

\begin{itemize}

\item I'm comparing a 5950X vs RTX 4090, which clearly affects the relative 
numbers.

\item The approach requires CUDA cooperative groups, which I think is RTX 2xxx 
and later, so fairly recent cards.

\item The size I'm getting the best result at currently is relatively large: 
8192 32-bit limbs, which is pretty big.  It still beats the CPU down to 2048 
limbs though (CPU takes $1.6\times$ longer here) and at that size there is a 
bunch of unused hardware on the GPU so it should be possible to do the three 
multiplies for Mandelbrot in parallel.

\end{itemize}

Anyhow, I wanted to post it because this is a pretty complex investigation and 
I expect to spend some more time on it because it's been fun to look at.

Here are some bullet points on the approach:

\begin{itemize}

\item Uses CUDA cooperative groups, so we should be able to do a reference 
orbit with a single kernel invocation.

\item 32-bit limbs, 128-bit intermediate results (2x64 integers) because of 
intermediate carries.  Really it's just 64 bits + plus a few more.  This 
approach is likely not super-efficient but it's where we're at.

\item Stores the input mantissa in the chip ``shared memory''.  For 8192 limbs, 
that's $8192 \cdot 4$ bytes $\cdot 2$ numbers to multiply $= 64$KB, and then 
stores $2 \cdot 16$KB extra for an intermediate Karatsuba result, for 96KB 
total.  This piece is negotiable and I could bring down/eliminate the shared 
memory requirement depending on how things progress.

\item One GPU thread per input limb, or two output limbs per thread.  It does a 
full 16384 limb output in this example and then truncates/shifts it.

\item The GPU floating point format has a separate integer exponent, which is 
overkill for Mandelbrot but I figured I'd keep it for now because it's not a 
performance problem.  It also keeps a sign separately.

\item The application I'm using to test this has a bunch of cases to verify 
that it's producing correct results.  It generates pseudo-random numbers with 
many \texttt{0xFFF...} limbs, zero-limbs, and related, to force carries/
borrows.  The test program compares all results against my own Karatsuba CPU-
based implementation I can use as a reference, and more importantly, the MPIR 
implementation (\texttt{mpf\_mul}) for correctness.

\item I've got MPIR to GPU and GPU to MPIR conversion capability, so it's easy 
to translate formats as needed.

\item The GPU implementation gets its best performance when it doesn't recurse, 
and instead switches straight to convolutions on the sub problems.  Recursing 
is still getting me slightly worse performance and I think I know why but 
haven't worked out how to fix it.

\end{itemize}

Here is some related work:

\begin{itemize}

\item A Study of High Performance Multiple Precision Arithmetic on Graphics 
Processing Units: Niall Emmart \cite{Emmart_GPU_MultiPrecision}

\item Missing a Trick: Karatsuba Variations: Michael Scott \cite{Scott_Karatsuba
_MissingTrick}

\item MFFT: A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT 
Framework: \cite{MFFT_GPU_FFT}

\item Karatsuba Multiplication in GMP: \cite{GMP_Karatsuba}

\item Karatsuba Algorithm: \cite{Karatsuba_Wikipedia}

\item Toom--Cook Multiplication: \cite{ToomCook_Wikipedia}

\item Sch{\"o}nhage--Strassen Algorithm: \cite{SchonhageStrassen_Wikipedia}

\item CGBN: CUDA Accelerated Multiple Precision Arithmetic Using Cooperative 
Groups: \cite{CGBN_NVlabs}

\end{itemize}


All the code is here / GPL etc but it's just an experiment and not integrated with the application: \\
\url{https://github.com/mattsaccount364/FractalShark}

It'll all end up in FractalShark eventually assuming I can get something end-to-
end working, and at this point I believe I can, but this is very slow-going 
yet.  Anyway, it's a fun area and I'll probably continuing dinking with it, so 
there probably won't be much new on FractalShark proper until I can get this 
behemoth under better control.  And we'll see if it works out -- lots of 
remaining details to resolve and it may not work decently when combined into a 
full reference orbit.  Overall though I'm really happy with where it's at.  
Happy to answer questions etc.

\bibliographystyle{plain} % or ieeetr, unsrt, alpha, etc.
\bibliography{notes}      % <-- filename WITHOUT .bib

\end{document}
