\documentclass[12pt]{article}

% --- Encoding / fonts (fixes OT1 issues + improves monospace metrics) ---
\usepackage[T1]{fontenc}
\usepackage{lmodern}

% --- Math ---
\usepackage{amsmath}
\usepackage{amssymb}

% --- Layout ---
\usepackage{geometry}
\geometry{a4paper, margin=1in}

% --- Better line breaking / fewer overfull boxes ---
\usepackage{microtype}
\emergencystretch=2em % last-resort "be less picky" knob for line breaks

% --- Links and cross-refs ---
\usepackage[colorlinks=true,linkcolor=blue,urlcolor=blue,citecolor=blue]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}

% --- Convenience for inline code that may need breaks ---
\newcommand{\code}[1]{\texttt{#1}}

\newcommand{\FractalShark}{FractalShark}
\newcommand{\FractalSharkItalic}{\textit{FractalShark}}


\title{\FractalSharkItalic{}: High-Performance Mandelbrot Rendering}
\author{Matthew Renzelmann}
\date{\today}

\begin{document}
\maketitle

\makeatletter
\renewcommand\numberline[1]{#1\enspace}
\makeatother

\tableofcontents
\clearpage

\section{\FractalShark{} Overview}
\label{sec:goal}

\FractalShark{} is an interactive Mandelbrot set renderer focused on
\textbf{extreme deep-zoom exploration}, numerical correctness, and
algorithmic experimentation.  It supports zoom depths ranging from
conventional views to magnifications exceeding $10^{10000}$, while
exposing internal rendering choices and precision tradeoffs to the user.

\paragraph{Interactive navigation and view control.}
\FractalShark{} provides direct mouse-driven navigation, including centering
the view at an arbitrary point, zooming in and out at the cursor location,
stepping backward through navigation history, and invoking automatic zoom
modes.  Window management options allow toggling between windowed and
full-screen modes, including square-aspect rendering for precise analysis.

\paragraph{Built-in views for demonstration and validation.}
The application includes an extensive set of built-in views covering a wide
range of numerical regimes: GPU precision limits, high-period locations,
known hard points, historical bug cases, regression tests, and ultra-deep
zoom demonstrations.  These views serve both as showcases of capability and
as repeatable test cases for performance and correctness.

\paragraph{Explicit algorithm selection and transparency.}
A central design goal of \FractalShark{} is to make rendering algorithms
explicit and selectable.  Users may rely on an automatic algorithm selector
or manually choose among many rendering paths, including:
\begin{itemize}
  \item GPU-based low-zoom renderers with selectable iteration precision and
        multiple numeric formats (single-, dual-, and quad-limb 32- and
        64-bit variants, as well as HDR-style representations).
  \item Scaled perturbation algorithms for extending low-precision arithmetic
        to deeper zooms.
  \item Bilinear approximation (BLA v1) perturbation paths.
  \item Linear approximation (LAv2) pipelines, including full rendering,
        linear-approximation-only, and perturbation-only variants.
  \item Reference-compression–aware algorithms, including Imagina-compatible
        ``max'' compression formats.
  \item CPU-only algorithms for very high precision, verification, and
        fallback scenarios.
\end{itemize}

Many of these modes are exposed explicitly for testing and comparison, and
not all are intended as default or production-quality paths.

\paragraph{Linear approximation configuration.}
Linear approximation behavior can be configured for multithreaded or
single-threaded execution and tuned via presets that prioritize accuracy,
performance, or memory usage.  These controls reflect ongoing development
and experimentation with LA parameter tradeoffs.

\paragraph{Image quality and coloring.}
\FractalShark{} supports GPU antialiasing at multiple sample levels and a
flexible palette system.  Palettes may be selected from predefined themes,
generated randomly, and rendered at configurable color depths.  Some palette
features are intentionally limited or disabled where they are known to be
incomplete.

\paragraph{Iteration limits and precision control.}
Users can dynamically adjust iteration limits, switch between 32-bit and
64-bit iteration counters, and modify shallow-zoom iteration precision.
These controls are primarily relevant for low-zoom or testing scenarios and
are not universally applicable to all rendering algorithms.

\paragraph{Perturbation and reference-orbit management.}
\FractalShark{} places strong emphasis on reference-orbit reuse and
perturbation-based rendering.  It supports multiple perturbation strategies
(single-threaded, multithreaded, periodicity-assisted, and experimental GPU
variants), along with tools to clear, inspect, save, reload, and reuse
reference orbits.  Automatic and manual reference-orbit persistence is
supported via file-backed storage.

\paragraph{Memory management and scalability.}
To enable extreme zoom depths without exhausting system memory, \FractalShark{}
employs file-backed storage, optional reference compression, configurable
memory limits, and automatic cleanup policies.  These mechanisms are closely
tied to recent allocator and reference-orbit refactors discussed in the
project development history.

\paragraph{Diagnostics, benchmarking, and testing.}
The application includes facilities for displaying detailed rendering
parameters, running repeatable benchmarks, executing regression tests, and
comparing reference orbits (including Imagina-compatible formats).  These
features reflect FractalShark’s dual role as both a visualization tool and a
development platform.

\paragraph{Data export and interoperability.}
\FractalShark{} can save rendered images, high-resolution bitmaps, iteration
counts, and reference orbits in multiple text and compressed formats.  It
supports loading external reference orbits and matching Imagina-compatible
data for cross-tool validation.

\medskip
\noindent
Overall, \FractalShark{} functions as both a \textbf{deep-zoom Mandelbrot
explorer} and a \textbf{research and experimentation platform} for
high-precision fractal rendering, prioritizing transparency, correctness,
and extensibility over a simplified or opaque user experience.


\section{Kernel List}

This section simply lists all the CUDA kernels in \FractalShark{}.

\begin{itemize}

\item \textbf{Mandelbrot base kernels}
  \begin{itemize}
    \item \texttt{mandel\_1x\_float}
    \item \texttt{mandel\_1x\_double}
    \item \texttt{mandel\_2x\_float}
    \item \texttt{mandel\_2x\_double}
    \item \texttt{mandel\_4x\_float}
    \item \texttt{mandel\_4x\_double}
    \item \texttt{mandel\_hdr\_float}
  \end{itemize}

\item \textbf{Mandelbrot perturbation kernels}
  \begin{itemize}
    \item \texttt{mandel\_1x\_double\_perturb\_bla}
    \item \texttt{mandel\_1xHDR\_float\_perturb\_bla}
    \item \texttt{mandel\_1xHDR\_float\_perturb\_lav2}
  \end{itemize}

\item \textbf{High-precision reference kernels}
  \begin{itemize}
    \item \texttt{HpSharkReferenceGpuKernel}
    \item \texttt{HpSharkReferenceGpuLoop}
  \end{itemize}

\item \textbf{Utility kernels}
  \begin{itemize}
    \item \texttt{antialiasing\_kernel}
    \item \texttt{max\_reduce}
    \item \texttt{max\_kernel}
  \end{itemize}

\item \textbf{Addition kernels}
  \begin{itemize}
    \item \texttt{AddKernel}
    \item \texttt{AddKernelTestLoop}
  \end{itemize}

\item \textbf{Multiplication (NTT) kernels}
  \begin{itemize}
    \item \texttt{MultiplyKernelNTT}
    \item \texttt{MultiplyKernelNTTTestLoop}
  \end{itemize}

\item \textbf{Disabled kernels}
  \begin{itemize}
    \item \texttt{mandel\_2x\_float\_perturb\_setup}
    \item \texttt{mandel\_2x\_float\_perturb}
  \end{itemize}

\end{itemize}


\section{Code Overview}

\begin{itemize}

\item All the CUDA kernels are in \texttt{render\_gpu.cu}. The two most 
interesting are probably \texttt{mandel\_1xHDR\_float\_perturb\_bla} and \texttt
{mandel\_1xHDR\_float\_perturb\_lav2}, which are the ones I've spent the most 
time on lately. For better performance at low zoom levels, you could look at 
\texttt{mandel\_1x\_float\_perturb}, which leaves out linear approximation and 
just does straight perturbation up to $\sim 10^{30}$, which corresponds with 
the 32-bit float exponent range.

\item One fun thing you can try is running with LAv2 + ``LA only''. This 
approach actually works pretty well once Linear Approximation kicks in at 
deeper zooms --- it gives you an idea of what the actual image should look like 
but is very fast, since it does no perturbation. The images it produces are not 
precise, and often leave out the fine detail; however, it's fun to play with 
when zooming in on a specific point.

\item The \texttt{mandel\_1x\_float} is the classic 32-bit float Mandelbrot and 
is screaming fast on a GPU. This one is optimized with fused multiply-add for 
fun even though it's kind of useless because you can barely zoom in before you 
get pixellation.

\item The most interesting reference orbit calculation is at \texttt{
AddPerturbationReferencePointMT3}. It includes a ``bad'' calculation which is 
used for the ``scaled'' CUDA kernels. The multithreaded approach handily beats 
the single-threaded implementation on my CPU in all scenarios.

\item There are CPU renderers, but they were mostly to learn/debug more easily, 
and aren't optimized heavily. They're much easier to understand and reason 
about though.

\end{itemize}


\section{The Mandelbrot set and escape-time rendering}
\label{sec:mandelbrot-intro}

The Mandelbrot set \(M\subset\mathbb{C}\) is defined as the set of complex
parameters \(c\) for which the orbit of
\begin{equation}
z_{n+1} = z_n^2 + c,\qquad z_0 = 0
\end{equation}
remains bounded. A common rendering method is \emph{escape-time}: iterate until
the orbit magnitude exceeds a bailout threshold, or until a maximum iteration
count is reached.

A standard bailout uses \(|z_n|^2 \ge 4\), since \(|z|>2\) implies divergence:
\begin{equation}
|z_n|^2 = \Re(z_n)^2 + \Im(z_n)^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}
For each pixel, the stored value is the smallest \(n\) (or a bounded proxy) at
which escape occurs, or the maximum iteration limit if escape never occurs.


\section{Pixel-to-parameter mapping}
\label{sec:mapping}

Each CUDA thread computes a pixel coordinate \((X,Y)\) and maps it to a complex
parameter \(c = x_0 + i y_0\). A typical kernel starts with:
\begin{verbatim}
int X = blockIdx.x * blockDim.x + threadIdx.x;
int Y = blockIdx.y * blockDim.y + threadIdx.y;
if (X >= width || Y >= height) return;
size_t idx = ConvertLocToIndex(X, height - Y - 1, width);
\end{verbatim}

\subsection{Thread-to-pixel mapping}
A 2D CUDA grid of 2D blocks covers the image. Each thread is responsible for one
pixel. The bounds check prevents out-of-range threads from writing.

\subsection{Y-axis convention}
The expression \code{height - Y - 1} flips \(Y\). This is common when the image
buffer uses a top-left origin but the complex-plane mapping assumes a
bottom-left origin (or vice versa).

\subsection{Affine map into the complex plane}
The parameter \(c\) is computed by an affine transform:

\begin{align}
x_0 &= cx + dx \cdot X, \\
y_0 &= cy + dy \cdot Y,
\end{align}

where \code{cx,cy} anchor the plane (e.g., the coordinate at pixel \((0,0)\)),
and \code{dx,dy} are per-pixel increments. Different kernels compute these
expressions in different numeric types; the intent is always the same: map each
pixel to its associated complex parameter \(c\).


\section{Mandelbrot recurrence in real arithmetic}
\label{sec:real-form}

Writing \(z = x + i y\) and \(c = x_0 + i y_0\), the iteration becomes:

\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + x_0, \\
y_{n+1} &= 2 x_n y_n + y_0.
\end{align}

The escape test is:

\begin{equation}
x_n^2 + y_n^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}

Many kernels cache squares:
\[
zrsqr = x^2,\quad zisqr = y^2,
\]
so that \(x^2-y^2\) and \(x^2+y^2\) can be formed cheaply as \code{zrsqr - zisqr}
and \code{zrsqr + zisqr}. This is especially valuable when \(x\) and \(y\) are
represented by multi-component expansion types.


\section{Mandelbrot base kernels}
\label{sec:mandel-base-kernels}

The goal of these kernels in \FractalShark{} is to \emph{render the Mandelbrot set}
by computing an \emph{escape-time} iteration count per pixel over a 2D image
grid efficiently. Each CUDA thread evaluates a single complex parameter \(c\)
corresponding to one pixel, iterates the Mandelbrot recurrence, and stores the
iteration count into an output buffer. Separate (or fused) stages can map
iteration counts to colors, apply palettes, and perform anti-aliasing.

Across these base kernels, the primary variation is the numeric representation
used for the orbit arithmetic: from IEEE-754 \code{float}/\code{double} up
through expansion types (float-float, double-double, quad-float, quad-double)
and HDR-normalized formats. The shared objective remains the same: compute the
escape-time for the Mandelbrot iteration as accurately and efficiently as needed
for a desired zoom depth.

Each kernel variant renders the same Mandelbrot escape-time field; the only
difference is the numeric type used for mapping and orbit iteration. The
following sections describe how each type realizes the same recurrence and
escape test.  This set of kernels does not use linear approximation or
perturbation; they simply evaluate the Mandelbrot iteration directly in the
chosen numeric format.

Throughout, \code{IterType} is the integer type used to store the escape-time
iteration count (e.g., \code{uint32\_t} or \code{uint64\_t}).

\subsection{Kernel: \code{mandel\_1x\_float}}
\label{sec:mandel-1x-float}

\subsubsection{Numeric type}
This variant uses IEEE-754 single precision \code{float}. It provides the
highest throughput but limits usable zoom depth due to rounding error and loss
of significance in \(c\) and the orbit.

\subsubsection{FMA-based orbit update}
The implementation uses fused multiply-add intrinsics:

\begin{verbatim}
ytemp = __fmaf_rd(-y, y, x0);     // x0 - y^2
xtemp = __fmaf_rd(x, x, ytemp);   // x^2 - y^2 + x0
xtemp2 = 2.0f * x;
y = __fmaf_rd(xtemp2, y, y0);     // 2xy + y0
x = xtemp;
\end{verbatim}

These intrinsics correspond exactly to:

\begin{align}
x &\leftarrow x^2 - y^2 + x_0,\\
y &\leftarrow 2xy + y_0.
\end{align}

Using FMA reduces intermediate rounding and can improve performance. The
\code{\_\_fmaf\_rd} variant rounds downward; if IEEE round-to-nearest is desired,
use \code{\_\_fmaf\_rn} (or plain \code{fmaf}).

\subsubsection{Escape test}
The kernel tests \(x^2+y^2 < 4\), which implies recomputing squares each loop in
this simplest variant.  Perhaps this kernel could explicitly cache the squares
for better performance.

\subsection{Kernel: \code{mandel\_1x\_double}}
\label{sec:mandel-1x-double}

\subsubsection{Numeric type}
This variant uses IEEE-754 \code{double} and therefore carries substantially
more mantissa precision than float. The Mandelbrot recurrence is identical, but
performance depends strongly on GPU FP64 throughput.  Consumer GPUs often
have much lower FP64 throughput than FP32, so this kernel may be slower than
\code{mandel\_1x\_float} on such hardware.

\subsubsection{Orbit update and escape test}
The update uses double-precision FMA intrinsics (e.g., \code{\_\_fma\_rd}) or
equivalent arithmetic to compute:
\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]
with the same bailout condition \(|z|^2 \ge 4\).

\subsection{Kernel: \code{mandel\_2x\_float}}
\label{sec:mandel-2x-float}

\subsubsection{Numeric type: float-float expansion}
This variant uses a float-float expansion type \code{dblflt} to represent a
value as:
\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]
where \code{head} (hi) carries the leading magnitude and \code{tail} (lo) is a
correction term. Arithmetic uses compensated routines such as
\code{add\_dblflt}, \code{sub\_dblflt}, \code{mul\_dblflt}, \code{sqr\_dblflt},
and often a specialized \code{mul\_dblflt2x(x,y)} to compute \(2xy\) with good
accuracy.

All double- and quad- float/double implementations are based on work from Andrew
Thall \cite{andrew-thall-dblflt}.  The implementations are modified here to
support both float and double base types.  These changes are unique to
\FractalShark{}.

\subsubsection{Mapping and orbit iteration}

The affine mapping for \(c\) is performed in \code{dblflt}:

\[
x_0 = cx + dx \cdot X,\qquad y_0 = cy + dy \cdot Y,
\]

and the orbit update follows the same real-form recurrence using expansion
operations. Cached squares are typically maintained as \code{dblflt}:

\[
zrsqr = x^2,\quad zisqr = y^2.
\]

\subsubsection{Escape test}
The implementations compares only the leading component (e.g., \code{head}) for
speed:

\[
zrsqr.\mathrm{head} + zisqr.\mathrm{head} < 4.
\]

This approach is fast but can misclassify points extremely near the boundary. A
fully robust bailout can incorporate both components (or a conservative bound).

\subsection{Kernel: \code{mandel\_2x\_double}}
\label{sec:mandel-2x-double}

\subsubsection{Numeric type: double-double expansion}
This variant uses a double-double type \code{dbldbl} representing:

\[
a \approx a_{\mathrm{hi}} + a_{\mathrm{lo}},
\]

with both components in double precision, yielding \(\sim\)106 bits of precision
in favorable cases. This approach enables deeper zoom while retaining a
structure similar to the float-float kernel.

\subsubsection{Mapping, orbit update, and escape test}

The kernel computes the same affine mapping and iterates the same recurrence,
but with double-double arithmetic. The escape predicate also uses the leading
component for speed.

\subsection{Kernel: \code{mandel\_4x\_float}}
\label{sec:mandel-4x-float}

\subsubsection{Numeric type: quad-float (4-term expansion)}

This variant uses a four-float expansion type \code{GQF::gqf\_real}:

\[
a \approx a_0 + a_1 + a_2 + a_3,
\]

with decreasing-magnitude components. Pixel coordinates and constants are lifted
into this type (e.g., \code{make\_qf(X,0,0,0)}), then the affine mapping and
orbit update are performed in quad-float arithmetic.  The implementation of this
numeric type is also based on Andrew Thall's work \cite{andrew-thall-dblflt}
with some minor changes specific to \FractalShark{}.

\subsubsection{Orbit update and escape test}
The update implements:

\[
x \leftarrow x^2 - y^2 + x_0,\qquad y \leftarrow 2xy + y_0,
\]

using quad-float operations (including specialized square and power-of-two
multiply helpers). The escape test can be performed in the full quad-float type:

\[
zrsqr + zisqr \le 4.
\]

\subsection{Kernel: \code{mandel\_4x\_double}}
\label{sec:mandel-4x-double}

\subsubsection{Numeric type: quad-double (4-term expansion)}
This variant uses a four-double expansion type \code{GQD::gqd\_real}:
\[
a \approx a_0 + a_1 + a_2 + a_3,\qquad a_k\in\mathbb{R}_{double}.
\]
It supports very deep zoom rendering with high numerical stability.

\subsubsection{Mapping, orbit update, and escape test}
The affine mapping and orbit update are evaluated in quad-double arithmetic. A
literal bailout constant (e.g., \code{4.0}) is promoted via overloads, so the
escape compare remains a full-precision comparison in the quad-double domain.

\subsection{Kernel: \code{mandel\_hdr\_float}}
\label{sec:mandel-hdr-float}

\subsubsection{Numeric type: HDR-normalized expansion}
This variant uses an HDR wrapper around an expansion type, e.g.:

\[
\code{HDRFloat<CudaDblflt<dblflt>>}.
\]

This implementation has no practical value because precision is limited by the
base expansion type; however, it serves as a testbed for HDR arithmetic in the
Mandelbrot context.  Thus, this kernel is primarily of academic interest.

\subsubsection{Reduction and stable comparisons}

The kernel frequently calls \code{HdrReduce()} on intermediate values. These
reductions are part of the numeric contract: norms and comparisons are assumed
to be applied to reduced/normalized values, enabling specialized comparators
without repeatedly materializing primitive scalars.

Instead of testing \(x^2+y^2 < 4\) in primitive form, the kernel maintains:

\[
zsq\_sum = zrsqr + zisqr
\]

and checks escape via a reduced comparator:

\begin{verbatim}
while (zsq_sum.compareToBothPositiveReduced(Four) < 0)
\end{verbatim}

This directly supports escape-time Mandelbrot rendering in HDR arithmetic while
keeping comparisons meaningful and stable.

\section{Iteration chunking via \code{iteration\_precision}}
\label{sec:chunking}

A few of the base kernels just described, which exclude linear approximation or
perturbation, are templated on an integer \code{iteration\_precision}
(\(1,2,4,8,16\)) and unroll multiple Mandelbrot steps inside the loop:

\begin{itemize}
  \item Each loop iteration performs \code{iteration\_precision} updates.
  \item The counter \code{iter} increases by that amount.
  \item The maximum iteration \code{n\_iterations} is adjusted so \code{iter}
        does not exceed the requested limit.
\end{itemize}

This approach reduces loop overhead. The trade-off is that the escape predicate
is typically checked only once per chunk; therefore the reported escape
iteration can be larger than the true first-escape iteration by up to
\code{iteration\_precision - 1}. For strict escape iteration counts (e.g., for
continuous/smooth coloring based on the first bailout), use a chunk size of 1 or
insert bailout checks within the unrolled body.

This optimization was mainly explored for educational purposes; in practice, the
benefit is small compared to other optimizations such as perturbation and linear
approximation.

\section{Perturbation Rendering of the Mandelbrot Set}
\label{sec:perturbation-concept}

This section explains \emph{perturbation} as a mathematical and algorithmic
technique for rendering the Mandelbrot set efficiently at deep zoom. The goal
is to compute accurate escape-time values for many nearby parameters \(c\)
while avoiding repeated high-precision evaluation of the full Mandelbrot
recurrence.

\subsection{Motivation: why perturbation is needed}

At large zoom depths, direct evaluation of

\[
z_{n+1} = z_n^2 + c,\qquad z_0=0,
\]

in floating-point arithmetic becomes inaccurate due to loss of significance in
\(c\) and accumulated rounding error. Using extended or arbitrary precision
solves the accuracy problem but is expensive when performed independently for
every pixel.

However, in a typical Mandelbrot image, nearby pixels correspond to parameters
\(c\) that differ only slightly. Their orbits therefore remain close for many
iterations. Perturbation exploits this coherence by computing one
high-precision \emph{reference orbit} and expressing nearby orbits as small
deviations from it.

\subsection{Reference orbit}

Choose a reference parameter \(c_\star\), usually the center of the current
view, and compute its orbit in sufficiently high precision:

\begin{equation}
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star = 0.
\end{equation}

The sequence \(\{z_n^\star\}\) is stored and reused for many pixels. This is the
only orbit that requires full high-precision evaluation.

\subsection{Delta formulation for nearby pixels}
\label{sec:perturb-delta-real}

For a pixel parameter \(c\) close to the reference parameter \(c_\star\), define
the parameter delta:

\begin{equation}
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star,
\end{equation}

and express the orbit at iteration \(n\) as a perturbation of the reference
orbit:

\begin{equation}
z_n = z_n^\star + \Delta z_n,
\end{equation}

where \(\Delta z_n\) is the \emph{orbit delta}. Substituting into the Mandelbrot
recurrence,

\[
z_{n+1} = z_n^2 + c,
\]

gives:

\begin{align}
z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) \\
&= (z_n^\star)^2 + 2z_n^\star \Delta z_n + (\Delta z_n)^2 + c_\star + \Delta c.
\end{align}

Subtracting the reference recurrence

\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star
\]

yields the \emph{exact delta update}:

\begin{equation}
\Delta z_{n+1} = 2z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturb-exact}
\end{equation}

No approximation has been made: perturbation preserves the exact Mandelbrot
dynamics as long as the reference orbit is computed accurately.

\subsubsection{Expansion into real and imaginary components}

To map \cref{eq:perturb-exact} directly to an implementation, write all
quantities in real and imaginary parts:

\begin{align*}
z_n^\star &= x_n^\star + i y_n^\star, \\
\Delta z_n &= \Delta x_n + i \Delta y_n, \\
\Delta c &= \Delta c_x + i \Delta c_y.
\end{align*}

First expand the quadratic term:

\begin{align}
(\Delta z_n)^2
&= (\Delta x_n + i \Delta y_n)^2 \\
&= (\Delta x_n^2 - \Delta y_n^2)
   + i(2 \Delta x_n \Delta y_n).
\end{align}

Next expand the linear term:

\begin{align}
2 z_n^\star \Delta z_n
&= 2(x_n^\star + i y_n^\star)(\Delta x_n + i \Delta y_n) \\
&= 2(x_n^\star \Delta x_n - y_n^\star \Delta y_n)
 + i\,2(x_n^\star \Delta y_n + y_n^\star \Delta x_n).
\end{align}

Combining terms gives the real and imaginary delta updates:

\begin{align}
\Delta x_{n+1}
&= 2(x_n^\star \Delta x_n - y_n^\star \Delta y_n)
   + (\Delta x_n^2 - \Delta y_n^2)
   + \Delta c_x, \label{eq:perturb-real} \\
\Delta y_{n+1}
&= 2(x_n^\star \Delta y_n + y_n^\star \Delta x_n)
   + 2\Delta x_n \Delta y_n
   + \Delta c_y. \label{eq:perturb-imag}
\end{align}

\subsubsection{Factored form used in implementations}

For numerical efficiency, \cref{eq:perturb-real,eq:perturb-imag} are often
evaluated in a factored form. Grouping terms yields:

\begin{align}
\Delta x_{n+1}
&= \Delta x_n\,(2x_n^\star + \Delta x_n)
   - \Delta y_n\,(2y_n^\star + \Delta y_n)
   + \Delta c_x, \label{eq:perturb-real-factored} \\
\Delta y_{n+1}
&= \Delta x_n\,(2y_n^\star + \Delta y_n)
   + \Delta y_n\,(2x_n^\star + \Delta x_n)
   + \Delta c_y. \label{eq:perturb-imag-factored}
\end{align}

This corresponds exactly to the compact complex form:

\[
\Delta z_{n+1}
= \Delta z_n \bigl(2 z_n^\star + \Delta z_n\bigr) + \Delta c,
\]

and maps directly onto typical GPU code using temporaries such as:

\[
(2x_n^\star + \Delta x_n), \quad (2y_n^\star + \Delta y_n).
\]

Despite the efficiency, \FractalShark{} implements the non-factored form
\cref{eq:perturb-real,eq:perturb-imag} everywhere.

\subsubsection{Reconstruction for escape testing}

Although perturbation evolves only the delta, escape-time rendering requires the
absolute orbit value:

\begin{equation}
z_n = z_n^\star + \Delta z_n
     = (x_n^\star + \Delta x_n) + i(y_n^\star + \Delta y_n).
\end{equation}

The standard Mandelbrot bailout condition is then applied:

\begin{equation}
|z_n|^2
= (x_n^\star + \Delta x_n)^2 + (y_n^\star + \Delta y_n)^2
\ge 4.
\end{equation}

This separation cleanly explains how perturbation math maps onto real-valued
implementation variables while preserving the exact Mandelbrot dynamics.


\subsection{Efficient perturbation update}

Equation~\eqref{eq:perturb-exact} can be rearranged into a form well suited for
implementation:

\begin{equation}
\Delta z_{n+1} = \Delta z_n \bigl(2z_n^\star + \Delta z_n\bigr) + \Delta c.
\label{eq:perturb-factored}
\end{equation}

This form requires only:

\begin{itemize}
\item one complex multiply,
\item one complex add,
\item access to the stored reference sample \(z_n^\star\).
\end{itemize}

Crucially, it avoids squaring large high-precision numbers for every pixel.
Instead, the expensive squaring is performed once for the reference orbit and
reused implicitly through \(z_n^\star\).

\subsection{Reconstructing the absolute orbit and escape test}

Perturbation evolves \(\Delta z_n\), but Mandelbrot rendering requires testing
escape of the absolute orbit. At each iteration, the current iterate is
reconstructed as:

\begin{equation}
z_n \approx z_n^\star + \Delta z_n.
\end{equation}

Escape-time rendering then applies the standard bailout condition:

\begin{equation}
|z_n|^2 = \Re(z_n)^2 + \Im(z_n)^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\end{equation}

The iteration index \(n\) at which this condition is first satisfied (or the
maximum iteration count if it never is) determines the pixel’s color.

\subsection{Numerical stability and rebasing}

Perturbation is efficient only while \(\Delta z_n\) remains small relative to
\(z_n^\star\). If \(|\Delta z_n|\) grows comparable to or larger than
\(|z_n^\star|\), numerical cancellation and loss of significance can degrade
accuracy.

To maintain stability, practical implementations use \emph{rebasing}. When
a stability criterion is violated, the current approximation is folded into a
new base:

\begin{equation}
z_n^\star \leftarrow z_n^\star + \Delta z_n,\qquad \Delta z_n \leftarrow 0,
\end{equation}

and perturbation continues relative to this updated reference state. This
preserves correctness while keeping deltas small.

\subsection{Why perturbation accelerates Mandelbrot rendering}

The efficiency gains come from two sources:

\begin{itemize}
\item \textbf{Reduced arithmetic cost.} Only one orbit is computed in full
high precision. All other pixels use cheaper delta updates.
\item \textbf{Spatial coherence.} Nearby pixels share the same reference orbit
for many iterations before diverging.
\end{itemize}

As a result, perturbation enables deep-zoom Mandelbrot rendering that would be
prohibitively slow if each pixel were evaluated independently in arbitrary
precision.

\subsection{Relation to other acceleration techniques}

Perturbation alone preserves the exact dynamics of the Mandelbrot map and
requires no linearization. It can be used by itself or combined with further
approximations (such as linear or bilinear approximation steps) to skip multiple
iterations at once. In all cases, perturbation provides the mathematical
foundation that makes efficient deep-zoom Mandelbrot rendering feasible.


\section{Reference Orbit Calculation}
\label{sec:ref-orbit-calc}

This section describes how \texttt{RefOrbitCalc} constructs (and optionally 
reuses) a high-precision \emph{reference orbit} for perturbation rendering of 
the quadratic map

\begin{equation}
  z_{n+1} = z_n^2 + c,\qquad z,c\in\mathbb{C},
\end{equation}

with an implementation that supports single-threaded CPU, multi-threaded CPU,
and GPU backends, plus several storage/compression modes that trade memory
footprint against recomputation.

\subsubsection{Expanded versus factored evaluation of the reference orbit}

The high-precision reference orbit evaluates the quadratic map

\[
z_{n+1} = z_n^2 + c,
\qquad
z_n = x_n + i y_n,
\qquad
c = c_x + i c_y,
\]

with full arbitrary-precision arithmetic.

Writing the iteration in real and imaginary components gives the
\emph{expanded form}:

\begin{align}
x_{n+1} &= x_n^2 - y_n^2 + c_x, \label{eq:ref-expanded-real} \\
y_{n+1} &= 2 x_n y_n + c_y. \label{eq:ref-expanded-imag}
\end{align}

This form follows directly from the algebraic definition of the polynomial.
Each term is evaluated explicitly, requiring two full-precision squares and
one full-precision multiplication per iteration.

The same quadratic map can be evaluated in a mathematically equivalent
\emph{factored form}. In particular, the real component may be written as

\begin{align}
x_{n+1}
&= (x_n - y_n)(x_n + y_n) + c_x, \label{eq:ref-factored-real}
\end{align}
while the imaginary component remains
\begin{align}
y_{n+1} &= 2 x_n y_n + c_y. \label{eq:ref-factored-imag}
\end{align}

In exact arithmetic, \cref{eq:ref-factored-real,eq:ref-factored-imag} are
identical to \cref{eq:ref-expanded-real,eq:ref-expanded-imag}. The difference
lies solely in how the products are formed.

From an implementation perspective, the expanded form evaluates
$x_n^2$ and $y_n^2$ independently and makes the subtraction
$x_n^2 - y_n^2$ explicit. This closely mirrors the mathematical definition
of the Mandelbrot polynomial and exercises the full squaring and
multiplication paths of the high-precision arithmetic.

The factored form reduces the number of full-precision squares by replacing
$x_n^2 - y_n^2$ with a single multiplication of the shared intermediates
$(x_n \pm y_n)$. This can reduce computational cost when multiplication and
squaring have similar expense, but it introduces stronger coupling between
terms and alters the way cancellation and rounding effects manifest in
finite-precision arithmetic.

In all cases, \FractalShark{} uses the expanded form despite the advantages offered
by the factored form. This choice simplifies reasoning about numerical behavior,
ensures that all arithmetic paths are exercised, and maintains consistency with
the perturbation update form used elsewhere in the codebase.  For better
performance it likely makes sense to implement both forms and compare their behavior
empirically.

\subsection{High-level pipeline and dispatch}
\label{subsec:ref-orbit-pipeline}

A reference orbit is stored in a \texttt{PerturbationResults<IterType,T,PExtras>}
instance, where:

\begin{itemize}
  \item \texttt{IterType} is the iteration index type (\texttt{uint32\_t} or \texttt{uint64\_t}).
  \item \texttt{T} is the low-precision numeric type used for downstream perturbation math (e.g.\ \texttt{float},
        \texttt{double}, \texttt{HDRFloat<...>}).
  \item \texttt{PExtras} selects the storage format for per-iteration orbit data:
        uncompressed (\texttt{Disable}), a lightweight compressor (\texttt{SimpleCompression}),
        or additional diagnostic bookkeeping (e.g.\ \texttt{Bad}).
\end{itemize}

Orbit construction is initiated through \texttt{AddPerturbationReferencePoint()}, which:

\begin{enumerate}
  \item Picks an initial guess \((c_x,c_y)\) (center of the current view if unset).
  \item Chooses an algorithm (\texttt{ST}, \texttt{MT}, reuse-based hybrids, or \texttt{GPU}) based on
        \texttt{m\_PerturbationAlg} and zoom factor heuristics.
  \item Allocates a new \texttt{PerturbationResults} slot, initializes metadata and bounds, and runs the chosen
        orbit kernel until escape, periodicity detection, or the maximum iteration count is reached.
\end{enumerate}

To control memory pressure, \texttt{OptimizeMemory()} monitors process commit 
usage and opportunistically drops cached orbits that are not of the currently 
demanded variant type when the working set exceeds a configurable threshold.

\subsection{Single-threaded authoritative orbit}
\label{subsec:ref-orbit-st}

The single-threaded path (\texttt{AddPerturbationReferencePointST}) computes 
the authoritative orbit directly using MPIR/GMP \texttt{mpf\_t} for the 
recurrence, while simultaneously emitting a low-precision shadow copy \((\hat{x}
_n,\hat{y}_n)\in T^2\) for downstream work (compression, bailout checks, 
periodicity tests).

\paragraph{State and initialization.}

Given a selected reference parameter \(c = c_x + i c_y\), the implementation:

\begin{itemize}
  \item Initializes \texttt{mpf\_t} temporaries for \(x,y,x^2\), and scratch products.
  \item Sets the initial iterate \((x_0,y_0) = (c_x,c_y)\) (this code uses the common convention \(z_0=c\)).
  \item Computes low-precision cast values \(\hat{c}_x,\hat{c}_y \in T\) either via \texttt{mpf\_get\_d} for
        native float/double, or via a mantissa/exponent extraction for extended formats.
\end{itemize}

\paragraph{Recurrence.}
Writing \(z_n = x_n + i y_n\), one iteration evaluates
\begin{align}
  x_{n+1} &= x_n^2 - y_n^2 + c_x, \\
  y_{n+1} &= 2 x_n y_n + c_y.
\end{align}
The implementation uses:
\begin{itemize}
  \item \texttt{mpf\_mul} and \texttt{mpf\_sub}/\texttt{mpf\_add} for the high-precision update.
  \item A low-precision snapshot \((\hat{x}_n,\hat{y}_n)\) acquired once per iteration for storage/compression,
        periodicity heuristics, and bailout checks.
\end{itemize}

\paragraph{Bailout.}

The bailout threshold is evaluated in low precision using

\begin{equation}
  \|\hat{z}_n + \hat{c}\|^2 = (\hat{x}_n + \hat{c}_x)^2 + (\hat{y}_n + \hat{c}_y)^2 > 256,
\end{equation}

which matches the code's use of \texttt{TwoFiftySix} and avoids a high-precision
norm each step.  Note that the bailout here is different from the one used in
the lower-precision kernels discussed previously; this is acceptable because the
reference orbit is used only for perturbation, not direct rendering.  The
difference is merely arbitrary.

\subsection{Periodicity tracking via \texorpdfstring{$\partial z/\partial c$}{dz/dc}}
\label{subsec:ref-orbit-periodicity}

Several modes enable periodicity detection. The implementation tracks the complex derivative

\(\frac{\partial z_n}{\partial c}\) in low precision:
\begin{equation}
  d_{n+1} = 2 z_n d_n + 1,\qquad d_0 = 1,
\end{equation}

with \(d_n = d_{x,n} + i d_{y,n}\). Expanding into real and imaginary parts yields:

\begin{align}
  d_{x,n+1} &= 2(x_n d_{x,n} - y_n d_{y,n}) + 1, \\
  d_{y,n+1} &= 2(x_n d_{y,n} + y_n d_{x,n}).
\end{align}

The code applies a radius-based heuristic: let

\begin{equation}
  n_2 = \max(|\hat{x}_n|,|\hat{y}_n|),\qquad
  r_0 = \max(|d_{x,n}|,|d_{y,n}|),
\end{equation}

and define a detection threshold

\begin{equation}
  n_3 = 2\,R_{\max}\,r_0,
\end{equation}

where \(R_{\max}\) is the maximum perturbation radius stored in \texttt{results}
. If \(n_2 < n_3\), the orbit is marked as \emph{maybe periodic} and the
reference loop terminates early (unless periodicity detection is disabled).
Otherwise, \((d_{x,n},d_{y,n})\) is advanced using the update above.

\section{Compression and Reference Orbit Reuse Modes}
\label{subsec:ref-orbit-compression-reuse}

Two orthogonal storage decisions are made while iterating:

\begin{enumerate}
  \item \textbf{Orbit storage} for perturbation use (\texttt{PExtras}):
  \begin{itemize}
    \item \texttt{Disable}: store every \((\hat{x}_n,\hat{y}_n)\) uncompressed.
    \item \texttt{SimpleCompression}: store a compressed subset of iterations using an error exponent
          determined by \texttt{Fractal::CompressionError}.
    \item \texttt{Bad}: store orbit values plus underflow/diagnostic flags.
  \end{itemize}
  \item \textbf{Reuse storage} for intermediate-precision regeneration (\texttt{ReuseMode}):
  \begin{itemize}
    \item \texttt{SaveForReuse1/2}: store uncompressed \texttt{mpf\_t} reuse entries.
    \item \texttt{SaveForReuse3}: store an intermediate-compressed reuse stream.
    \item \texttt{SaveForReuse4}: store a maximally-compressed intermediate reuse stream.
  \end{itemize}
\end{enumerate}

The next two sections describe these mechanisms in more detail.

\section{Reuse-based orbit regeneration}
\label{subsec:ref-orbit-reuse}

We consider iteration of the quadratic map
\[
f_c(z) = z^2 + c,\qquad z,c \in \mathbb{C},
\]
and distinguish between an \emph{authoritative} reference orbit computed in very high precision and a family of
\emph{intermediate precision} reference orbits constructed via perturbation from that authoritative orbit.

\subsubsection{Authoritative Reference Orbit}

Let
\[
c_0 \in \mathbb{C}
\]
denote the authoritative reference parameter.  The corresponding authoritative reference orbit is defined by
\begin{equation}
z^{(0)}_0 = 0, \qquad
z^{(0)}_{n+1} = \bigl(z^{(0)}_n\bigr)^2 + c_0,
\label{eq:authoritative_orbit}
\end{equation}
and is computed using sufficiently high precision that it is treated as exact for all practical purposes.
This orbit constitutes the single source of truth for all subsequent perturbative constructions.

\subsubsection{Perturbation Formulation}

For any nearby parameter
\[
c = c_0 + \Delta c,
\]
the corresponding orbit may be written as
\[
z_n(c) = z^{(0)}_n + \Delta z_n,
\]
where $\Delta z_n$ represents the perturbation relative to the authoritative orbit.
Substituting into the defining recurrence yields the exact perturbation equation
\begin{align}
\Delta z_0 &= 0, \\
\Delta z_{n+1}
&= 2 z^{(0)}_n \Delta z_n + (\Delta z_n)^2 + \Delta c.
\label{eq:perturbation_exact}
\end{align}
Equation~\eqref{eq:perturbation_exact} is the fundamental relation used both to construct intermediate reference orbits
and to evaluate per-pixel perturbations during rendering.

\subsubsection{Intermediate Precision Reference Orbits}

An intermediate precision reference orbit is defined at a parameter
\[
c_1 = c_0 + \Delta c_1,
\]
where $\Delta c_1$ is chosen such that the perturbation $\Delta z^{(1)}_n$ remains bounded within a fixed target accuracy
(e.g.\ absolute error $\lesssim 10^{-100}$) when represented in a chosen intermediate precision arithmetic.

The intermediate reference orbit is defined by
\[
z^{(1)}_n \equiv z^{(0)}_n + \Delta z^{(1)}_n,
\]
where $\Delta z^{(1)}_n$ is obtained by iterating
\begin{equation}
\Delta z^{(1)}_{n+1}
= 2 z^{(0)}_n \Delta z^{(1)}_n
  + \bigl(\Delta z^{(1)}_n\bigr)^2
  + \Delta c_1
\label{eq:intermediate_perturbation}
\end{equation}
in the intermediate precision format.

In the single-thread execution path (beginning at
\texttt{AddPerturbationReferencePointST}), this construction is performed sequentially using the cached authoritative
orbit samples $\{z^{(0)}_n\}$ as coefficients.
No approximation beyond finite-precision rounding is introduced; the intermediate orbit is mathematically equivalent
to directly iterating $f_{c_1}$, subject only to the chosen precision bound.

\subsubsection{Per-Pixel Perturbation from an Intermediate Orbit}

For an individual pixel parameter
\[
c_{\text{px}} = c_1 + \delta c,
\]
with $|\delta c| \ll |\Delta c_1|$, the final orbit is expressed as
\[
z_n(c_{\text{px}}) = z^{(1)}_n + \delta z_n,
\]
where $\delta z_n$ satisfies
\begin{equation}
\delta z_{n+1}
= 2 z^{(1)}_n \delta z_n
  + (\delta z_n)^2
  + \delta c.
\label{eq:pixel_perturbation}
\end{equation}
Because $|\delta c|$ is small, $\delta z_n$ remains well within the same fixed precision envelope used for the
intermediate reference orbit.

\subsubsection{SaveForReuse Modes}

Both \texttt{SaveForReuse1} and \texttt{SaveForReuse2} correspond to the mathematical framework described above.
They differ only in execution strategy.

\paragraph{SaveForReuse1.}
In \texttt{SaveForReuse1}, the authoritative orbit samples $\{z^{(0)}_n\}$ are stored after initial computation and reused
whenever intermediate reference orbits are constructed.
This avoids recomputation of \eqref{eq:authoritative_orbit} and ensures that all perturbative steps are driven by the same
authoritative data.

\paragraph{SaveForReuse2.}
\texttt{SaveForReuse2} is a multithreaded optimization of the same procedure.
It performs the identical perturbation recurrences
\eqref{eq:intermediate_perturbation} and \eqref{eq:pixel_perturbation}, using the same authoritative orbit samples and
producing the same intermediate and per-pixel results.
The distinction lies solely in how data is staged, reused, and synchronized across threads in the multithreaded path.

\paragraph{Equivalence.}
From a mathematical standpoint,
\[
\texttt{SaveForReuse1} \;\equiv\; \texttt{SaveForReuse2}.
\]
Both modes define the same authoritative orbit, the same intermediate reference
orbits, and the same per-pixel perturbation orbits.  Any differences are
strictly implementation-level optimizations and do not affect numerical results.
\texttt{SaveForReuse3} and \texttt{SaveForReuse4} are addressed in a subsequent
section.


\section{Multi-threaded Reference Orbit Acceleration}
\label{subsec:ref-orbit-mt}

The \texttt{MT3} path parallelizes high-cost MPIR operations by splitting the per-iteration work across
threads. Two recurring patterns appear:

\paragraph{Asynchronous squaring.}
For authoritative orbit computation, two worker threads compute \(x_n^2\) and \(y_n^2\) concurrently while the
main thread evaluates the cross term for \(y_{n+1} = 2 x_n y_n + c_y\), performs periodicity checks in low
precision, and orchestrates reuse/serialization. The main update then becomes:

\begin{equation}
  x_{n+1} = x_n^2 - y_n^2 + c_x,
\end{equation}

where \(x_n^2\) and \(y_n^2\) are returned from the worker threads.

\paragraph{Lock-free handoff with prefetch.}

Threads communicate through a minimal \texttt{ThreadPtrs<T>} mailbox containing atomic \texttt{In} and \texttt{Out}
pointers. The protocol is:

\begin{enumerate}
  \item Producer stores a work pointer in \texttt{In}.
  \item Worker spins until it swaps \texttt{In} to \texttt{nullptr}, prefetches the pointed-to operands, executes MPIR
        arithmetic, then publishes the same pointer in \texttt{Out}.
  \item Producer spins until it swaps \texttt{Out} back to \texttt{nullptr}, then consumes the computed fields.
\end{enumerate}

To mitigate cache miss latency on large MPIR limb arrays, the worker explicitly
prefetches both MPIR headers and limb ranges (64-byte stride), which is helpful
when iterating at very high precision.  There is likely room for improvement
here but it's not bad.

\section{GPU reference orbit backend}
\label{subsec:ref-orbit-gpu}

In many numerical algorithms, correctness and stability depend on arithmetic
precision far exceeding that of standard IEEE~754 floating-point formats.
Fractal reference orbits are a canonical example: small rounding errors
introduced early in an iteration can grow exponentially, eventually corrupting
orbit classification, perturbation terms, or bailout logic. While
arbitrary-precision libraries like MPIR can provide the required accuracy, their
performance is often insufficient when reference orbits must be computed
repeatedly or at very high precision. This creates a fundamental tension between
numerical fidelity and throughput.

To resolve this tension, high-precision arithmetic operations---in particular
multiplication, addition, and subtraction of large significands---must be
implemented with both mathematical rigor and architectural efficiency. Addition
and subtraction are dominated by carry propagation across hundreds or thousands
of limbs, while multiplication scales quadratically unless asymptotically faster
algorithms are employed. For precisions relevant to deep zoom reference orbits,
naive limb-by-limb multiplication quickly becomes the dominant cost,
overwhelming all other parts of the computation.

\subsection{Why Fast Multiplication Matters for Reference Orbits}

Generating many fractal reference orbits typically involves iterating a
recurrence relation (e.g.\ $z_{n+1} = f(z_n)$) at very high precision, often for
thousands of iterations per reference point. Each iteration requires multiple
high-precision multiplications and additions, and the total cost grows linearly
with iteration count and superlinearly with precision. Even modest speedups in
the core arithmetic therefore compound dramatically over the full computation.

By using an NTT-based multiplication strategy, large significand products can be
computed in $O(n \log n)$ time instead of $O(n^2)$, shifting the performance
profile of high-precision arithmetic into a regime where GPUs can be effectively
utilized. When combined with carefully engineered carry propagation and
normalization steps, this enables high-precision multiply/add/subtract
operations that are fast enough to make reference orbit computation practical at
scales that would otherwise be prohibitive.

\subsection{High-Precision Addition and Carry Propagation}

While high-precision multiplication often dominates asymptotic complexity,
addition and subtraction are also critical in practice for high-precision
iterative algorithms. Each arithmetic iteration of a reference orbit typically
performs several additions and subtractions on large significands, and these
operations occur at every step, regardless of whether multiplication is used. At
precisions of hundreds or thousands of limbs, even ``simple'' addition becomes a
nontrivial operation due to carry propagation across the entire limb array.

In a naive implementation, carry propagation is inherently serial: a carry-out
from limb $i$ must be resolved before limb $i+1$ can be finalized. This leads to
$O(n)$ latency with strict sequential dependence, which is particularly
ill-suited to massively parallel architectures such as GPUs. When reference
orbits are computed on such hardware, naive carry handling can become a
synchronization bottleneck that negates the performance gains achieved by fast
multiplication.

\subsection{Parallel Prefix Formulation of Carry Propagation}

Carry propagation can be reformulated as a prefix problem by observing that each
limb contributes a local carry \emph{transfer function} rather than a single
bit. Given a limb sum and an incoming carry, the limb either:

\begin{itemize}
\item \emph{absorbs} the carry (producing no carry-out),
\item \emph{propagates} the carry unchanged, or
\item \emph{generates} a new carry regardless of the input.
\end{itemize}

These behaviors can be encoded as elements of a small algebra with an
associative composition operator. The final carry into each limb is then the
prefix composition of all preceding transfer functions. Because the operator is
associative, standard parallel prefix (scan) algorithms can be used to compute
all carries in $O(\log n)$ depth.

This perspective generalizes classical carry-lookahead adders from hardware
design to software implementations operating on large limb arrays. On GPUs, it
allows the carry problem to be decomposed across threads and warps, enabling
high throughput while preserving exact arithmetic semantics.

\subsection{Single-Pass Parallel Prefix with Decoupled Look-back}

Traditional parallel prefix scans typically require multiple global
synchronization phases or multiple kernel launches, which are costly on GPUs and
limit scalability. Single-pass parallel prefix algorithms address this by
computing local prefixes within thread blocks and resolving inter-block
dependencies through a structured communication scheme.

The \emph{Single-pass Parallel Prefix Scan with Decoupled Look-back}
\cite{merrill2016single} technique further refines this approach. Each block
computes its local carry prefix independently, while a lightweight look-back
mechanism resolves the carry-in from preceding blocks without forcing all blocks
to synchronize globally. By decoupling local computation from global dependency
resolution, blocks can make forward progress independently, stalling only when
strictly necessary to obtain an upstream carry.

For high-precision addition and subtraction, this approach is especially
effective: the vast majority of limbs can be processed in parallel, and long
carry chains become rare and efficiently handled when they do occur. The result
is a carry propagation scheme with near-linear throughput, minimal
synchronization overhead, and better scaling to large precisions. When paired
with fast NTT-based multiplication, this makes fully high-precision arithmetic
practical for demanding workloads such as reference orbit generation.

The GPU backend delegates authoritative reference orbit generation to
\texttt{HpShark} kernels specialized by precision.  This section introduces the
key concepts and then goes into how the implementation actually works.

\subsection{Number Theoretic Transform and Multiply}

High-precision floating-point multiplication ultimately reduces to multiplying
two large integers (the significands), followed by normalization and exponent
adjustment. A standard way to accelerate large-integer multiplication is to
compute the convolution of digit-limbs using a fast transform. Over the reals,
one would use an FFT; over modular arithmetic, the analogous tool is the
\emph{Number Theoretic Transform} (NTT).

\subsection{From Large-Integer Multiplication to Convolution}

Let the two nonnegative integers to be multiplied be represented in base $B$ as

\[
A = \sum_{i=0}^{n-1} a_i B^i,\qquad
C = \sum_{i=0}^{n-1} c_i B^i,
\]

with $0 \le a_i, c_i < B$. Their product is

\[
A\cdot C = \sum_{k=0}^{2n-2} \left(\sum_{i=0}^{k} a_i c_{k-i}\right) B^k.
\]

The coefficients

\[
s_k = \sum_{i=0}^{k} a_i c_{k-i}
\]

form the \emph{discrete convolution} of the sequences $(a_i)$ and $(c_i)$. If we
can compute $(s_k)$ quickly, then we can recover the product (with subsequent
carry propagation in base $B$).

\subsection{NTT as an FFT Over a Finite Field}

The NTT computes a discrete Fourier transform, but with all operations performed
modulo a prime $p$ rather than over $\mathbb{C}$. Choose a prime modulus $p$ and
an $N$-th primitive root of unity $\omega \in \mathbb{Z}_p$ such that

\[
\omega^N \equiv 1 \pmod p,\qquad
\omega^k \not\equiv 1 \pmod p \text{ for } 0<k<N.
\]

Then the forward NTT of a length-$N$ sequence $x = (x_0,\dots,x_{N-1})$ is

\[
X_k = \sum_{j=0}^{N-1} x_j \,\omega^{jk} \pmod p,\qquad k=0,\dots,N-1,
\]

and the inverse NTT is

\[
x_j = N^{-1} \sum_{k=0}^{N-1} X_k \,\omega^{-jk} \pmod p,\qquad j=0,\dots,N-1,
\]

where $N^{-1}$ is the multiplicative inverse of $N$ modulo $p$.

Just as with the complex FFT, the key property is that the transform diagonalizes convolution:

\[
\mathrm{NTT}(x \star y) \;=\; \mathrm{NTT}(x)\odot \mathrm{NTT}(y),
\]

where $\odot$ denotes pointwise multiplication and $\star$ denotes cyclic
convolution modulo $x^N-1$. To compute the \emph{linear} convolution needed for
multiplication, we pad both inputs with zeros to length $N \ge 2n$ so that the
cyclic convolution coincides with the linear convolution.

\subsection{Why a Special Prime Helps: \texttt{MagicPrime}}

A practical NTT requires:

\begin{enumerate}

\item $N$ to be highly composite (typically a power of two) so that
Cooley--Tukey style butterflies apply efficiently;

\item a modulus $p$ such that $N \mid (p-1)$, guaranteeing the existence of an
$N$-th primitive root of unity in $\mathbb{Z}_p$;

\item fast modular multiplication on the target hardware (here, GPUs with
efficient 64-bit integer operations).

\end{enumerate}

The prime used here, denoted \texttt{MagicPrime}, is chosen specifically to
satisfy these constraints. The critical mathematical implication of using a
prime modulus is that $\mathbb{Z}_p$ is a field, so every nonzero element has a
multiplicative inverse and the NTT is well-defined and invertible when $\omega$
exists.

The most important structural requirement is:

\[
N \mid (p-1).
\]

When $N$ is a power of two, say $N = 2^m$, this becomes

\[
2^m \mid (p-1).
\]

Thus, the larger the power of two dividing $(p-1)$, the larger the supported
transform sizes.

\subsection{The ``Goldilocks'' Form and Power-of-Two Roots}

A particularly convenient choice on 64-bit hardware is a prime of the form

\[
p = 2^{64} - 2^{32} + 1,
\]

often called a ``Goldilocks'' prime. (In the codebase this role is played by
\texttt{MagicPrime}.) Two consequences make this form attractive:

\paragraph{(1) Large power-of-two factor in $p-1$.}

We have

\[
p - 1 = 2^{64} - 2^{32} = 2^{32}\left(2^{32}-1\right),
\]

so $2^{32}$ divides $(p-1)$. Therefore, for any $N = 2^m$ with $m \le 32$, there
exists an $N$-th root of unity in $\mathbb{Z}_p$. This enables power-of-two NTTs
up to length $2^{32}$ in principle (practically limited by memory and
implementation constraints), which is ample for large-limb convolutions.

\paragraph{(2) Efficient modular arithmetic with 64-bit operations.}

While modular multiplication in $\mathbb{Z}_p$ conceptually involves products up
to $p^2$, this modulus is engineered so reductions can be implemented
efficiently using 64-bit and 128-bit intermediates and/or Montgomery reduction.
The prime is close to $2^{64}$, which aligns well with native unsigned integer
ranges, and its special structure admits reduction strategies that avoid slow
division.

\subsection{NTT-Based Multiplication at a High Level}

Given limb sequences $(a_i)$ and $(c_i)$:

\begin{enumerate}

  \item Choose $N$ as a power of two with $N \ge 2n$, and choose a primitive
  $N$-th root $\omega \in \mathbb{Z}_p$.

  \item Zero-pad both sequences to length $N$ (interpreting limbs as elements of
  $\mathbb{Z}_p$).

  \item Compute $A_k = \mathrm{NTT}(a)_k$ and $C_k = \mathrm{NTT}(c)_k$.

  \item Compute pointwise products $P_k = A_k \cdot C_k \pmod p$.

  \item Compute $p_j = \mathrm{NTT}^{-1}(P)_j$ to obtain the convolution
  coefficients modulo $p$.

\end{enumerate}

Finally, because the convolution coefficients are computed modulo $p$, we must
ensure they represent the \emph{true integer} convolution values, not values
wrapped modulo $p$. This is achieved by choosing the limb base $B$ and transform
length $N$ so that each exact coefficient $s_k$ satisfies

\[
0 \le s_k < p,
\]

(or more generally, can be reconstructed from one or more moduli). With a single
sufficiently large prime (as is typical with a 64-bit Goldilocks prime and
appropriately sized limbs), the coefficients can be recovered directly and then
normalized via carry propagation in base $B$.

\medskip

This section establishes the mathematical foundation: \texttt{MagicPrime} is
selected so that large power-of-two NTTs exist (because $2^m \mid p-1$) and so
that modular arithmetic is efficient on 64-bit GPU hardware. Subsequent sections
will describe how this is specialized for high-precision floating-point
significands, including limb packing, Montgomery-domain multiplication, twiddle
scheduling, and normalization/carry handling.

\subsection{NTT in \FractalShark{}}

Fundamentally, the NTT implementation in \FractalShark{} is somewhat naive.  It
doesn't make much effort at memory layout or optimized coalescing, which is
likely costing it in performance.  This section describes how the NTT-based
multiplication is realized in practice, following the structure of
\FractalShark{}'s CUDA implementation. The overall goal is to compute the exact
convolution of two large significand arrays using modular arithmetic modulo
\texttt{MagicPrime}, and then to recover a canonical high-precision result via
normalization and carry propagation.

\subsubsection{Plan Construction and Parameter Selection}

Before launching any GPU kernels, an NTT ``plan'' is constructed. The plan
determines:

\begin{itemize}

  \item the coefficient bit-width $b$ used to pack the original 32-bit limbs
  into NTT coefficients,

  \item the packed coefficient length $L$,

  \item the transform size $N$, chosen as a power of two with $N \ge 2L$.

\end{itemize}

Correctness requires that no convolution coefficient overflow the prime modulus.
If the coefficients are bounded by $2^b$ and the transform length is $N$, then a
conservative bound on the largest convolution term is

\[
\max_k s_k \;\le\; N \cdot 2^{2b}.
\]

The plan builder enforces

\[
2b + \log_2 N + \delta \;\le\; 64,
\]

for a small safety margin $\delta$, ensuring that all exact convolution values
lie strictly below \texttt{MagicPrime}. This guarantees that the modular
convolution coincides with the true integer convolution.

\subsubsection{Roots of Unity and Montgomery Domain}

The CUDA setup phase precomputes all roots of unity required for the radix--2
NTT. For each stage $s$, a primitive $2^s$-th root $\omega_s$ and its inverse
are generated, along with:

\begin{itemize}

  \item powers of a twist root $\psi^i$ and $\psi^{-i}$,

  \item the modular inverse $N^{-1}$.

\end{itemize}

All constants are stored in Montgomery representation. As a result, every
modular multiplication inside the NTT butterflies is implemented as a Montgomery
multiply, avoiding explicit modular reduction by division.

\subsubsection{Packing, Twisting, and Forward NTT}

The input significands are initially stored as arrays of 32-bit limbs. These are
packed into base-$2^b$ coefficients

\[
A(x) = \sum_{i=0}^{L-1} a_i x^i,\qquad
C(x) = \sum_{i=0}^{L-1} c_i x^i,
\]

with $0 \le a_i, c_i < 2^b$. During packing, each coefficient is:

\begin{enumerate}

  \item mapped into $\mathbb{Z}_p$,

  \item multiplied by the twist factor $\psi^i$,

  \item converted into Montgomery form.

\end{enumerate}

After packing, an in-place radix--2 forward NTT is performed. A bit-reversal
permutation places coefficients into the correct order, followed by iterative
butterfly stages:

\[
(u, v) \;\mapsto\; (u + \omega v,\; u - \omega v),
\]

with all operations performed modulo \texttt{MagicPrime}. A grid-stride loop is
used so that threads collectively cover all $N$ coefficients, independent of the
exact grid dimensions.

\subsubsection{Multiway Pointwise Multiplication}

In the transform domain, convolution reduces to pointwise multiplication. To
amortize the cost of the NTT, the implementation computes three related
convolutions simultaneously using a standard three-multiply decomposition.
Conceptually, if

\[
X = X_r + iX_i,\qquad Y = Y_r + iY_i,
\]

then the products

\[
X_rY_r,\quad X_iY_i,\quad (X_r+X_i)(Y_r+Y_i)
\]

are sufficient to reconstruct both real and imaginary components. Accordingly,
three frequency-domain products are computed per transform index:

\[
\widehat{XX}_k,\quad \widehat{YY}_k,\quad \widehat{XY}_k,
\]

each via a Montgomery modular multiplication modulo \texttt{MagicPrime}.

\subsubsection{Inverse NTT and Untwisting}

Following pointwise multiplication, inverse radix--2 NTTs are applied using the
inverse roots of unity. The inverse transform yields coefficients still in
Montgomery form and still containing the twist factor. These are corrected by:

\begin{enumerate}
\item multiplying by $\psi^{-i}$,
\item multiplying by $N^{-1}$,
\item converting out of Montgomery representation.
\end{enumerate}

After this step, the data represents the exact integer convolution coefficients in base $2^b$.

\subsubsection{Unpacking and Normalization}

The final stage converts the convolution coefficients back into the original
limb representation. Base-$2^b$ digits are unpacked into wide accumulators
(typically 128-bit), after which carry propagation and normalization are
performed to produce a canonical high-precision result. This normalization step
is handled separately and is optimized using parallel-prefix techniques as
described in earlier sections.

\medskip

Together, these steps implement \FractalShark{}'s NTT-based multiplication
pipeline: from high-precision significand limbs, through modular convolution in
$\mathbb{Z}_{\texttt{MagicPrime}}$, and back to an exact, normalized
high-precision product suitable for subsequent arithmetic in reference orbit
computation.

\subsection{Amortizing Synchronization via Simultaneous Products}

In the presented GPU implementation, a dominant cost of large-scale NTT-based
multiplication is not the modular arithmetic itself, but the required grid-wide
synchronization. Each major phase of the pipeline— packing and twisting of
inputs, forward NTT, pointwise multiplication, inverse NTT, and untwisting with
normalization—requires that all threads observe a consistent global state. This
is enforced using explicit grid-level barriers (e.g.,
\texttt{cooperative\_groups::grid\_group::sync()}), whose latency grows with
grid size and is largely independent of the amount of arithmetic performed
between barriers.

To reduce the amortized cost of these synchronization points, the implementation
computes three related convolution products, $X\cdot Y$, $X^2$, and $Y^2$,
within a single NTT pipeline. All three products share the same forward
transforms, inverse transforms, twiddle scheduling, temporary buffers, and
synchronization boundaries. As a result, the fixed cost of grid-wide
coordination and memory visibility is incurred once, while producing three
mathematically independent convolution results. The additional arithmetic
required for the extra pointwise products consists only of a small number of
Montgomery multiplications per frequency index and is negligible compared to the
cost of global synchronization and memory traffic.

At the code level, this strategy is realized by a fused front-end that packs the
input limb arrays, applies twist factors, and converts values into Montgomery
form while producing multiple frequency-domain streams in a single pass. A
single grid-stride pointwise multiplication phase then computes the three
products concurrently. These are followed by a single multiway inverse radix--2
NTT, after which a unified untwisting, scaling by $N^{-1}$, and conversion out
of Montgomery representation are performed. Explicit grid-wide barriers appear
only at true phase boundaries, serving as producer--consumer separators between
global-memory stages rather than as fine-grained synchronization.

This design is well matched to the target workload. In high-precision reference
orbit computation, both cross terms and squared terms arise naturally and
repeatedly. By folding these operations into a single multiway NTT, the
implementation reduces the total number of kernel phases and synchronization
events, increases arithmetic intensity per barrier, and improves overall GPU
utilization. Consequently, the effective performance of the multiplication
pipeline is governed primarily by arithmetic throughput and memory bandwidth,
rather than by synchronization overhead.

\subsection{Usage in \FractalShark{}}
The key design is a persistent \emph{combo} object returned by initialization:

\begin{enumerate}
  \item \texttt{InitHpSharkReferenceKernel}: allocates device/host state for the reference orbit and sets
        \((c_x,c_y)\), max radius, and launch configuration.
  \item \texttt{InvokeHpSharkReferenceKernel}: advances the orbit in bounded batches of at most
        \(\texttt{MaxOutputIters}\), storing results in \texttt{OutputIters}.
  \item \texttt{ShutdownHpSharkReferenceKernel}: frees persistent resources.
\end{enumerate}

Because the precision must be fixed at compile time for the \texttt{
HpSharkFloatParams} specialization, \texttt{DispatchByPrecision} rounds the
requested precision to a power of two and chooses from a fixed set (\(\{256,512,
\dots,524288\}\) bits). Each invocation appends the emitted \texttt{OutputIters}
records into the CPU-side \texttt{PerturbationResults}. Periodicity and escape
are reported through \texttt{PeriodicityStatus} and handled similarly to the CPU
paths.


\section{Perturbation-only Mandelbrot rendering (without linear approximation)}
\label{sec:perturb-only}

This kernel can render the Mandelbrot set using \emph{perturbation alone}, i.e.,
without taking any LA v2 linear-approximation steps. The same CUDA entry point
\code{mandel\_1xHDR\_float\_perturb\_lav2<IterType,T,SubType,Mode,PExtras>} is
used; the behavior is selected at compile time via \code{LAv2Mode}. In
particular, when \code{Mode} includes only the perturbation path (e.g.\
\code{LAv2Mode::PO}), the kernel skips the LA stage traversal and runs only the
perturbation loop against a stored reference orbit.

\subsection{Rendering objective}
\label{sec:perturb-only-goal}

The goal remains standard escape-time rendering for
\[
z_{n+1} = z_n^2 + c,\qquad z_0=0,
\]
with bailout \(|z|^2 \ge 4\). For each pixel, the kernel computes the parameter
\(c\), iterates until escape or \code{n\_iterations}, and stores the resulting
iteration count in \code{OutputIterMatrix[idx]}.

In perturbation rendering, the expensive high-precision orbit evaluation for
each pixel is avoided by reusing a \emph{reference orbit} computed at a
reference parameter \(c_\star\), then evolving only the \emph{delta orbit} for
nearby pixels.

\subsection{Reference orbit and delta formulation}
\label{sec:perturb-only-deltas}

Let the reference parameter be \(c_\star\), with stored reference orbit
\(\{z_n^\star\}\) satisfying
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a pixel parameter \(c\) near \(c_\star\), define the parameter delta
\[
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star,
\]
and the orbit delta
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star.
\]
Substituting \(z_n = z_n^\star + \Delta z_n\) into the Mandelbrot recurrence
yields the \emph{exact} delta recurrence:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right) \\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
Perturbation uses this recurrence directly: it is not a linearization. The work
per iteration is reduced because \(z_n^\star\) is fetched from storage rather
than recomputed in high precision.

\subsection{Pixel parameter delta \texorpdfstring{$\Delta c$}{Delta c}}
\label{sec:perturb-only-deltac}

For each pixel \((X,Y)\), the kernel constructs a delta parameter relative to a
chosen reference center (sign conventions incorporate the image mapping):
\begin{align}
\Delta c_x &= dx \cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy \cdot Y - \texttt{centerY},
\end{align}
and stores this as \code{DeltaSub0} (with scalar components \code{DeltaSub0X},
\code{DeltaSub0Y}). The perturbation state starts at
\[
\Delta z_0 = 0,
\]
so \code{DeltaSubN} is initialized to zero.

\subsection{Perturbation recurrence used in the kernel}
\label{sec:perturb-only-update}

Writing the reference sample as \(z^\star = x^\star + i y^\star\) and the delta
as \(\Delta z = \Delta x + i \Delta y\), the delta recurrence can be expressed
in a factored form convenient for implementation:
\begin{equation}
\Delta z \leftarrow \Delta z \cdot (2 z^\star + \Delta z) + \Delta c.
\label{eq:perturb-factor}
\end{equation}
This identity is equivalent to
\(\Delta z_{n+1} = 2 z_n^\star \Delta z_n + (\Delta z_n)^2 + \Delta c\), because
\(\Delta z\cdot(2z^\star+\Delta z) = 2z^\star\Delta z + (\Delta z)^2\).

The kernel implements \cref{eq:perturb-factor} in real arithmetic by forming
the sums
\[
(2x^\star + \Delta x),\qquad (2y^\star + \Delta y),
\]
then updating:
\begin{align}
\Delta x &\leftarrow \Delta x\,(2x^\star + \Delta x) - \Delta y\,(2y^\star + \Delta y) + \Delta c_x, \\
\Delta y &\leftarrow \Delta x\,(2y^\star + \Delta y) + \Delta y\,(2x^\star + \Delta x) + \Delta c_y.
\end{align}
In the code, the intermediate quantities correspond to:
\begin{verbatim}
tempSum1 = 2*zy + DeltaSubNYOrig;  // (2 y^\star + \Delta y)
tempSum2 = 2*zx + DeltaSubNXOrig;  // (2 x^\star + \Delta x)
\end{verbatim}
followed by the real/imag updates. For extended or HDR types, the kernel routes
the same math through type-specialized implementations
(\code{T::custom\_perturb2} / \code{T::custom\_perturb3}) to keep the inner loop
tight and to enforce the type's reduction/normalization rules.

\subsection{Reconstructing the absolute orbit for escape testing}
\label{sec:perturb-only-reconstruct}

Perturbation evolves \(\Delta z_n\), but escape-time rendering requires a
bailout test on the absolute orbit \(z_n\). Each iteration reconstructs:
\[
z_n \approx z_n^\star + \Delta z_n,
\]
using the stored reference sample \(z_n^\star\) and the current delta. In the
kernel, this appears as:
\[
x = x^\star + \Delta x,\qquad y = y^\star + \Delta y.
\]
The bailout test is then performed on
\[
|z|^2 = x^2 + y^2,
\]
with the canonical threshold \(4\). For HDR and related types, the norm and the
comparison are performed on reduced values using specialized reduced
comparators to keep the escape decision stable at deep zoom.

\subsection{Reference index management and rebasing}
\label{sec:perturb-only-rebase}

The perturbation loop advances a reference-orbit index \code{RefIteration} in
lockstep with \code{iter}, fetching \(z^\star\) samples from \code{Perturb}. The
kernel includes a \emph{rebasing} mechanism that resets the delta
representation when it becomes ill-conditioned. Conceptually, if the delta
becomes comparable to or larger than the reconstructed orbit, the decomposition
\(z = z^\star + \Delta z\) stops being numerically advantageous. In that case,
the kernel folds the delta into the base by setting:
\[
\Delta z \leftarrow z,\qquad \text{and restart the reference index.}
\]
Operationally, the code compares the reconstructed orbit magnitude proxy
against the delta magnitude proxy, and also rebasing if the reference index
reaches the end of the stored orbit samples (to avoid out-of-range sampling).
After rebasing, perturbation continues from the new base representation.

This mechanism keeps the perturbation method usable across a wide range of
pixels and iteration depths while preserving the core rendering objective:
compute escape-time using a stable bailout on the reconstructed orbit.

\subsection{Using \code{LAv2Mode} to select perturbation-only execution}
\label{sec:perturb-only-mode}

The kernel is structured as two compile-time phases:
\begin{itemize}
\item an LA v2 phase guarded by \code{Mode == Full || Mode == LAO},
\item a perturbation phase guarded by \code{Mode == Full || Mode == PO}.
\end{itemize}
Therefore, perturbation-only rendering is achieved by instantiating the kernel
with a mode that includes only the perturbation path (e.g.\ \code{LAv2Mode::PO}).
In this configuration:
\begin{itemize}
\item \code{DeltaSub0} is computed from the pixel location,
\item \code{DeltaSubN} remains initialized to \(\Delta z_0 = 0\),
\item the kernel runs the perturbation loop, reconstructing \(z\) each step for
      bailout tests,
\item the final escape-time count is written to \code{OutputIterMatrix}.
\end{itemize}
This provides a complete Mandelbrot renderer based purely on reference-orbit
perturbation, without relying on any linear-approximation hierarchy.


\section{Approximation-based acceleration for Mandelbrot rendering}
\label{sec:approx-accel}

At deep zoom, the cost of iterating high-precision types for every pixel can be
dominant. The remaining components described below are still in service of the
same rendering goal: compute escape-time for \(z_{n+1}=z_n^2+c\), but by reusing
a \emph{reference orbit} and evolving \emph{deltas} for nearby pixels.

\subsection{Bilinear approximation (BLA) for orbit deltas}
\label{sec:bilinear-approx}

\subsubsection{Reference orbit and delta formulation}
Let the reference parameter be \(c_\star\) with orbit:
\[
z_{n+1}^\star = (z_n^\star)^2 + c_\star,\qquad z_0^\star=0.
\]
For a nearby pixel parameter \(c = c_\star + \Delta c\), define:
\[
\Delta z_n \stackrel{\mathrm{def}}{=} z_n - z_n^\star,\qquad
\Delta c \stackrel{\mathrm{def}}{=} c - c_\star.
\]
Expanding the recurrence gives:
\begin{align}
\Delta z_{n+1}
&= (z_n^\star + \Delta z_n)^2 + (c_\star + \Delta c) - \left((z_n^\star)^2 + c_\star\right)\\
&= 2 z_n^\star\,\Delta z_n + (\Delta z_n)^2 + \Delta c.
\end{align}
When \(\Delta z_n\) remains small, the quadratic term can be neglected,
yielding a linearized update:
\begin{equation}
\Delta z_{n+1} \approx A_n\,\Delta z_n + B_n\,\Delta c,\qquad
A_n = 2z_n^\star,\quad B_n = 1.
\end{equation}
BLA generalizes this into precomputed multi-step maps that \emph{jump} multiple
iterations while maintaining an explicit validity bound.

\subsubsection{What \code{BLA<T>} stores}
A \code{BLA<T>} instance stores two complex coefficients:
\[
A = A_x + iA_y,\qquad B = B_x + iB_y,
\]
plus:
\begin{itemize}
  \item \code{r2}: a squared-radius validity bound used during lookup,
  \item \code{l}: the number of Mandelbrot iterations summarized by this step.
\end{itemize}
These objects exist to accelerate \emph{escape-time evaluation} by evolving
\(\Delta z\) cheaply for many pixels, then reconstructing \(z \approx z^\star +
\Delta z\) to perform bailout checks consistent with Mandelbrot rendering.

\subsubsection{Applying a step: complex multiply-add}
The method \code{getValue(RealDeltaSubN, ImagDeltaSubN, RealDeltaSub0, ImagDeltaSub0)}
applies:
\[
\Delta z \leftarrow A\,\Delta z + B\,\Delta c,
\]
expanded into real arithmetic:
\begin{align}
\Re(\Delta z') &= A_x \Re(\Delta z) - A_y \Im(\Delta z) + B_x \Re(\Delta c) - B_y \Im(\Delta c), \\
\Im(\Delta z') &= A_x \Im(\Delta z) + A_y \Re(\Delta z) + B_x \Im(\Delta c) + B_y \Re(\Delta c).
\end{align}

\subsubsection{Composing steps to build longer jumps}
If one step maps \(\Delta z \mapsto A_x\Delta z + B_x\Delta c\) and a second maps
\(\Delta z' \mapsto A_y\Delta z' + B_y\Delta c\), the composition is:
\[
A_{\text{new}} = A_yA_x,\qquad B_{\text{new}} = A_yB_x + B_y.
\]
This supports a hierarchy of step sizes (often powers of two) for quickly
advancing delta orbits while rendering the escape-time field.

\subsubsection{GPU lookup: selecting a valid aligned step}
A GPU-side helper such as \code{GPU\_BLAS} stores the hierarchy and selects a
step that is both \emph{aligned} with the current iteration index and
\emph{valid} under the current bound check (typically comparing a computed
squared-magnitude proxy \code{z2} against \code{r2}). When a step is valid, the
renderer can advance the orbit by \code{l} iterations at a cost far below
performing \code{l} full high-precision Mandelbrot updates.

\subsection{LA v2 linear approximation with perturbation (HDR kernel)}
\label{sec:la-v2-perturb}

This kernel family combines staged linear-approximation steps with a
perturbation finisher loop against a stored reference orbit. The rendering
objective remains escape-time Mandelbrot evaluation; the kernel accelerates
that evaluation by evolving deltas and periodically reconstructing \(z\) to
perform bailout checks.

\subsubsection{Parameter delta per pixel}
Each pixel constructs a parameter offset \(\Delta c\) relative to a selected
reference center (sign conventions may incorporate the image \(Y\)-flip):
\begin{align}
\Delta c_x &= dx\cdot X - \texttt{centerX}, \\
\Delta c_y &= -dy\cdot Y - \texttt{centerY},
\end{align}
and packs \(\Delta c=\Delta c_x+i\Delta c_y\) into \code{DeltaSub0}.

\subsubsection{Delta state and reconstruction}
The kernel maintains \(\Delta z_n\) in \code{DeltaSubN} and reconstructs an
absolute orbit estimate using the stored reference orbit sample
\(z_j^\star\):
\[
z \approx z_j^\star + \Delta z.
\]
Escape-time rendering then proceeds by applying approximation steps when valid,
or performing perturbation updates otherwise, while periodically testing the
bailout condition on the reconstructed \(z\).

\subsubsection{Perturbation update}
Given a reference sample \(z^\star=x^\star+iy^\star\) and \(\Delta z=\Delta
x+i\Delta y\), the perturbation form is:
\[
\Delta z \leftarrow \Delta z\cdot(2z^\star+\Delta z) + \Delta c.
\]
In real arithmetic, with temporaries corresponding to \((2x^\star+\Delta x)\)
and \((2y^\star+\Delta y)\), this yields:
\begin{align}
\Delta x &\leftarrow \Delta x\,(2x^\star+\Delta x) - \Delta y\,(2y^\star+\Delta y) + \Delta c_x, \\
\Delta y &\leftarrow \Delta x\,(2y^\star+\Delta y) + \Delta y\,(2x^\star+\Delta x) + \Delta c_y.
\end{align}
In code comments, write these as \(x^\star\) and \(\Delta x\) (and similarly for
\(y\)) rather than using Unicode symbols.

\subsubsection{Escape test}
After updating \(\Delta z\), reconstruct \(z\approx z^\star+\Delta z\) and test:
\[
|z|^2 = x^2+y^2 \ge 4 \quad\Rightarrow\quad \text{escaped.}
\]
For HDR/expanded types, norms and comparisons are performed using reduced values
and specialized reduced comparators, preserving meaningful bailout decisions at
deep zoom.


\section{Summary: rendering-oriented view}
\label{sec:summary}

All kernels and acceleration schemes described here serve the same rendering
task: compute per-pixel escape-time for the Mandelbrot recurrence
\(z_{n+1}=z_n^2+c\). The implementation provides a precision ladder:
\begin{itemize}
  \item \code{1x} float/double: direct iteration for speed and moderate zoom,
  \item \code{2x} and \code{4x} expansions: deeper zoom with controlled error,
  \item HDR-normalized expansions: stable deep zoom with explicit reduction,
  \item BLA / LA v2 + perturbation: reuse a reference orbit to accelerate
        high-precision escape-time rendering for nearby pixels.
\end{itemize}
The common structure (pixel mapping, orbit iteration, bailout test, iteration
count store) ensures each section remains directly tied to Mandelbrot rendering
as the end goal.


\section{Development Notes}

This section simply has some development notes that were posted over time.

\subsection{2025-12-29 News --- Version 0.5}
\begin{itemize}
  \item \textbf{Major feature: GPU-accelerated Mandelbrot reference orbit.} Adds
  an experimental CUDA reference-orbit calculator intended to complement the
  existing CPU/MPIR multithreaded approach.

  \item \textbf{High-precision arithmetic pipeline on GPU.} Includes a full
  multiply/add/subtract pipeline, a Number Theoretic Transform (NTT)
  implementation, parallel-prefix carry propagation, and HDR-style exponent
  tracking.

  \item \textbf{Interoperability and correctness.} Supports conversion between
  MPIR and the GPU high-precision float format and adds manually-run test
  infrastructure.

  \item \textbf{Performance notes.} Release notes report large speedups at high
  limb counts on RTX 4090/5090-class GPUs; intended to shine at very deep
  built-in views.

  \item \textbf{Project plumbing.} Hooks up GitHub Actions to produce “official”
  builds, plus some refactoring; notes a startup windowing quirk.
\end{itemize}

\subsection{2025-11-30 News}

The neat thing about this GPU approach is that it still has low-hanging fruit 
related to optimization, unlike MPIR.  I've been wasting a bunch of time on the 
(Thanksgiving in USA) holiday weekend working on it.  I'll keep fussing with it 
and will probably not post again until I put out a version of \FractalShark{} with 
it hooked up, which I expect to happen later in December when I have time off.  
All code is on github, just no new version yet since it remains rather 
experimental and hacked up.

Example times in ms of updated implementation, comparing against serial host-
based approach (1 thread MPIR AVX2 for experiments).  This is the first 20000 
iterations of View 30 in \FractalShark{}, which is a depth $\sim 1\mathrm{e}{100000
}$ point.  This uses 16384 limbs on the GPU.  The CPU/MPIR uses the minimum 
bits required for that point, which is less, because the GPU implementation 
requires a power of 2.

\begin{verbatim}
Host (ms)    GPU(ms)    Ratio
57990        2055    28.2189781
58227        2022    28.79673591
57478        2041    28.16168545
57538        2014    28.56901688
55997        2015    27.79007444

Averages:
57446        2029.4    28.30729816
\end{verbatim}

Here's a summary of what I want to get done before \FractalShark{} 0.5 happens:

\begin{itemize}

\item Perform code cleanup, better comments etc.  It's a mess right now, this 
is just a weekend hobby after all and because it was unclear this would even 
work it's a real hack job.

\item Improve integration with \FractalShark{}, it's really just hacked in there 
currently since I've mostly used a standalone test program

\item Improve reference orbit memory usage - I would like to be able to 
allocate a fixed amount of memory to store the orbit and expand as needed.  
Right now it pessimistically allocates a lot, and it may be the wrong amount 
because it does periodicity detection on the GPU.  This is just engineering 
work and nothing fundamental, but will take time and without it the current 
implementation is rather impractical.

\item Automatically choose kernel launch parameters, right now it's manual.  
More engineering.  This is required to support other cards than mine.  In any 
case this implementation requires a feature call cooperative groups, which I 
believe implies Nvidia RTX 2XXX series or newer.

\end{itemize}

Those are the main things offhand, and I'm hoping with time off in December I can clean these up.

\subsection{2025-9-1 News}

The full reference orbit works with CUDA, though without periodicity detection 
or high precision to ``float exp'' conversion.  That's future work but I'm not 
worried about it.  To be clear, this is not hooked up end-to-end with 
\FractalShark{} itself, it's only working in a standalone test environment.  But 
the results are promising and prove it works.

This initial implementation relies on Karatsuba for the multiplies/squaring and 
then follows those with the high-precision adds/subtracts.  Initial results 
suggest a $\sim 12\times$ perf improvement relative to single-threaded CPU 
only, when comparing an overclocked 5950X vs an RTX 4090 with CUDA.  I'm happy 
with that, but not completely.

The main performance problem is this Karatsuba implementation.  Getting decent 
performance out of Karatsuba obviously requires recursion, and that gets costly 
on the GPU.  This implementation recurses several levels, which avoids costly 
local memory spill, but bites us because of register pressure.  The high 
register pressure limits the parallelism we can achieve.  The nice thing about 
Karatsuba for me is that it's not that hard to understand conceptually, so it 
was a great initial target for someone who doesn't know what they're doing.

Now that it's working, and I have a better sense of what's going on, I'm going 
to try a full NTT-based high-precision multiply approach.  The idea here is to 
rely on the number theoretic transform, similar to FFT, and parallelize the 
high precision multiply that way.

With this commit, we have a working host-based (CPU-only) approach to NTT high-
precision multiply that supports power-of-2 mantissa sizes and should scale 
effectively to CUDA but that's TBD.  It will be at least several months more 
work at my current rate (a few hours on the weekends) to achieve a first-cut 
CUDA implementation.

\subsubsection{NTT-based high-precision multiply (magic prime $2^{64} - 2^{32} + 1$)}

AI-generated slop follows in this subsection.  It looks accurate.

I'm experimenting with an NTT implementation over the 64-bit ``magic'' prime $p 
= 2^{64} - 2^{32} + 1$. This prime is NTT-friendly: it admits $2^{32}$-th roots 
of unity, so power-of-two transform sizes are straightforward, and it enables 
fast modular reduction on 128-bit products using the identity $2^{64} \equiv 2^{
32} - 1 \pmod p$.

\paragraph{High-level plan}

\begin{itemize}

\item Represent big mantissas as base-2 limbs (currently 32-bit limbs are 
convenient on GPU/CPU). Choose $N =$ next power of two $\ge 2\cdot L$ ($L =$ 
limb count) for the convolution length.

\item Forward NTT(A), NTT(B) mod $p$, pointwise multiply, inverse NTT, multiply 
by $N^{-1} \bmod p$, then perform carry propagation back to the chosen limb 
base.

\item Use iterative radix-2 Cooley--Tukey with an explicit bit-reversal 
permutation (DIT). Twiddles (powers of a primitive root) are precomputed and 
cached.

\item Butterflies and pointwise products operate in Montgomery form; 128-bit 
products are reduced via Montgomery multiplication ($R = 2^{64}$). A direct 
pseudo-Mersenne fold $(\text{lo} + (\text{hi} \ll 32) - \text{hi})$ exists but 
isn’t used on the hot path.

\end{itemize}

\paragraph{Notes and guardrails}

\begin{itemize}

\item Single-prime NTT is attractive here because $p$ fits in 64 bits and gives 
ample dynamic range; if/when larger bases or tighter bounds are desired, a multi
-prime CRT variant is the next step.

\item Power-of-two sizes only: that matches the current host prototype and 
simplifies CUDA mapping.

\item Carry fix-up remains outside the NTT and is done in base-$2^k$ with linear
-time passes; lazy (deferred) carries may help throughput.

\end{itemize}

\paragraph{Why this might beat Karatsuba on GPU}

\begin{itemize}

\item Avoids deep recursion and its register pressure; most work is regular 
butterflies, which parallelize and schedule well.

\item Pointwise multiplies dominate cost but are simple $64\times 64 \rightarrow
 128$ with fast reduction; memory access is structured and coalesced.

\end{itemize}

If the CUDA path pans out, the NTT route should scale better across precisions 
while keeping occupancy higher than the recursive Karatsuba path.

\paragraph{References (NTT / GPU big-int)}
\begin{itemize}
\item CGBN: CUDA Big-Num with Cooperative Groups \cite{CGBN_NVlabs}
\item Number-theoretic transform \cite{NTT_Wikipedia}
\end{itemize}

\subsection{2025-6-15 News}

This page actually gets traffic occasionally, so I just wanted to post a short 
update.  Since last August, I've been working on a CUDA-based, high-precision 
reference orbit implementation.  The objective is to beat \FractalShark{}'s 
existing multithreaded reference-orbit performance at higher digit counts, at 
least if you have a decent card.  Scroll down to ``[2025-6-15](\#2025-6-15)'' 
for the latest information on this subject.

Still fussing with it, with some delays because of vacation etc.  Having some 
issues with the optimized ``add'' implementation that does the 5-way add/
subtract.  It's a fun project, but has ended up more complex than I'd 
expected.  The reference implementation is almost working the way I want.

Worst case I could dump it and fallback to a series of regular A+B adds/
subtracts but I'm pretty determined to make the optimized approach work.  TBD 
if the performance actually pays off.  (Yes, I can hear you saying the 
Mandelbrot multiplies/squares dominate the cost, but it's bugging me and fun to 
play with).

\subsection{2025-4-26}

This high-precision arithmetic project is a lot of fun even if it's pretty 
ameteur-hour -- I know I'm leaving a lot of perf on the table yet.

Here's a brief update.  I'm happy enough with Karatsuba multiply now.  I've got 
$3\times$ parallel multiplies working.  For Mandelbrot, that corresponds to $x^2
$, $y^2$ and $x\cdot y$.  Rather than doing an optimized squaring 
implementation, I'm just jamming everything into the same Karatsuba 
implementation, so that all the synchronization is re-used.

A few weeks ago I had CUDA floating point add working.  That's much easier of 
course, though carry propagation is interesting.  I tried a parallel prefix sum 
but the performance was a bit underwhelming in the average case, which is what 
I care about.  I instead implemented a more naive strategy that has better 
average case perf and linear worst-case performance, which I think I'm fine 
with for Mandelbrot.  I'm not using warp-level primitives and haven't hooked up 
shared memory on that, so it's horrid performance compared to simply doing it 
on the CPU but as a percentage of the total it's minor, because large 
multiplies are so costly.  I'm not that worried about Add at this point.

I'm currently hooking up a 5-way combined add/subtract that does $A - B + C$ 
and $D + E$ in parallel to produce two outputs.  These inputs corresponds to $X^
2 - Y^2 + C_x$ and $2\cdot X\cdot Y + C_y$.

Strategy-wise, the idea is to complete this multi-way add operator, and then we 
should be able to do a reference orbit using alternating $3$-way multiply and $5
$-way add calls in CUDA.  It also needs periodicity detection and high-
precision to float+exp conversion, which shouldn't be bad.  Maybe in a few 
months I'll have something working end-to-end.

I was also speculating about trying Schönhage--Strassen CUDA multiply, but 
that's a ways out.

\subsection{2025-3}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepency arises in the CUDA 
calculation.  The discrepency points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \texttt{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \texttt{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \texttt{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

Current best result (5950X MPIR vs RTX 4090), 50000 sequential multiplies of 
7776-limb numbers, 128 blocks, 96 threads/block, uses shared memory and 162 
registers (max 255):

Host iter time: 26051 ms

GPU iter time: 2757 ms

Edited from earlier, fussing with perf-related parameters.  I'm very happy with how it's looking now.

\subsection{2025-2 -- What's going on with this native CUDA reference orbit calculation?}

It's been a month so here's an update before I go on vacation.  I'm still 
focusing on multiply performance and correctness.  I have a pretty aggressive 
test framework set up now and am evaluating it with various number sizes and 
hardware allocations.  Supporting weird lengths makes it easier to apply more 
levels of Karatsuba recursion.

I've added optional debug-specific logic that calculates checksums of each 
intermediate step and outputs those as well.  The host calculates the same 
intermediate checksums using my reference CPU implementation and comparison of 
the two happens in the test framework.  This approach is a pretty handy way to 
debug this nightmarish stuff because it just compares these checksums and 
immediately identifies where the first discrepency arises in the CUDA 
calculation.  The discrepency points right at the bad chunk of code.  Getting 
this checksumming strategy to work reliably was a real pain but it's a lot 
easier to debug than just getting a result that says ``wrong answer.''

For additional validation, it's initializing all dynamically-allocated CUDA 
global memory with a \texttt{0xCDCDCDCD} pattern, so if the implementation 
misses a byte, or overwrites something incorrectly, the checksum immediately 
captures it and makes it clear where the problem occurred.  This is not default 
CUDA behavior so I just put in a \texttt{memset}.  This approach also helps 
ensure that I have clear definitions for how many digits are being processed at 
each point in the calculation, since CUDA doesn't have nice \texttt{std::vector}
 or related containers.

One annoying thing I hit is slow compilation times.  It's using templates 
aggressively, so the kernel it spits out is optimized for a specific length of 
number.  That's OK in principle because we can just produce a series of kernels 
for different precisions but the downside is compiling a bunch of them takes 
quite a while and produces large code sizes.  It may make more sense to 
introduce more runtime variables and rely less on templates here but as it is 
this endeavor is mostly an academic exercise anyway, and I'm not expecting this 
thing to replace the existing CPU-based reference orbit calculations we have in 
the general sense.  But it'd be cool to get high performance in some meaningful 
range of scenarios anyway, hehe.

I'll probably try moving to $3\times$ parallel multiplies soon as a step toward 
a reference orbit, because I want to check that this thing can still compile 
effectively with that change.  This kernel already requires a fair number of 
registers in order to perform (avoid spilling registers to memory) and that's a 
bit of a concern because if $3\times$ parallel multiplies pushes it over the 
edge, performance will suffer.  There are various things I could do to decrease 
register usage of course, but all this stuff takes time.  Once $3\times$ 
multiplies works, then adding the additions/subtracts for a reference orbit 
should be OK.  Those would take place after the multiplies so should have no 
adverse effect on register use.

After that I still have to deal with periodicity and truncating the high-
precision values to float/exp, and all that will take more time.  This part may 
actually be rather costly perf-wise if I'm not careful because the naive 
approach is to serialize it with the rest of the calculation but that's a waste 
of hardware.

In a nutshell, this is a fun project and I'm having a blast, but it ended up 
bigger than I anticipated given how far I'm from the actual goal.  I'll keep 
grinding away at and we'll see where it can go.

\subsection{2025-1 -- What's going on with this native CUDA reference orbit calculation?}

The repository (``TestCuda'') has a new Karatsuba, high-precision, floating 
point multiply implementation working on my GPU and results are showing the CPU 
take $3$--$4\times$ longer (MPIR/AVX2) than the GPU on sequential multiplies of 
random numbers.  That's a key point -- sequential multiplies, so the result is 
applicable to e.g.\ a reference orbit calculation.  I'm really happy with this 
result, because there's leftover hardware on the GPU that could be used to run 
a couple of these in parallel (e.g.\ two squares and a multiplication or three 
squares for Mandelbrot).

Getting high-throughput high-precision on-GPU is already more-or-less solved:  
Nvidia already provides a library for it (see related work below).  But getting 
sequential to work decently at sizes that are still interesting (e.g.\ ones we 
might actually try on the Mandelbrot) is not as widely investigated, which is 
why I've been dwelling on this for a while and still have only mediocre results 
:p

A variety of caveats currently:

\begin{itemize}

\item I'm comparing a 5950X vs RTX 4090, which clearly affects the relative 
numbers.

\item The approach requires CUDA cooperative groups, which I think is RTX 2xxx 
and later, so fairly recent cards.

\item The size I'm getting the best result at currently is relatively large: 
8192 32-bit limbs, which is pretty big.  It still beats the CPU down to 2048 
limbs though (CPU takes $1.6\times$ longer here) and at that size there is a 
bunch of unused hardware on the GPU so it should be possible to do the three 
multiplies for Mandelbrot in parallel.

\end{itemize}

Anyhow, I wanted to post it because this is a pretty complex investigation and 
I expect to spend some more time on it because it's been fun to look at.

Here are some bullet points on the approach:

\begin{itemize}

\item Uses CUDA cooperative groups, so we should be able to do a reference 
orbit with a single kernel invocation.

\item 32-bit limbs, 128-bit intermediate results (2x64 integers) because of 
intermediate carries.  Really it's just 64 bits + plus a few more.  This 
approach is likely not super-efficient but it's where we're at.

\item Stores the input mantissa in the chip ``shared memory''.  For 8192 limbs, 
that's $8192 \cdot 4$ bytes $\cdot 2$ numbers to multiply $= 64$KB, and then 
stores $2 \cdot 16$KB extra for an intermediate Karatsuba result, for 96KB 
total.  This piece is negotiable and I could bring down/eliminate the shared 
memory requirement depending on how things progress.

\item One GPU thread per input limb, or two output limbs per thread.  It does a 
full 16384 limb output in this example and then truncates/shifts it.

\item The GPU floating point format has a separate integer exponent, which is 
overkill for Mandelbrot but I figured I'd keep it for now because it's not a 
performance problem.  It also keeps a sign separately.

\item The application I'm using to test this has a bunch of cases to verify 
that it's producing correct results.  It generates pseudo-random numbers with 
many \texttt{0xFFF...} limbs, zero-limbs, and related, to force carries/
borrows.  The test program compares all results against my own Karatsuba CPU-
based implementation I can use as a reference, and more importantly, the MPIR 
implementation (\texttt{mpf\_mul}) for correctness.

\item I've got MPIR to GPU and GPU to MPIR conversion capability, so it's easy 
to translate formats as needed.

\item The GPU implementation gets its best performance when it doesn't recurse, 
and instead switches straight to convolutions on the sub problems.  Recursing 
is still getting me slightly worse performance and I think I know why but 
haven't worked out how to fix it.

\end{itemize}

Here is some related work:

\begin{itemize}

\item A Study of High Performance Multiple Precision Arithmetic on Graphics 
Processing Units: Niall Emmart \cite{Emmart_GPU_MultiPrecision}

\item Missing a Trick: Karatsuba Variations: Michael Scott \cite{Scott_Karatsuba
_MissingTrick}

\item MFFT: A GPU Accelerated Highly Efficient Mixed-Precision Large-Scale FFT 
Framework: \cite{MFFT_GPU_FFT}

\item Karatsuba Multiplication in GMP: \cite{GMP_Karatsuba}

\item Karatsuba Algorithm: \cite{Karatsuba_Wikipedia}

\item Toom--Cook Multiplication: \cite{ToomCook_Wikipedia}

\item Sch{\"o}nhage--Strassen Algorithm: \cite{SchonhageStrassen_Wikipedia}

\item CGBN: CUDA Accelerated Multiple Precision Arithmetic Using Cooperative 
Groups: \cite{CGBN_NVlabs}

\end{itemize}

All the code is here / GPL etc but it's just an experiment: \\
\url{https://github.com/mattsaccount364/FractalShark}

It'll all end up in \FractalShark{} eventually assuming I can get something end-to-
end working, and at this point I believe I can, but this is very slow-going 
yet.  Anyway, it's a fun area and I'll probably continuing dinking with it, so 
there probably won't be much new on \FractalShark{} proper until I can get this 
behemoth under better control.  And we'll see if it works out -- lots of 
remaining details to resolve and it may not work decently when combined into a 
full reference orbit.  Overall though I'm really happy with where it's at.  
Happy to answer questions etc.


\subsection{2025-07-28 News --- Version 0.46}

Version \texttt{0.46} was published as a pre-release, focusing on refactoring,
reference compression experimentation, save-file compatibility, and test infrastructure.

\begin{itemize}

  \item Continued internal \textbf{refactoring} of core rendering and control logic.
  
  \item Experimental support for \textbf{Imagina-style “max” reference compression},
        as an alternative to the existing “simple” scheme.
  
  \item Added preliminary support for \textbf{Imagina-compatible save files}.

  \item Enhanced internal \textbf{test infrastructure} with broader coverage and tooling.
    
  \item Work in progress toward additional infrastructure improvements.

\end{itemize}

This release was published on GitHub on July 28, 2025 (UTC) and is tagged as a
pre-release build.  


\subsection{2024-04-20 --- Version 0.45}

\begin{itemize}
  
  \item Adds a first cut at reference compression for an intermediate “perturbed
  perturbation” reference-orbit strategy.

  \item Includes three internal variants (v1: uncompressed/3 threads; v2:
  uncompressed/4 threads; v3: compressed/4 threads).

  \item v3 becomes the default at very deep “auto” perturbation settings;
  guidance given for when to stick to the older MT2 + periodicity workflow.

\end{itemize}


\subsection{2024-03-31 News --- Version 0.44}

Version \texttt{0.44} represents a substantial internal refactor emphasizing memory management, allocator control, and reference-orbit performance.

\begin{itemize}

  \item Removed the Boost dependency and introduced a custom
  \textbf{HighPrecision} wrapper.

  \item Improved reference-orbit performance, especially for low-precision,
  high-period locations.

  \item Improved performance of the \textbf{perturbed perturbation} algorithm.

  \item Introduced a custom file-backed \textbf{bump allocator} with stable
  interior pointers.

  \item Extended \texttt{GrowableVector} to support allocator-style usage
  without increasing committed memory.
  
  \item Updated reference orbit and linear approximation save-file formats.

  \item Added an \textbf{automatic perturbation mode} switching between single-threaded and multithreaded execution.

  \item Instrumented for memory-leak detection and fixed all known leaks.

  \item Fixed a long-standing correctness bug in BLAv1 dating back to version
  0.21.

  \item \textbf{Clarified ``perturbed perturbation'' design.} The implementation
  is framed as storing an intermediate-resolution reference orbit (e.g.\
  $\sim$500 bits), then using low-precision perturbation off that intermediate
  orbit to generate subsequent nearby reference orbits more efficiently.
  :contentReference[oaicite:3]{index=3}

\end{itemize}


\subsection{2024-03-03 News --- Version 0.43}
\begin{itemize}
  
  \item Significant performance wins for lower-precision, high-period
  reference-orbit computation.
  
  \item Adds a custom per-thread bump allocator for MPIR high-precision numbers.
  
  \item Rebalances work across the existing 3-thread multithreaded
  reference-orbit approach.

  \item \textbf{Allocator usage guidance.} The bump allocator is primarily
  beneficial for low-precision, high-period reference orbits; at very high
  precision the arithmetic dominates and allocation overhead is negligible. A
  suggested heuristic is to use the fixed-block bump allocator below roughly
  1000 digits and fall back to the global allocator above that.
  :contentReference[oaicite:0]{index=0}

  \item \textbf{Empirical sizing recommendation.} Tested successfully on a $\sim
  1\mathrm{e}{6000}$ location with about 4\,MB of bump-allocator space;
  recommendation is to use the smallest practical buffer (cache behavior matters).
  :contentReference[oaicite:1]{index=1}

  \item \textbf{Implementation refinement note.} In the current multithreaded
  orbit implementation, most allocations may occur on the main thread;
  thread-local allocation may be unnecessary. A few small allocator issues were
  identified for cleanup in the following release.
  :contentReference[oaicite:2]{index=2}

\end{itemize}


\subsection{2024-02-24 News --- Version 0.42}

Version \texttt{0.42} introduced major improvements to linear approximation performance, benchmarking, and configurability.

\begin{itemize}
  \item Added \textbf{multithreaded linear approximation table generation}.
  \item Dramatically improved performance for high-period locations, especially with reference compression enabled.
  \item Added fine-grained benchmarking and performance breakdowns.
  \item Enabled regeneration of linear approximation tables independently of per-pixel data.
  \item Introduced adjustable linear approximation presets trading memory, accuracy, and performance.
\end{itemize}

This release corrected a prior precision regression and restored high-accuracy defaults.


\subsection{2024-01-15 News --- Version 0.41}

\begin{itemize}
  \item Fixes a minor bug in reference compression / GPU.
  \item Memory-use overhaul for storing LA tables and reference orbit to minimize RAM needed for hard views (e.g., View \#27).
  \item Default mode now uses temporary files + memory-mapped IO; avoids needing 2$\times$ memory during resize/copy and reduces committed-memory requirements with no measurable overhead per the release note.
\end{itemize}


\subsection{2024-01-01 News --- Version 0.4}

\begin{itemize}
  \item First working implementation of \textbf{runtime} reference compression
  (decompress-on-demand during rendering).

  \item Motivated as reducing end-to-end RAM requirements dramatically on very
  high-period locations; not enabled by default for “Auto” rendering and
  described as work-in-progress.

  \item \textbf{Motivation: memory pressure.} Reference compression is
  explicitly framed as a response to high RAM requirements at difficult views
  (notably after enabling 64-bit iteration counts).
  :contentReference[oaicite:14]{index=14}
\end{itemize}


\subsection{2023-12 News --- Version 0.32}

Version \texttt{0.32} focused on usability, correctness, and incremental architectural cleanup.

\begin{itemize}
  \item Added \textbf{progressive rendering}, allowing partial image updates
  during long GPU renders.
  \item Added a \textbf{2x32 non-HDR linear approximation} path for numerically
  difficult shallow zooms.
  \item Fixed a subtle but severe correctness bug in the 2x32 implementation
  present in version 0.31.
  \item Added a basic regression test suite.
  \item Refactored code and restored buildability across inactive code paths.
\end{itemize}


\subsection{2023-12-09 News --- Version 0.31}

Version \texttt{0.31} expanded the set of available rendering algorithms and
substantially improved default performance at shallow and mid-depth zooms.

\begin{itemize}
  \item Added \textbf{1x32 linear approximation / perturbation} for
  high-performance shallow zooms.
  
  \item Added \textbf{1x64 linear approximation / perturbation}, primarily for
  correctness testing.
  
  \item Implemented \textbf{automatic render-algorithm selection} based on zoom
  depth.
  
  \item Eliminated unnecessary reference-orbit copying, significantly improving
  frame-to-frame performance.
  
  \item Fixed several bugs, including 2x32 correctness issues and unaligned
  HDRx64 memory access.

  \item \textbf{Auto-selection policy detail.} Auto mode is described as staging
  from direct 32-bit rendering at very shallow zooms, to 1x32 perturb-only, to
  1x32 LA, and finally 1x32 float-exp LA at deeper zooms, with the goal of
  improving default performance below roughly $10^{38}$.
  :contentReference[oaicite:6]{index=6}

  \item \textbf{Reference-orbit reuse optimization.} Removing unnecessary
  reference-orbit copying is called out as a major win for deep-zoom
  frame-to-frame responsiveness when reusing a reference orbit.
  :contentReference[oaicite:7]{index=7}

\end{itemize}


\subsection{2023-11-26 News --- Version 0.3}

\begin{itemize}
  
  \item Adds HDRx2x32 linear approximation: faster than LA/HDRx64 on consumer
  cards while retaining nearly native-64-bit precision.
  
  \item Notes that a “Debug View 20” anomaly is resolved under HDRx2x32,
  supporting the hypothesis that HDRx32 precision limits caused the issue.
  
  \item Misc performance improvements and bug fixes elsewhere.

  \item \textbf{Debug View 20 consistency improvement.} The ``Debug View 20''
  anomaly is reported as resolved under HDRx2x32, supporting the conclusion that
  the earlier HDRx32 behavior was a precision-limit issue.
  :contentReference[oaicite:4]{index=4}

  \item \textbf{HDRx2x32 precision characterization.} Described as ``Linear
  Approximation + Float EXP'' using a pair of 32-bit floats plus an exponent,
  with an estimated combined precision of roughly 46 bits.
  :contentReference[oaicite:5]{index=5}

\end{itemize}


\subsection{2023-11-12 News --- Version 0.24}

\begin{itemize}
\item \textbf{UI/UX improvements.} Adds a hotkey help view and general UI
cleanup. :contentReference[oaicite:8]{index=8}
\item \textbf{Random palette generation.} Adds a random palette generator; notes
the palette system remains intentionally constrained and may be expanded.
:contentReference[oaicite:9]{index=9}
\item \textbf{Rendering diagnostics.} Adds a ``Show Rendering Details'' view for
inspecting current algorithm/settings. :contentReference[oaicite:10]{index=10}
\item \textbf{Early HDRx2x32 work.} Begins a GPU HDR 2x32 float implementation
(explicitly noted as not working yet). :contentReference[oaicite:11]{index=11}
\item \textbf{Crash diagnostics.} Automatically generates a crash dump on
unhandled exceptions. :contentReference[oaicite:12]{index=12}
\end{itemize}


\subsection{2023-11-05 News --- Version 0.23}
\begin{itemize}
  \item Adds switchable 64-bit iteration count support (enabling tens of
  billions of iterations per pixel, leveraging LA).
  \item Moves antialiasing/coloring into its own CUDA kernel for better
  performance.
  \item Multiple memory-reduction optimizations (systems-level) and related perf
  work.
  \item Adds reference-orbit save/load (no compression yet), useful for
  debugging deep spots.
  \item Notes a known bug manifesting in built-in View \#20 under a particular
  configuration (64-bit iterations + default HDRx32 LAv2), with long render
  times due to slow reference-orbit calculation.
\end{itemize}


\subsection{2023-10-14 News --- Version 0.22}

\begin{itemize}

  \item Significant LAv2 performance improvements (bug fixes); notes BLAv1 still
  winning in some cases.

  \item Fixes an LAv2 correctness bug; release note claims no known correctness
  bugs remaining.

  \item \textbf{Performance data point.} Reports a minibrot example improving
  from roughly 470\,ms to 370\,ms (render-only; reference orbit excluded) on the
  author’s machine. :contentReference[oaicite:13]{index=13}

\end{itemize}


\subsection{2023-10-08 News --- Version 0.21}
\begin{itemize}
  \item Performance improvements to BLA and LAv2.
  \item Memory-usage improvements (commit size / virtual address space issues
  called out).
  \item Generates more CUDA kernels for older GPUs (system requirements
  unchanged).
  \item Notes LAv2 is strong for many deep-zoom cases but still behind older BLA
  in some shallower cases.
\end{itemize}


\subsection{2023-09-16 News --- Version 0.2}

Version \texttt{0.2} introduced a major algorithmic milestone: the integration
of Imagina's linear approximation implementation as a CUDA kernel.

\begin{itemize}
  \item Added Imagina-based linear approximation (\texttt{LAv2}), now the
  default rendering algorithm.
  \item Resolved known correctness bugs in the new approximation path.
  \item Fixed a major performance regression caused by an earlier implementation oversight.
  \item Achieved large performance gains at deeper zoom levels compared to the
  existing bilinear approximation.
\end{itemize}

While the legacy bilinear approximation remains faster at very shallow zooms,
the new approach dominates once meaningful magnification is reached.


\subsection{2023-07-22 News --- Version 0.11}

Version \texttt{0.11} was released with a focus on performance improvements and
improved diagnostics.

\begin{itemize}
  \item Approximately \textbf{+10\% performance improvement} in the HDRx32 CUDA
  rendering path.
  \item Fixed several minor bugs affecting correctness and stability.
  \item Improved CUDA error reporting by displaying descriptive error strings
  instead of numeric codes.
\end{itemize}

A long-duration technology demonstration video accompanied this release,
showcasing a zoom to approximately $10^{4000}$ using the default HDRx32/BLA CUDA
kernel.  Rendering and post-processing each required roughly one and a half
days.

\bibliographystyle{plain} % or ieeetr, unsrt, alpha, etc.
\bibliography{notes}      % <-- filename WITHOUT .bib

\end{document}
