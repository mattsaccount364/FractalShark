#include "Multiply.cuh"

#include <cuda_runtime.h>

#include "HpGpu.cuh"
#include "BenchmarkTimer.h"

#include <iostream>
#include <vector>
#include <gmp.h>
#include <cstring>
#include <sstream>
#include <iomanip>
#include <cmath>
#include <algorithm>

#include <cooperative_groups.h>
namespace cg = cooperative_groups;


// Structs for carry handling (similar to addition)
struct GlobalMulBlockData {
    // Define any necessary global data for multiplication
    // Placeholder: can be expanded as needed
};

struct PartialSum {
    uint64_t sum;
};

#include <cooperative_groups.h>
namespace cg = cooperative_groups;

// Device function to perform high-precision multiplication
__device__ void MultiplyHelper(
    const HpGpu *A,
    const HpGpu *B,
    HpGpu *Out,
    uint64_t *carryOuts_phase3, // Array to store carry-out from Phase 3
    uint64_t *carryOuts_phase6, // Array to store carry-out from Phase 6
    uint64_t *carryIns,          // Array to store carry-in for each block
    cg::grid_group grid
) {
    // Calculate global digit index
    int globalDigit = blockIdx.x * ThreadsPerBlock + threadIdx.x;

    // Ensure we do not exceed the number of digits
    if (globalDigit >= HpGpu::NumUint32) return;

    // Shared memory per block
    __shared__ uint32_t carryBitsShared[ThreadsPerBlock];

    // Phase 1: Compute partial products for each digit
    uint64_t partialProductLow = 0;
    uint64_t partialProductHigh = 0;

#pragma unroll
    for (int j = 0; j < HpGpu::NumUint32; ++j) {
        int otherDigit = globalDigit - j;
        if (otherDigit >= 0 && otherDigit < HpGpu::NumUint32) {
            uint64_t a = static_cast<uint64_t>(A->Digits[j]);
            uint64_t b = static_cast<uint64_t>(B->Digits[otherDigit]);
            uint64_t product = a * b;

            // Add lower 32 bits
            uint32_t productLow = static_cast<uint32_t>(product & 0xFFFFFFFF);

            // Store the original lower 32 bits before addition
            uint32_t oldLow = static_cast<uint32_t>(partialProductLow & 0xFFFFFFFF);

            partialProductLow += productLow;

            // Correct carry detection: check if lower 32 bits overflowed
            if ((partialProductLow & 0xFFFFFFFF) < oldLow) {
                partialProductHigh += 1;
            }

            // Add upper 32 bits
            uint32_t productHigh = static_cast<uint32_t>(product >> 32);
            partialProductHigh += productHigh;
        }
    }

    // Phase 2: Store lower 32 bits in Out and prepare carry for carryOuts_phase3
    Out->Digits[globalDigit] = static_cast<uint32_t>(partialProductLow & 0xFFFFFFFF);
    carryBitsShared[threadIdx.x] = static_cast<uint32_t>(partialProductHigh & 0xFFFFFFFF);
    __syncthreads();

    // Phase 3: Record carry-out for the block
    if (threadIdx.x == ThreadsPerBlock - 1) {
        uint32_t totalCarry = 0;
        for (int i = 0; i < ThreadsPerBlock; ++i) {
            totalCarry += carryBitsShared[i];
        }
        carryOuts_phase3[blockIdx.x] = static_cast<uint64_t>(totalCarry);
    }
    __syncthreads();

    // Synchronize all blocks before computing carry-ins
    grid.sync();

    // Phase 4: Compute carry-ins using prefix sum on carryOuts_phase3
    if (blockIdx.x == 0) {
        // First block has no carry-in
        carryIns[blockIdx.x] = 0;
    } else {
        // Sum all previous carryOuts_phase3 to get carryIn for this block
        carryIns[blockIdx.x] = 0;
        for (int i = 0; i < blockIdx.x; ++i) {
            carryIns[blockIdx.x] += carryOuts_phase3[i];
        }
    }
    __syncthreads();

    // Phase 5: Apply carry-ins to each block's output digits
    if (threadIdx.x == ThreadsPerBlock - 1) {
        uint64_t sum = static_cast<uint64_t>(Out->Digits[globalDigit]) + carryIns[blockIdx.x];
        Out->Digits[globalDigit] = static_cast<uint32_t>(sum & 0xFFFFFFFF);
        carryBitsShared[threadIdx.x] = static_cast<uint32_t>(sum >> 32);
    } else {
        carryBitsShared[threadIdx.x] = 0; // No carry-out for non-last threads
    }
    __syncthreads();

    // Phase 6: Record any new carry-outs generated by carry-ins
    if (threadIdx.x == ThreadsPerBlock - 1) {
        uint32_t totalCarry = 0;
        for (int i = 0; i < ThreadsPerBlock; ++i) {
            totalCarry += carryBitsShared[i];
        }
        carryOuts_phase6[blockIdx.x] = static_cast<uint64_t>(totalCarry);
    }
    __syncthreads();

    // Synchronize all blocks before handling final carry-outs
    grid.sync();

    // Phase 7: Handle any remaining carry-outs beyond the mantissa
    // **Modified Phase 7 to aggregate carry-outs from all blocks**
    if (blockIdx.x == 0 && threadIdx.x == 0) {
        uint64_t totalCarry = 0;
        // Aggregate all carryOuts_phase3 and carryOuts_phase6 from all blocks
        for (int i = 0; i < NumBlocks; ++i) {
            totalCarry += carryOuts_phase3[i] + carryOuts_phase6[i];
        }

        // **Handle carry-outs from cross-products (j + otherDigit >= NumUint32)**
        // Since the current loop in Phase 1 does not handle j + otherDigit >= NumUint32,
        // we need to account for these carry-outs separately.
        // The totalCarry now includes carry-outs from partialProductHigh across all digits.

        if (totalCarry > 0) {
            // Determine the number of shifts needed based on the magnitude of totalCarry
            int shifts = 0;
            while (totalCarry > 0xFFFFFFFF) {
                totalCarry >>= 32;
                shifts += 1;
            }

            // Shift the mantissa left by 'shifts' digits (32 bits each)
            for (int s = 0; s < shifts; ++s) {
                for (int d = HpGpu::NumUint32 - 1; d > 0; --d) {
                    Out->Digits[d] = Out->Digits[d - 1];
                }
                Out->Digits[0] = 0; // Clear the least significant digit after shift
            }

            // Add the remaining totalCarry to the most significant digit
            Out->Digits[HpGpu::NumUint32 - 1] += static_cast<uint32_t>(totalCarry & 0xFFFFFFFF);

            // Increment the exponent accordingly
            Out->Exponent += shifts * 32;
        }

        // **Additionally, handle carry-outs from cross-products beyond Phase3 and Phase6**
        // For example, if there are cross-products that result in carry-outs not captured,
        // they should be added here. However, since the current loop in Phase1 does not
        // compute j + otherDigit >= NumUint32, these are already encapsulated in carryOuts.
        // Ensure that the multiplication correctly aggregates all necessary carry-outs.
    }
    __syncthreads();

    // Phase 8: Initialize result properties (only block 0's thread 0 does this)
    if (blockIdx.x == 0 && threadIdx.x == 0) {
        // Determine the sign of the result
        Out->IsNegative = A->IsNegative != B->IsNegative;
        // Calculate the initial exponent of the result
        Out->Exponent = A->Exponent + B->Exponent;
        // Note: Any additional exponent adjustments have been handled in Phase 7
    }
}

__global__ void MultiplyKernel(
    const HpGpu *A,
    const HpGpu *B,
    HpGpu *Out,
    uint64_t *carryOuts_phase3,
    uint64_t *carryOuts_phase6,
    uint64_t *carryIns) {

    // Initialize cooperative grid group
    cg::grid_group grid = cg::this_grid();

    // Call the MultiplyHelper function
    MultiplyHelper(A, B, Out, carryOuts_phase3, carryOuts_phase6, carryIns, grid);
}


void ComputeMultiplyGpu(void *kernelArgs[]) {

    cudaError_t err = cudaLaunchCooperativeKernel(
        (void *)MultiplyKernel,
        dim3(NumBlocks),
        dim3(ThreadsPerBlock),
        kernelArgs,
        0, // Shared memory size
        0 // Stream
    );

    cudaDeviceSynchronize();
}

void ComputeMultiplyGpuTestLoop(void *kernelArgs[]) {

    //cudaError_t err = cudaLaunchCooperativeKernel(
    //    (void *)MultiplyKernelTestLoop,
    //    dim3(NumBlocks),
    //    dim3(ThreadsPerBlock),
    //    kernelArgs,
    //    0, // Shared memory size
    //    0 // Stream
    //);

    //cudaDeviceSynchronize();
}

